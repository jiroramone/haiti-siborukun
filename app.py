import streamlit as st
import pandas as pd
import numpy as np
import re
import plotly.express as px
import openpyxl

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(page_title="é…ç½®é¦¬åˆ¸è¡“ Web", layout="wide")

# ==========================================
# 1. å…±é€šãƒ­ã‚¸ãƒƒã‚¯ & ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
# ==========================================

def to_half_width(text):
    if isinstance(text, (list, pd.Series, np.ndarray)):
        text = str(text)
    if pd.isna(text): return text
    text = str(text)
    table = str.maketrans('ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™', '0123456789')
    text = text.translate(table)
    text = re.sub(r'[^\d\.]', '', text)
    return text

def normalize_name(x):
    if pd.isna(x): return ''
    normalized_name = str(x).strip().replace('ã€€', '').replace(' ', '')
    normalized_name = re.sub(r'[â˜…â˜†â–²â–³â—‡]', '', normalized_name)
    if ',' in normalized_name: normalized_name = normalized_name.split(',')[0]
    text = re.sub(r'[0-9\.]+[Rr]', '', normalized_name)
    text = re.sub(r'\(.*?\)', '', text)
    return text.replace('/', '').strip()

@st.cache_data
def load_data(file):
    """
    ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã‚’è¡Œã†é–¢æ•°
    Windows(Shift-JIS)ã¨Mac(UTF-8)ã®ä¸¡æ–¹ã«å¯¾å¿œ
    """
    df = None
    
    # 1. Excelãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç†
    if file.name.endswith('.xlsx'):
        try:
            file.seek(0)
            df = pd.read_excel(file, engine='openpyxl')
        except Exception as e:
            return pd.DataFrame(), f"Excelèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}"
            
    # 2. CSVãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç†
    else:
        try:
            file.seek(0)
            df = pd.read_csv(file, encoding='utf-8', on_bad_lines='skip')
        except UnicodeDecodeError:
            try:
                file.seek(0)
                df = pd.read_csv(file, encoding='cp932', on_bad_lines='skip')
            except Exception as e:
                return pd.DataFrame(), f"CSVèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼(æ–‡å­—ã‚³ãƒ¼ãƒ‰åˆ¤å®šä¸èƒ½): {e}"
        except Exception as e:
            return pd.DataFrame(), f"CSVäºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼: {e}"

    # --- ãƒ‡ãƒ¼ã‚¿æ•´å½¢ ---
    df.columns = df.columns.str.strip()
    
    rename_map = {
        'å ´æ‰€': 'å ´å', 'é–‹å‚¬': 'å ´å', 'å˜ã‚ªãƒƒã‚º': 'å˜ï½µï½¯ï½½ï¾', 
        'èª¿æ•™å¸«': 'å©èˆ', 'èª¿æ•™å¸«å': 'å©èˆ', 'å©èˆå': 'å©èˆ',
        'é¨æ‰‹å': 'é¨æ‰‹',
        'ãƒ¬ãƒ¼ã‚¹': 'R', 'ï¼²': 'R', 'ãƒ¬ãƒ¼ã‚¹å': 'R',
        'ç€': 'ç€é †', 'ç€ é †': 'ç€é †', 'ç•ª': 'æ­£ç•ª', 'é¦¬ç•ª': 'æ­£ç•ª'
    }
    df = df.rename(columns=rename_map)

    # é‡è¤‡ã‚«ãƒ©ãƒ ã®å‰Šé™¤
    df = df.loc[:, ~df.columns.duplicated()]

    if 'å ´å' not in df.columns: df['å ´å'] = 'Unknown'

    target_numeric_cols = ['R', 'æ­£ç•ª', 'å˜ï½µï½¯ï½½ï¾', 'é€†ç•ª', 'æ­£å¾ªç’°', 'é€†å¾ªç’°', 'é ­æ•°']
    for col in target_numeric_cols:
        if col in df.columns:
            df[col] = df[col].apply(to_half_width)
            df[col] = pd.to_numeric(df[col], errors='coerce')

    if 'R' not in df.columns or 'æ­£ç•ª' not in df.columns:
        return pd.DataFrame(), "ã‚¨ãƒ©ãƒ¼: å¿…é ˆåˆ—ï¼ˆãƒ¬ãƒ¼ã‚¹å/Rã€é¦¬ç•ª/æ­£ç•ªï¼‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"

    df = df.dropna(subset=['R', 'æ­£ç•ª'])
    df['R'] = df['R'].astype(int)
    df['æ­£ç•ª'] = df['æ­£ç•ª'].astype(int)

    for col in ['é¨æ‰‹', 'å©èˆ', 'é¦¬ä¸»']:
        if col in df.columns:
            df[col] = df[col].apply(normalize_name)
        else:
            df[col] = ''
            
    # å¿…é ˆåˆ—ç¢ºä¿
    required_cols = ['R', 'å ´å', 'é¦¬å', 'æ­£ç•ª', 'é¨æ‰‹', 'å©èˆ', 'é¦¬ä¸»', 'å˜ï½µï½¯ï½½ï¾', 'é€†ç•ª', 'æ­£å¾ªç’°', 'é€†å¾ªç’°', 'é ­æ•°']
    for col in required_cols:
        if col not in df.columns:
            df[col] = np.nan

    # ä¿å­˜ãƒ‡ãƒ¼ã‚¿ç”¨ã®åˆ—
    save_cols = ['å±æ€§', 'ã‚¿ã‚¤ãƒ—', 'ãƒ‘ã‚¿ãƒ¼ãƒ³', 'æ¡ä»¶', 'ã‚¹ã‚³ã‚¢', 'ç€é †', 'å‚¾å‘åŠ ç‚¹', 'ç·åˆã‚¹ã‚³ã‚¢']
    existing_save_cols = [c for c in save_cols if c in df.columns]
    
    final_cols = required_cols + existing_save_cols
    
    return df[final_cols].copy(), "success"

# ==========================================
# 2. é…ç½®è¨ˆç®—ãƒ»åˆ†æãƒ­ã‚¸ãƒƒã‚¯
# ==========================================

def calc_haichi_numbers(df):
    check_cols = ['é€†ç•ª', 'æ­£å¾ªç’°', 'é€†å¾ªç’°']
    if set(check_cols).issubset(df.columns) and df[check_cols].notna().all().all():
        df['è¨ˆç®—_é€†ç•ª'] = df['é€†ç•ª']
        df['è¨ˆç®—_æ­£å¾ªç’°'] = df['æ­£å¾ªç’°']
        df['è¨ˆç®—_é€†å¾ªç’°'] = df['é€†å¾ªç’°']
        return df
    
    max_umaban = df.groupby(['å ´å', 'R'])['æ­£ç•ª'].transform('max')
    df['ä½¿ç”¨é ­æ•°'] = max_umaban.fillna(16).astype(int)
    if 'é ­æ•°' in df.columns:
        df['ä½¿ç”¨é ­æ•°'] = df['é ­æ•°'].fillna(df['ä½¿ç”¨é ­æ•°']).astype(int)
    
    df['ä½¿ç”¨é ­æ•°'] = np.maximum(df['ä½¿ç”¨é ­æ•°'], df['æ­£ç•ª'])
    
    def calc(row):
        t = int(row['ä½¿ç”¨é ­æ•°'])
        s = int(row['æ­£ç•ª'])
        g = (t + 1) - s
        sj = t + s
        gj = t + g
        return pd.Series([g, sj, gj])
    
    df[['è¨ˆç®—_é€†ç•ª', 'è¨ˆç®—_æ­£å¾ªç’°', 'è¨ˆç®—_é€†å¾ªç’°']] = df.apply(calc, axis=1)
    return df

def get_pair_pattern(row1, row2):
    def val(x):
        try: return int(float(x)) 
        except: return None
    r1 = [val(row1['æ­£ç•ª']), val(row1['è¨ˆç®—_é€†ç•ª']), val(row1['è¨ˆç®—_æ­£å¾ªç’°']), val(row1['è¨ˆç®—_é€†å¾ªç’°'])]
    r2 = [val(row2['æ­£ç•ª']), val(row2['è¨ˆç®—_é€†ç•ª']), val(row2['è¨ˆç®—_æ­£å¾ªç’°']), val(row2['è¨ˆç®—_é€†å¾ªç’°'])]
    label = list("ABCDEFGHIJKLMNOP")
    pairs = [label[i * 4 + j] for i in range(4) for j in range(4)
             if r1[i] is not None and r2[j] is not None and r1[i] == r2[j] and r1[i] != 0]
    return ",".join(pairs)

def get_common_values(group):
    cols = ['æ­£ç•ª', 'è¨ˆç®—_é€†ç•ª', 'è¨ˆç®—_æ­£å¾ªç’°', 'è¨ˆç®—_é€†å¾ªç’°']
    common_set = None
    for _, row in group.iterrows():
        current_set = set()
        for col in cols:
            val = row.get(col)
            if pd.notna(val):
                try:
                    num = int(float(val))
                    if num != 0: current_set.add(num)
                except: continue
        if common_set is None: common_set = current_set
        else: common_set = common_set.intersection(current_set)
        if not common_set: return None
    if common_set: return ','.join(map(str, sorted(list(common_set))))
    return None

def analyze_logic(df_curr, df_prev=None):
    df_curr = calc_haichi_numbers(df_curr)
    if df_prev is not None and not df_prev.empty:
        df_prev = calc_haichi_numbers(df_prev)
    
    rec_list = []
    
    # A. é’å¡—
    blue_keys = set()
    for col in ['é¨æ‰‹', 'å©èˆ', 'é¦¬ä¸»']:
        if col not in df_curr.columns: continue
        if col == 'é¨æ‰‹': group_keys = ['å ´å', col]
        else: group_keys = [col]
        try:
            for name_key, group in df_curr.groupby(group_keys):
                if len(group) < 2: continue
                target_name = name_key[1] if col == 'é¨æ‰‹' else name_key
                if not target_name: continue
                common_vals = get_common_values(group)
                if common_vals:
                    all_races_display = [f"{r['å ´å']}{r['R']}" for _, r in group.iterrows()]
                    priority = 0.3 if col == 'é¨æ‰‹' else (0.2 if col == 'å©èˆ' else 0.1)
                    for _, row in group.iterrows():
                        current_race_str = f"{row['å ´å']}{row['R']}"
                        other_races = [s for s in all_races_display if s != current_race_str]
                        other_races = sorted(list(set(other_races)))
                        remark = f'[{col}] å…±é€šå€¤({common_vals}) [ä»–:{",".join(other_races)}]'
                        rec_list.append({
                            'å ´å': row['å ´å'], 'R': row['R'], 'æ­£ç•ª': row['æ­£ç•ª'], 'é¦¬å': row['é¦¬å'],
                            'å±æ€§': f"{col}:{target_name}", 
                            'ã‚¿ã‚¤ãƒ—': f'â˜… {col}é’å¡—', 
                            'ãƒ‘ã‚¿ãƒ¼ãƒ³': 'Blue', 
                            'æ¡ä»¶': remark,
                            'ã‚¹ã‚³ã‚¢': 9.0 + priority
                        })
                        blue_keys.add((row['å ´å'], row['R'], row['é¦¬å'], row['å±æ€§']))
        except: continue

    # B. é’å¡—ã®éš£
    if blue_keys:
        blue_lookup = {}
        for b in blue_keys:
            key = (b[0], b[1]) 
            if key not in blue_lookup: blue_lookup[key] = []
            blue_lookup[key].append({'é¦¬å': b[2], 'å±æ€§': b[3]})

        for (place, race), group in df_curr.groupby(['å ´å', 'R']):
            key = (place, race)
            if key not in blue_lookup: continue
            blue_horses_info = blue_lookup[key]
            group = group.sort_values('æ­£ç•ª')
            umaban_map = {int(row['æ­£ç•ª']): row for _, row in group.iterrows()}
            blue_horse_names = [b['é¦¬å'] for b in blue_horses_info]

            for b_info in blue_horses_info:
                b_row = group[group['é¦¬å'] == b_info['é¦¬å']]
                if b_row.empty: continue
                b_row = b_row.iloc[0]
                curr_num = int(b_row['æ­£ç•ª'])
                source_attr = b_info['å±æ€§']
                for t_num in [curr_num - 1, curr_num + 1]:
                    if t_num in umaban_map:
                        t_row = umaban_map[t_num]
                        if t_row['é¦¬å'] not in blue_horse_names:
                            rec_list.append({
                                'å ´å': place, 'R': race, 'æ­£ç•ª': t_num, 'é¦¬å': t_row['é¦¬å'],
                                'å±æ€§': f"(é’å¡—éš£) <{source_attr}>", 
                                'ã‚¿ã‚¤ãƒ—': 'â–³ é’å¡—ã®éš£',
                                'ãƒ‘ã‚¿ãƒ¼ãƒ³': 'BlueNeighbor',
                                'æ¡ä»¶': f"é’å¡—#{curr_num}({source_attr})ã®éš£",
                                'ã‚¹ã‚³ã‚¢': 9.0
                            })

    # C. é€šå¸¸ãƒšã‚¢ (é¨æ‰‹)
    if 'é¨æ‰‹' in df_curr.columns:
        for (place, name), group in df_curr.groupby(['å ´å', 'é¨æ‰‹']):
            if len(group) < 2: continue
            group = group.sort_values('R').to_dict('records')
            for i in range(len(group)-1):
                curr, next_r = group[i], group[i+1]
                pat = get_pair_pattern(curr, next_r)
                if pat:
                    label = "â— ãƒãƒ£ãƒ³ã‚¹" if any(x in pat for x in ['C','D','G','H']) else "â—‹ ç‹™ã„ç›®"
                    base_score = 4.0 if label.startswith("â—") else 3.0
                    rec_list.append({
                        'å ´å': curr['å ´å'], 'R': curr['R'], 'æ­£ç•ª': curr['æ­£ç•ª'], 'é¦¬å': curr['é¦¬å'],
                        'å±æ€§': f"é¨æ‰‹:{name}", 'ã‚¿ã‚¤ãƒ—': label, 'ãƒ‘ã‚¿ãƒ¼ãƒ³': pat, 
                        'æ¡ä»¶': f"[é¨æ‰‹] ãƒšã‚¢({next_r['R']}R #{next_r['æ­£ç•ª']})", 'ã‚¹ã‚³ã‚¢': base_score + 0.3
                    })
                    rec_list.append({
                        'å ´å': next_r['å ´å'], 'R': next_r['R'], 'æ­£ç•ª': next_r['æ­£ç•ª'], 'é¦¬å': next_r['é¦¬å'],
                        'å±æ€§': f"é¨æ‰‹:{name}", 'ã‚¿ã‚¤ãƒ—': label, 'ãƒ‘ã‚¿ãƒ¼ãƒ³': pat, 
                        'æ¡ä»¶': f"[é¨æ‰‹] ãƒšã‚¢({curr['R']}R #{curr['æ­£ç•ª']})", 'ã‚¹ã‚³ã‚¢': base_score + 0.3
                    })

    # C. é€šå¸¸ãƒšã‚¢ (å©èˆãƒ»é¦¬ä¸»)
    for col in ['å©èˆ', 'é¦¬ä¸»']:
        if col not in df_curr.columns: continue
        for name, group in df_curr.groupby(col):
            if len(group) < 2: continue
            group = group.sort_values(['R', 'å ´å']).to_dict('records')
            for i in range(len(group)-1):
                curr, next_r = group[i], group[i+1]
                pat = get_pair_pattern(curr, next_r)
                if pat:
                    label = "â— ãƒãƒ£ãƒ³ã‚¹" if any(x in pat for x in ['C','D','G','H']) else "â—‹ ç‹™ã„ç›®"
                    base_score = 4.0 if label.startswith("â—") else 3.0
                    cond_curr = f"[{col}] ãƒšã‚¢({next_r['å ´å']}{next_r['R']}R #{next_r['æ­£ç•ª']})"
                    cond_next = f"[{col}] ãƒšã‚¢({curr['å ´å']}{curr['R']}R #{curr['æ­£ç•ª']})"
                    bonus = 0.2
                    rec_list.append({
                        'å ´å': curr['å ´å'], 'R': curr['R'], 'æ­£ç•ª': curr['æ­£ç•ª'], 'é¦¬å': curr['é¦¬å'],
                        'å±æ€§': f"{col}:{name}", 'ã‚¿ã‚¤ãƒ—': label, 'ãƒ‘ã‚¿ãƒ¼ãƒ³': pat, 
                        'æ¡ä»¶': cond_curr, 'ã‚¹ã‚³ã‚¢': base_score + bonus
                    })
                    rec_list.append({
                        'å ´å': next_r['å ´å'], 'R': next_r['R'], 'æ­£ç•ª': next_r['æ­£ç•ª'], 'é¦¬å': next_r['é¦¬å'],
                        'å±æ€§': f"{col}:{name}", 'ã‚¿ã‚¤ãƒ—': label, 'ãƒ‘ã‚¿ãƒ¼ãƒ³': pat, 
                        'æ¡ä»¶': cond_next, 'ã‚¹ã‚³ã‚¢': base_score + bonus
                    })

    # D. å‰æ—¥åŒé…ç½®
    if df_prev is not None and not df_prev.empty:
        for idx, row in df_curr.iterrows():
            race = row['R']
            name = row['é¨æ‰‹']
            if not name: continue
            prev_rows = df_prev[(df_prev['å ´å'] == row['å ´å']) & (df_prev['R'] == race) & (df_prev['é¨æ‰‹'] == name)]
            for _, p_row in prev_rows.iterrows():
                is_seiban = (p_row['æ­£ç•ª'] == row['æ­£ç•ª'])
                is_gyaku = (p_row['è¨ˆç®—_é€†ç•ª'] == row['è¨ˆç®—_é€†ç•ª'])
                if is_seiban or is_gyaku:
                    reason = "æ­£ç•ª" if is_seiban else "é€†ç•ª"
                    prev_rank = pd.to_numeric(p_row.get('ç€é †'), errors='coerce')
                    condition_text = f"[é¨æ‰‹] å‰æ—¥{race}RåŒé…ç½®({reason})"
                    if pd.notna(prev_rank):
                        if prev_rank > 3: condition_text += " <âš ï¸å‰æ—¥å‡¡èµ°>"
                        else: condition_text += " <âœ¨å‰æ—¥å¥½èµ°>"
                    
                    rec_list.append({
                        'å ´å': row['å ´å'], 'R': race, 'æ­£ç•ª': row['æ­£ç•ª'], 'é¦¬å': row['é¦¬å'],
                        'å±æ€§': f"é¨æ‰‹:{name}", 'ã‚¿ã‚¤ãƒ—': 'â˜… å‰æ—¥åŒé…ç½®', 
                        'ãƒ‘ã‚¿ãƒ¼ãƒ³': 'PrevDay',
                        'æ¡ä»¶': condition_text, 
                        'ã‚¹ã‚³ã‚¢': 8.3
                    })

    if not rec_list:
        return pd.DataFrame()
        
    res_df = pd.DataFrame(rec_list)
    
    agg_funcs = {
        'å±æ€§': lambda x: ' + '.join(sorted(set(x))),
        'ã‚¿ã‚¤ãƒ—': lambda x: ' / '.join(sorted(set(x), key=lambda s: 0 if 'â˜…' in s else 1)), 
        'ãƒ‘ã‚¿ãƒ¼ãƒ³': lambda x: ','.join(sorted(set(x))),
        'æ¡ä»¶': lambda x: ' / '.join(sorted(set(x))),
        'ã‚¹ã‚³ã‚¢': 'sum',
        'æ­£ç•ª': 'first'
    }
    
    res_df = res_df.groupby(['å ´å', 'R', 'é¦¬å'], as_index=False).agg(agg_funcs)
    res_df = res_df.sort_values(['å ´å', 'R', 'ã‚¹ã‚³ã‚¢'], ascending=[True, True, False])
    
    if 'ç€é †' not in res_df.columns: res_df['ç€é †'] = np.nan
    
    return res_df

# ==========================================
# 3. Webã‚¢ãƒ—ãƒªç”»é¢ (Streamlit)
# ==========================================

st.title("ğŸ‡ é…ç½®é¦¬åˆ¸è¡“ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ åˆ†æ")

# ã‚µã‚¤ãƒ‰ãƒãƒ¼
with st.sidebar:
    st.header("1. ãƒ‡ãƒ¼ã‚¿å…¥åŠ›")
    uploaded_file = st.file_uploader("å½“æ—¥ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ (ã¾ãŸã¯ä¿å­˜ãƒ‡ãƒ¼ã‚¿)", type=['xlsx', 'csv'])
    prev_file = st.file_uploader("å‰æ—¥ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ (ä»»æ„)", type=['xlsx', 'csv'])
    
    st.markdown("---")
    st.header("2. ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜")
    st.caption("ç€é †ã‚’å…¥åŠ›ã—ãŸçŠ¶æ…‹ã§ã“ã“ã‹ã‚‰CSVã‚’ä¿å­˜ã—ã€æ¬¡å›èª­ã¿è¾¼ã‚€ã¨ç¶šãã‹ã‚‰å†é–‹ã§ãã¾ã™ã€‚")
    
    if 'analyzed_df' in st.session_state and not st.session_state['analyzed_df'].empty:
        csv = st.session_state['analyzed_df'].to_csv(index=False).encode('utf-8-sig')
        st.download_button(
            label="ğŸ’¾ ç¾åœ¨ã®çŠ¶æ…‹ã‚’ä¿å­˜ (CSV)",
            data=csv,
            file_name="race_progress_save.csv",
            mime="text/csv"
        )
    else:
        st.button("ğŸ’¾ ç¾åœ¨ã®çŠ¶æ…‹ã‚’ä¿å­˜", disabled=True)

if uploaded_file:
    df_raw, status = load_data(uploaded_file)
    df_prev, _ = load_data(prev_file) if prev_file else (None, None)
    
    if status != "success":
        st.error(status)
    else:
        if 'analyzed_df' not in st.session_state:
            if 'ãƒ‘ã‚¿ãƒ¼ãƒ³' in df_raw.columns and 'ã‚¹ã‚³ã‚¢' in df_raw.columns:
                st.success("ğŸ“‚ ä¿å­˜ãƒ‡ãƒ¼ã‚¿ã‚’æ¤œçŸ¥ã—ã¾ã—ãŸã€‚å¾©å…ƒã—ã¾ã™ã€‚")
                result_df = df_raw
            else:
                with st.spinner('å…¨ãƒ¬ãƒ¼ã‚¹åˆ†æä¸­...'):
                    result_df = analyze_logic(df_raw, df_prev)

            if not result_df.empty:
                result_df['id'] = result_df.index
                st.session_state['analyzed_df'] = result_df
            else:
                st.session_state['analyzed_df'] = pd.DataFrame()

        if not st.session_state['analyzed_df'].empty:
            
            st.subheader("ğŸ“ çµæœå…¥åŠ› & æ¨å¥¨é¦¬ãƒªã‚¹ãƒˆ")
            st.info("é–‹å‚¬å ´ã”ã¨ã®ã‚¿ãƒ–ã‚’åˆ‡ã‚Šæ›¿ãˆã¦å…¥åŠ›ã—ã€æœ€å¾Œã«ã€Œæ›´æ–°ãƒœã‚¿ãƒ³ã€ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")
            
            full_df = st.session_state['analyzed_df'].copy()
            places = sorted(full_df['å ´å'].unique())
            display_cols = ['å ´å', 'R', 'æ­£ç•ª', 'é¦¬å', 'å±æ€§', 'ã‚¿ã‚¤ãƒ—', 'ãƒ‘ã‚¿ãƒ¼ãƒ³', 'æ¡ä»¶', 'ã‚¹ã‚³ã‚¢', 'ç€é †']
            
            with st.form("result_entry_form"):
                tabs = st.tabs(places)
                edited_dfs = [] 
                
                for tab, place in zip(tabs, places):
                    with tab:
                        valid_cols = [c for c in display_cols if c in full_df.columns]
                        place_df = full_df[full_df['å ´å'] == place][valid_cols]
                        
                        race_list = sorted(place_df['R'].unique())
                        if race_list:
                            r_tabs = st.tabs([f"{r}R" for r in race_list])
                            
                            for r_tab, r_num in zip(r_tabs, race_list):
                                with r_tab:
                                    race_data = place_df[place_df['R'] == r_num]
                                    
                                    edited_chunk = st.data_editor(
                                        race_data,
                                        column_config={
                                            "ç€é †": st.column_config.NumberColumn(
                                                "ç€é †", help="ç¢ºå®šç€é †ã‚’å…¥åŠ›", min_value=1, max_value=18, step=1, format="%d"
                                            ),
                                            "ã‚¹ã‚³ã‚¢": st.column_config.ProgressColumn(
                                                "æ³¨ç›®åº¦", format="%.1f", min_value=0, max_value=20,
                                            ),
                                        },
                                        disabled=["å ´å", "R", "é¦¬å", "æ­£ç•ª", "å±æ€§", "ã‚¿ã‚¤ãƒ—", "ãƒ‘ã‚¿ãƒ¼ãƒ³", "æ¡ä»¶", "ã‚¹ã‚³ã‚¢"],
                                        hide_index=True,
                                        use_container_width=True,
                                        height=300,
                                        key=f"editor_{place}_{r_num}"
                                    )
                                    edited_dfs.append(edited_chunk)
                
                st.markdown("---")
                submit_btn = st.form_submit_button("ğŸ”„ å…¨ã‚¿ãƒ–ã®å…¥åŠ›ã‚’ç¢ºå®šã—ã¦æ›´æ–°")

            if submit_btn:
                if edited_dfs:
                    combined_df = pd.concat(edited_dfs, ignore_index=True)
                    combined_df = combined_df.sort_values(['å ´å', 'R', 'ã‚¹ã‚³ã‚¢'], ascending=[True, True, False])
                    st.session_state['analyzed_df'] = combined_df
                    st.success("ã™ã¹ã¦ã®ãƒ‡ãƒ¼ã‚¿ã‚’æ›´æ–°ã—ã¾ã—ãŸï¼")

            # ==========================================
            # 4. é›†è¨ˆ & ã‚°ãƒ©ãƒ•
            # ==========================================
            current_df = st.session_state['analyzed_df']
            df_hits = current_df[current_df['ç€é †'].notna()].copy()
            df_hits['ç€é †'] = pd.to_numeric(df_hits['ç€é †'], errors='coerce')
            df_fuku = df_hits[df_hits['ç€é †'] <= 3] 

            st.divider()
            st.subheader("ğŸ“Š ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å‚¾å‘åˆ†æ")

            if not df_hits.empty:
                c1, c2, c3 = st.columns(3)
                with c1: st.metric("æ¶ˆåŒ–ãƒ¬ãƒ¼ã‚¹", len(df_hits['R'].unique()))
                with c2: 
                    rate = len(df_fuku)/len(df_hits)*100 if len(df_hits)>0 else 0
                    st.metric("æ¨å¥¨é¦¬ è¤‡å‹ç‡", f"{rate:.1f}%")
                with c3: st.metric("çš„ä¸­æ•°", f"{len(df_fuku)} é ­")

                graph_places = sorted(df_hits['å ´å'].unique())
                if graph_places:
                    g_tabs = st.tabs(graph_places)
                    for g_tab, place in zip(g_tabs, graph_places):
                        with g_tab:
                            col_g1, col_g2 = st.columns([1, 1])
                            place_hits = df_hits[df_hits['å ´å'] == place]
                            place_fuku = df_fuku[df_fuku['å ´å'] == place]
                            
                            if not place_fuku.empty:
                                all_patterns = []
                                for p in place_fuku['ãƒ‘ã‚¿ãƒ¼ãƒ³']:
                                    if p: all_patterns.extend(str(p).split(','))
                                
                                if all_patterns:
                                    pat_counts = pd.Series(all_patterns).value_counts().reset_index()
                                    pat_counts.columns = ['ãƒ‘ã‚¿ãƒ¼ãƒ³', 'çš„ä¸­æ•°']
                                    with col_g1:
                                        fig = px.pie(pat_counts, values='çš„ä¸­æ•°', names='ãƒ‘ã‚¿ãƒ¼ãƒ³', 
                                                     title=f'ã€{place}ã€‘ çš„ä¸­ãƒ‘ã‚¿ãƒ¼ãƒ³', hole=0.4)
                                        st.plotly_chart(fig, use_container_width=True)
                                else:
                                    with col_g1: st.info("ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ‡ãƒ¼ã‚¿ãªã—")
                            else:
                                with col_g1: st.info("çš„ä¸­ãƒ‡ãƒ¼ã‚¿ã¯ã‚ã‚Šã¾ã›ã‚“")
                            
                            with col_g2:
                                st.write(f"**{place} ã®çµæœä¸€è¦§**")
                                place_hits_disp = place_hits.copy()
                                place_hits_disp['é¦¬å'] = place_hits_disp.apply(
                                    lambda x: f":blue[**{x['é¦¬å']}**]" if 'Blue' in str(x['ãƒ‘ã‚¿ãƒ¼ãƒ³']) else x['é¦¬å'], 
                                    axis=1
                                )
                                st.dataframe(place_hits_disp[['R', 'é¦¬å', 'å±æ€§', 'ã‚¿ã‚¤ãƒ—', 'ç€é †']], use_container_width=True, hide_index=True)

                # --- å‚¾å‘ã‚¹ã‚³ã‚¢åŠ ç®— & æ¬¡ãƒ¬ãƒ¼ã‚¹è¡¨ç¤º & è²·ã„ç›® ---
                st.markdown("### ğŸ“ˆ æ¬¡ãƒ¬ãƒ¼ã‚¹ã®æ³¨ç›®é¦¬ãƒ»æ¨å¥¨è²·ã„ç›®")
                
                downgraded_attrs = set()
                hit_patterns = set()
                
                if not df_fuku.empty:
                    for _, row in df_fuku.iterrows():
                        pats = str(row['ãƒ‘ã‚¿ãƒ¼ãƒ³']).split(',')
                        hit_patterns.update(pats)
                        if 'BlueNeighbor' in str(row['ãƒ‘ã‚¿ãƒ¼ãƒ³']):
                            found = re.findall(r'<(.*?)>', str(row['å±æ€§']))
                            downgraded_attrs.update(found)
                
                if downgraded_attrs:
                    st.warning(f"âš ï¸ ä»¥ä¸‹ã®å±æ€§ã§ã€Œé’å¡—ã®éš£ã€ãŒå¥½èµ°ã—ã¾ã—ãŸã€‚è©²å½“ã™ã‚‹é’å¡—æœ¬å‘½é¦¬ã®è©•ä¾¡ã‚’ä¸‹ã’ã¾ã™: {', '.join(downgraded_attrs)}")

                future_races = current_df[current_df['ç€é †'].isna()].copy()
                
                if not future_races.empty:
                    def calc_bonus(row):
                        row_pat = row.get('ãƒ‘ã‚¿ãƒ¼ãƒ³', '')
                        if not row_pat or pd.isna(row_pat): return 0.0
                        pats = str(row_pat).split(',')
                        bonus = 0.0
                        for p in pats:
                            if p in hit_patterns and len(p) == 1: 
                                bonus += 2.0 
                        if 'Blue' in pats:
                            my_attrs = str(row.get('å±æ€§', ''))
                            for bad_attr in downgraded_attrs:
                                if bad_attr in my_attrs:
                                    bonus -= 3.0
                                    break
                        return bonus

                    def get_bet_recommendation(row):
                        score = row['ç·åˆã‚¹ã‚³ã‚¢']
                        pat_str = str(row.get('ãƒ‘ã‚¿ãƒ¼ãƒ³', ''))
                        my_pats = pat_str.split(',')
                        matched = [p for p in my_pats if p in hit_patterns]
                        is_trend_horse = len(matched) > 0
                        is_blue = 'Blue' in my_pats

                        if score >= 13: rank = "S"
                        elif score >= 11: rank = "A"
                        elif score >= 9: rank = "B"
                        elif is_blue: rank = "C"
                        else: rank = "D"

                        if rank == "S":
                            return "ğŸš€ é‰„æ¿ç´š" if is_trend_horse else "â— ä¸å‹•è»¸"
                        elif rank == "A":
                            return "âœ¨ å‚¾å‘è»¸" if is_trend_horse else "â— è»¸å€™è£œ"
                        elif rank == "B":
                            return "ğŸ”¥ æ¿€ç†±ç›¸æ‰‹" if is_trend_horse else "â—‹ ç›¸æ‰‹ç­†é ­"
                        elif rank == "C":
                            return "â˜… å‚¾å‘åˆè‡´ç©´" if is_trend_horse else "â–² é’å¡—ç©´"
                        else: 
                            if is_trend_horse: return "æ³¨ å‚¾å‘åˆè‡´"
                            return "â–³ ç´"

                    future_races['å‚¾å‘åŠ ç‚¹'] = future_races.apply(calc_bonus, axis=1)
                    future_races['ç·åˆã‚¹ã‚³ã‚¢'] = future_races['ã‚¹ã‚³ã‚¢'] + future_races['å‚¾å‘åŠ ç‚¹']
                    future_races['æ¨å¥¨è²·ã„ç›®'] = future_races.apply(get_bet_recommendation, axis=1)
                    
                    future_places = sorted(future_races['å ´å'].unique())
                    if future_places:
                        f_tabs = st.tabs(future_places)
                        
                        for tab, place in zip(f_tabs, future_places):
                            with tab:
                                place_future = future_races[future_races['å ´å'] == place]
                                if not place_future.empty:
                                    future_r_list = sorted(place_future['R'].unique())
                                    r_tabs = st.tabs([f"{r}R" for r in future_r_list])
                                    
                                    for r_tab, r_num in zip(r_tabs, future_r_list):
                                        with r_tab:
                                            target_df = place_future[place_future['R'] == r_num]
                                            target_df = target_df.sort_values('ç·åˆã‚¹ã‚³ã‚¢', ascending=False)
                                            
                                            target_df['é¦¬å'] = target_df.apply(
                                                lambda x: f":blue[**{x['é¦¬å']}**]" if 'Blue' in str(x['ãƒ‘ã‚¿ãƒ¼ãƒ³']) else x['é¦¬å'], 
                                                axis=1
                                            )
                                            
                                            top_horses = target_df.head(3)
                                            if len(top_horses) >= 2:
                                                h1 = top_horses.iloc[0]
                                                h2 = top_horses.iloc[1]
                                                h1_score = h1['ç·åˆã‚¹ã‚³ã‚¢']
                                                h2_score = h2['ç·åˆã‚¹ã‚³ã‚¢']
                                                h1_name = str(h1['é¦¬å']).replace(':blue[**', '').replace('**]', '')
                                                
                                                if h1_score >= 12 and h2_score >= 9:
                                                    st.success(f"ğŸ”¥ **{r_num}R å‹è² ãƒ¬ãƒ¼ã‚¹**: {h1['æ­£ç•ª']} - {h2['æ­£ç•ª']} (ãƒ¯ã‚¤ãƒ‰ãƒ»é¦¬é€£)")
                                                elif h1_score >= 12:
                                                    st.info(f"ğŸ’¡ **{r_num}R å˜è¤‡æ¨å¥¨**: {h1['æ­£ç•ª']} ({h1_name})")
                                                else:
                                                    st.caption(f"ğŸ² {r_num}R ã¯æ··æˆ¦æ¨¡æ§˜ã§ã™ã€‚")
                                            
                                            st.dataframe(
                                                target_df[['R', 'é¦¬å', 'ã‚¿ã‚¤ãƒ—', 'ãƒ‘ã‚¿ãƒ¼ãƒ³', 'ã‚¹ã‚³ã‚¢', 'å‚¾å‘åŠ ç‚¹', 'ç·åˆã‚¹ã‚³ã‚¢', 'æ¨å¥¨è²·ã„ç›®']],
                                                use_container_width=True,
                                                hide_index=True
                                            )
                                else:
                                    st.info("æ®‹ã‚Šãƒ¬ãƒ¼ã‚¹ã¯ã‚ã‚Šã¾ã›ã‚“")
                    else:
                        st.info("å…¨ã¦ã®ãƒ¬ãƒ¼ã‚¹ãŒçµ‚äº†ã—ã¾ã—ãŸã€‚")
                else:
                    st.info("å…¨ã¦ã®ãƒ¬ãƒ¼ã‚¹ãŒçµ‚äº†ã—ã¾ã—ãŸã€‚")
            else:
                st.info("ã¾ã ç€é †ãŒå…¥åŠ›ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚çµæœã‚’å…¥åŠ›ã—ã¦æ›´æ–°ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")
