import streamlit as st
import pandas as pd
import numpy as np
import re
import plotly.express as px
import openpyxl
import requests

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(page_title="é…ç½®é¦¬åˆ¸è¡“ Web", layout="wide")

# ==========================================
# 1. å…±é€šãƒ­ã‚¸ãƒƒã‚¯ & ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
# ==========================================

def to_half_width(text):
    if isinstance(text, (list, pd.Series, np.ndarray)):
        text = str(text)
    if pd.isna(text): return text
    text = str(text)
    table = str.maketrans('ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™', '0123456789')
    text = text.translate(table)
    text = re.sub(r'[^\d\.]', '', text)
    return text

def normalize_name(x):
    if pd.isna(x): return ''
    normalized_name = str(x).strip().replace('ã€€', '').replace(' ', '')
    normalized_name = re.sub(r'[â˜…â˜†â–²â–³â—‡]', '', normalized_name)
    if ',' in normalized_name: normalized_name = normalized_name.split(',')[0]
    text = re.sub(r'[0-9\.]+[Rr]', '', normalized_name)
    text = re.sub(r'\(.*?\)', '', text)
    return text.replace('/', '').strip()

@st.cache_data
def load_data(file):
    """ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã‚’è¡Œã†"""
    df = None
    if file.name.endswith('.xlsx'):
        try:
            file.seek(0)
            df = pd.read_excel(file, engine='openpyxl')
        except Exception as e:
            return pd.DataFrame(), f"Excelèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}"
    else:
        try:
            file.seek(0)
            df = pd.read_csv(file, encoding='utf-8', on_bad_lines='skip')
        except UnicodeDecodeError:
            try:
                file.seek(0)
                df = pd.read_csv(file, encoding='cp932', on_bad_lines='skip')
            except Exception as e:
                return pd.DataFrame(), f"CSVèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}"
        except Exception as e:
            return pd.DataFrame(), f"CSVäºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼: {e}"

    # ãƒ‡ãƒ¼ã‚¿æ•´å½¢
    df.columns = df.columns.str.strip()
    rename_map = {
        'å ´æ‰€': 'å ´å', 'é–‹å‚¬': 'å ´å', 
        'èª¿æ•™å¸«': 'å©èˆ', 'èª¿æ•™å¸«å': 'å©èˆ', 'å©èˆå': 'å©èˆ',
        'é¨æ‰‹å': 'é¨æ‰‹',
        'ãƒ¬ãƒ¼ã‚¹': 'R', 'ï¼²': 'R', 'ãƒ¬ãƒ¼ã‚¹å': 'R',
        'ç€': 'ç€é †', 'ç€ é †': 'ç€é †', 'ç•ª': 'æ­£ç•ª', 'é¦¬ç•ª': 'æ­£ç•ª',
        'å˜ã‚ªãƒƒã‚º': 'å˜ï½µï½¯ï½½ï¾', 'å˜å‹ã‚ªãƒƒã‚º': 'å˜ï½µï½¯ï½½ï¾', 'ã‚ªãƒƒã‚º': 'å˜ï½µï½¯ï½½ï¾', 'å˜å‹': 'å˜ï½µï½¯ï½½ï¾', 'å˜': 'å˜ï½µï½¯ï½½ï¾'
    }
    df = df.rename(columns=rename_map)
    df = df.loc[:, ~df.columns.duplicated()]

    if 'å ´å' not in df.columns: df['å ´å'] = 'Unknown'

    target_numeric_cols = ['R', 'æ­£ç•ª', 'å˜ï½µï½¯ï½½ï¾', 'é€†ç•ª', 'æ­£å¾ªç’°', 'é€†å¾ªç’°', 'é ­æ•°']
    for col in target_numeric_cols:
        if col in df.columns:
            df[col] = df[col].apply(to_half_width)
            df[col] = pd.to_numeric(df[col], errors='coerce')

    if 'R' not in df.columns or 'æ­£ç•ª' not in df.columns:
        return pd.DataFrame(), "ã‚¨ãƒ©ãƒ¼: å¿…é ˆåˆ—ï¼ˆãƒ¬ãƒ¼ã‚¹å/Rã€é¦¬ç•ª/æ­£ç•ªï¼‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"

    df = df.dropna(subset=['R', 'æ­£ç•ª'])
    df['R'] = df['R'].astype(int)
    df['æ­£ç•ª'] = df['æ­£ç•ª'].astype(int)

    for col in ['é¨æ‰‹', 'å©èˆ', 'é¦¬ä¸»']:
        if col in df.columns:
            df[col] = df[col].apply(normalize_name)
        else:
            df[col] = ''
            
    required_cols = ['R', 'å ´å', 'é¦¬å', 'æ­£ç•ª', 'é¨æ‰‹', 'å©èˆ', 'é¦¬ä¸»', 'å˜ï½µï½¯ï½½ï¾', 'é€†ç•ª', 'æ­£å¾ªç’°', 'é€†å¾ªç’°', 'é ­æ•°']
    for col in required_cols:
        if col not in df.columns:
            df[col] = np.nan

    save_cols = ['å±æ€§', 'ã‚¿ã‚¤ãƒ—', 'ãƒ‘ã‚¿ãƒ¼ãƒ³', 'æ¡ä»¶', 'ã‚¹ã‚³ã‚¢', 'ç€é †', 'å‚¾å‘åŠ ç‚¹', 'ç·åˆã‚¹ã‚³ã‚¢']
    existing_save_cols = [c for c in save_cols if c in df.columns]
    
    return df[required_cols + existing_save_cols].copy(), "success"

# â˜…ä¿®æ­£: åˆ—åã®ãƒ•ãƒ©ãƒƒãƒˆåŒ–ã‚’è¡Œã„ã€KeyErrorã‚’å®Œå…¨ã«é˜²ãé–¢æ•°
def fetch_odds_from_web(url, force_mode=False):
    
    def try_fetch(target_url):
        try:
            # User-Agentå½è£…
            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
            }
            response = requests.get(target_url, headers=headers, timeout=15)
            response.raise_for_status()
            
            if "netkeiba" in target_url:
                response.encoding = 'euc-jp'
            else:
                response.encoding = response.apparent_encoding

            dfs = []
            for parser in ['bs4', 'lxml', 'html5lib']:
                try:
                    dfs = pd.read_html(response.text, flavor=parser)
                    if dfs: break
                except: continue
            
            if not dfs: return None, "No tables found"

            target_df = None
            debug_logs = []

            for i, df in enumerate(dfs):
                # â˜…é‡è¦: åˆ—åã®å¼·åˆ¶ãƒ•ãƒ©ãƒƒãƒˆåŒ– (MultiIndexè§£æ¶ˆ)
                # ã“ã‚Œã«ã‚ˆã‚Šã€df.columns ã¯å¿…ãšå˜ç´”ãªæ–‡å­—åˆ—ã®ãƒªã‚¹ãƒˆã«ãªã‚Šã¾ã™
                flat_cols = []
                for c in df.columns:
                    if isinstance(c, tuple):
                        # ã‚¿ãƒ—ãƒ«ã®ä¸­èº«ã‚’çµåˆã—ã€ã‚´ãƒŸã‚’é™¤å»
                        col_str = ''.join([str(x) for x in c if 'Unnamed' not in str(x)])
                    else:
                        col_str = str(c)
                    # ã‚¹ãƒšãƒ¼ã‚¹ã‚„æ”¹è¡Œã‚’å‰Šé™¤
                    col_str = col_str.replace(' ', '').replace('ã€€', '').replace('\n', '').replace('\r', '')
                    flat_cols.append(col_str)
                
                # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®åˆ—åã‚’ä¸Šæ›¸ã
                df.columns = flat_cols
                
                debug_logs.append(f"Table {i} Cols: {flat_cols}")

                # --- åˆ¤å®šãƒ­ã‚¸ãƒƒã‚¯ ---
                has_umaban = any('é¦¬ç•ª' in c for c in flat_cols)
                has_odds_related = any(x in c for c in flat_cols for x in ['å˜å‹', 'ã‚ªãƒƒã‚º', 'äººæ°—', 'äºˆæƒ³'])

                if has_umaban and has_odds_related:
                    # renameç”¨ã®ãƒãƒƒãƒ”ãƒ³ã‚°è¾æ›¸ä½œæˆ
                    # ã“ã“ã§ã¯ã‚‚ã†åˆ—åã¯å˜ç´”ãªæ–‡å­—åˆ—ã«ãªã£ã¦ã„ã‚‹ã®ã§ã€KeyErrorã¯èµ·ããªã„
                    rename_dict = {}
                    
                    for col_name in flat_cols:
                        if 'é¦¬ç•ª' in col_name:
                            rename_dict[col_name] = 'æ­£ç•ª'
                        
                        elif 'å˜å‹' in col_name: rename_dict[col_name] = 'å˜ï½µï½¯ï½½ï¾'
                        elif 'äºˆæƒ³ã‚ªãƒƒã‚º' in col_name: rename_dict[col_name] = 'å˜ï½µï½¯ï½½ï¾'
                        elif 'ã‚ªãƒƒã‚º' in col_name and 'å˜ï½µï½¯ï½½ï¾' not in rename_dict.values(): rename_dict[col_name] = 'å˜ï½µï½¯ï½½ï¾'
                        elif 'äººæ°—' in col_name: rename_dict[col_name] = 'äººæ°—_temp'

                    # ãƒªãƒãƒ¼ãƒ å®Ÿè¡Œ
                    df = df.rename(columns=rename_dict)

                    # å¿…é ˆåˆ—ãŒã‚ã‚‹ã‹ç¢ºèª
                    if 'æ­£ç•ª' in df.columns:
                        # ã‚ªãƒƒã‚ºåˆ—ãŒãªã„å ´åˆã€äººæ°—åˆ—ã§ä»£ç”¨ã‚’è©¦ã¿ã‚‹
                        if 'å˜ï½µï½¯ï½½ï¾' not in df.columns and 'äººæ°—_temp' in df.columns:
                            df = df.rename(columns={'äººæ°—_temp': 'å˜ï½µï½¯ï½½ï¾'})
                        
                        if 'å˜ï½µï½¯ï½½ï¾' in df.columns:
                            target_df = df
                            break
            
            if target_df is not None:
                # å¿…è¦ãªåˆ—ã ã‘æŠ½å‡º (ç¢ºå®Ÿã«å­˜åœ¨ã™ã‚‹ã“ã¨ã‚’ç¢ºèªæ¸ˆã¿)
                res = target_df[['æ­£ç•ª', 'å˜ï½µï½¯ï½½ï¾']].copy()
                
                res['æ­£ç•ª'] = pd.to_numeric(res['æ­£ç•ª'], errors='coerce')
                
                def clean_odds(x):
                    s = str(x).strip()
                    if s in ['--', '---', 'å–æ¶ˆ', 'é™¤å¤–', 'nan', 'NaN', 'None']:
                        return np.nan
                    s = re.sub(r'\(.*?\)', '', s) 
                    try: return float(s)
                    except: return np.nan
                
                res['å˜ï½µï½¯ï½½ï¾'] = res['å˜ï½µï½¯ï½½ï¾'].apply(clean_odds)
                res = res.dropna(subset=['æ­£ç•ª'])
                
                if res['å˜ï½µï½¯ï½½ï¾'].isna().all():
                    st.toast("âš ï¸ ã‚ªãƒƒã‚ºæ•°å€¤ãªã—ã€‚é¦¬ç•ªã®ã¿èª­ã¿è¾¼ã¿ã¾ã™ã€‚", icon="â„¹ï¸")
                    return res, "NaN Warning"
                    
                return res, "Success"
            
            return None, debug_logs
            
        except Exception as e:
            return None, str(e)

    # --- ãƒ¡ã‚¤ãƒ³å‡¦ç† ---
    target_url = url
    if not force_mode:
        if "sp.netkeiba.com" in target_url:
            target_url = target_url.replace("sp.netkeiba.com", "race.netkeiba.com")
        if "race.netkeiba.com" in target_url and "shutuba.html" in target_url:
            target_url = target_url.replace("shutuba.html", "odds.html")
    
    result_df, msg = try_fetch(target_url)
    
    if result_df is None or result_df.empty:
        if target_url != url:
            fallback_url = url
            if "sp.netkeiba.com" in fallback_url:
                fallback_url = fallback_url.replace("sp.netkeiba.com", "race.netkeiba.com")
            
            result_df, msg_fallback = try_fetch(fallback_url)
            
            if result_df is None:
                st.error("âŒ å–å¾—å¤±æ•—")
                with st.expander("ğŸ” è©³ç´°ãƒ­ã‚°"):
                    st.write(f"1: {msg}")
                    st.write(f"2: {msg_fallback}")
                return None, "Failed"
        else:
            st.error("âŒ å–å¾—å¤±æ•—")
            with st.expander("ğŸ” è©³ç´°ãƒ­ã‚°"):
                st.write(msg)
            return None, "Failed"
            
    return result_df, "Success"

# ==========================================
# 2. é…ç½®è¨ˆç®—ãƒ»åˆ†æãƒ­ã‚¸ãƒƒã‚¯
# ==========================================

def calc_haichi_numbers(df):
    check_cols = ['é€†ç•ª', 'æ­£å¾ªç’°', 'é€†å¾ªç’°']
    if set(check_cols).issubset(df.columns) and df[check_cols].notna().all().all():
        df['è¨ˆç®—_é€†ç•ª'] = df['é€†ç•ª']
        df['è¨ˆç®—_æ­£å¾ªç’°'] = df['æ­£å¾ªç’°']
        df['è¨ˆç®—_é€†å¾ªç’°'] = df['é€†å¾ªç’°']
        return df
    
    max_umaban = df.groupby(['å ´å', 'R'])['æ­£ç•ª'].transform('max')
    df['ä½¿ç”¨é ­æ•°'] = max_umaban.fillna(16).astype(int)
    if 'é ­æ•°' in df.columns:
        df['ä½¿ç”¨é ­æ•°'] = df['é ­æ•°'].fillna(df['ä½¿ç”¨é ­æ•°']).astype(int)
    df['ä½¿ç”¨é ­æ•°'] = np.maximum(df['ä½¿ç”¨é ­æ•°'], df['æ­£ç•ª'])
    
    def calc(row):
        t = int(row['ä½¿ç”¨é ­æ•°'])
        s = int(row['æ­£ç•ª'])
        g = (t + 1) - s
        sj = t + s
        gj = t + g
        return pd.Series([g, sj, gj])
    
    df[['è¨ˆç®—_é€†ç•ª', 'è¨ˆç®—_æ­£å¾ªç’°', 'è¨ˆç®—_é€†å¾ªç’°']] = df.apply(calc, axis=1)
    return df

def get_pair_pattern(row1, row2):
    def val(x):
        try: return int(float(x)) 
        except: return None
    r1 = [val(row1['æ­£ç•ª']), val(row1['è¨ˆç®—_é€†ç•ª']), val(row1['è¨ˆç®—_æ­£å¾ªç’°']), val(row1['è¨ˆç®—_é€†å¾ªç’°'])]
    r2 = [val(row2['æ­£ç•ª']), val(row2['è¨ˆç®—_é€†ç•ª']), val(row2['è¨ˆç®—_æ­£å¾ªç’°']), val(row2['è¨ˆç®—_é€†å¾ªç’°'])]
    label = list("ABCDEFGHIJKLMNOP")
    pairs = [label[i * 4 + j] for i in range(4) for j in range(4)
             if r1[i] is not None and r2[j] is not None and r1[i] == r2[j] and r1[i] != 0]
    return ",".join(pairs)

def get_common_values(group):
    cols = ['æ­£ç•ª', 'è¨ˆç®—_é€†ç•ª', 'è¨ˆç®—_æ­£å¾ªç’°', 'è¨ˆç®—_é€†å¾ªç’°']
    common_set = None
    for _, row in group.iterrows():
        current_set = set()
        for col in cols:
            val = row.get(col)
            if pd.notna(val):
                try:
                    num = int(float(val))
                    if num != 0: current_set.add(num)
                except: continue
        if common_set is None: common_set = current_set
        else: common_set = common_set.intersection(current_set)
        if not common_set: return None
    if common_set: return ','.join(map(str, sorted(list(common_set))))
    return None

def analyze_logic(df_curr, df_prev=None):
    df_curr = calc_haichi_numbers(df_curr)
    if df_prev is not None and not df_prev.empty:
        df_prev = calc_haichi_numbers(df_prev)
    
    rec_list = []
    
    # A. é’å¡—
    blue_keys = set()
    for col in ['é¨æ‰‹', 'å©èˆ', 'é¦¬ä¸»']:
        if col not in df_curr.columns: continue
        if col == 'é¨æ‰‹': group_keys = ['å ´å', col]
        else: group_keys = [col]
        try:
            for name_key, group in df_curr.groupby(group_keys):
                if len(group) < 2: continue
                target_name = name_key[1] if col == 'é¨æ‰‹' else name_key
                if not target_name: continue
                common_vals = get_common_values(group)
                if common_vals:
                    all_races_display = [f"{r['å ´å']}{r['R']}" for _, r in group.iterrows()]
                    priority = 1.0 if col == 'é¨æ‰‹' else 0.2
                    for _, row in group.iterrows():
                        current_race_str = f"{row['å ´å']}{row['R']}"
                        other_races = [s for s in all_races_display if s != current_race_str]
                        other_races = sorted(list(set(other_races)))
                        remark = f'[{col}] å…±é€šå€¤({common_vals}) [ä»–:{",".join(other_races)}]'
                        odds_val = row.get('å˜ï½µï½¯ï½½ï¾', np.nan)
                        rec_list.append({
                            'å ´å': row['å ´å'], 'R': row['R'], 'æ­£ç•ª': row['æ­£ç•ª'], 'é¦¬å': row['é¦¬å'],
                            'å˜ï½µï½¯ï½½ï¾': odds_val,
                            'å±æ€§': f"{col}:{target_name}", 
                            'ã‚¿ã‚¤ãƒ—': f'â˜… {col}é’å¡—', 
                            'ãƒ‘ã‚¿ãƒ¼ãƒ³': 'é’', 
                            'æ¡ä»¶': remark,
                            'ã‚¹ã‚³ã‚¢': 9.0 + priority
                        })
                        blue_keys.add((row['å ´å'], row['R'], row['é¦¬å'], row['å±æ€§']))
        except: continue

    # B. é’å¡—ã®éš£
    if blue_keys:
        blue_lookup = {}
        for b in blue_keys:
            key = (b[0], b[1]) 
            if key not in blue_lookup: blue_lookup[key] = []
            blue_lookup[key].append({'é¦¬å': b[2], 'å±æ€§': b[3]})

        for (place, race), group in df_curr.groupby(['å ´å', 'R']):
            key = (place, race)
            if key not in blue_lookup: continue
            blue_horses_info = blue_lookup[key]
            group = group.sort_values('æ­£ç•ª')
            umaban_map = {int(row['æ­£ç•ª']): row for _, row in group.iterrows()}
            blue_horse_names = [b['é¦¬å'] for b in blue_horses_info]

            for b_info in blue_horses_info:
                b_row = group[group['é¦¬å'] == b_info['é¦¬å']]
                if b_row.empty: continue
                b_row = b_row.iloc[0]
                curr_num = int(b_row['æ­£ç•ª'])
                source_attr = b_info['å±æ€§']
                blue_odds = pd.to_numeric(b_row.get('å˜ï½µï½¯ï½½ï¾'), errors='coerce')
                
                for t_num in [curr_num - 1, curr_num + 1]:
                    if t_num in umaban_map:
                        t_row = umaban_map[t_num]
                        if t_row['é¦¬å'] not in blue_horse_names:
                            neighbor_odds = pd.to_numeric(t_row.get('å˜ï½µï½¯ï½½ï¾'), errors='coerce')
                            neighbor_score = 9.0
                            
                            # éš£ã®ã‚ªãƒƒã‚º < æœ¬ä½“ã®ã‚ªãƒƒã‚º ãªã‚‰ã‚¹ã‚³ã‚¢åŠ ç®— (é€†è»¢)
                            if pd.notna(blue_odds) and pd.notna(neighbor_odds):
                                if neighbor_odds < blue_odds:
                                    neighbor_score += 2.0
                            
                            rec_list.append({
                                'å ´å': place, 'R': race, 'æ­£ç•ª': t_num, 'é¦¬å': t_row['é¦¬å'],
                                'å˜ï½µï½¯ï½½ï¾': neighbor_odds,
                                'å±æ€§': f"(é’å¡—éš£) <{source_attr}>", 
                                'ã‚¿ã‚¤ãƒ—': 'â–³ é’å¡—ã®éš£',
                                'ãƒ‘ã‚¿ãƒ¼ãƒ³': 'é’éš£',
                                'æ¡ä»¶': f"é’å¡—#{curr_num}({source_attr})ã®éš£",
                                'ã‚¹ã‚³ã‚¢': neighbor_score
                            })

    # C. é€šå¸¸ãƒšã‚¢ (é¨æ‰‹)
    if 'é¨æ‰‹' in df_curr.columns:
        for (place, name), group in df_curr.groupby(['å ´å', 'é¨æ‰‹']):
            if len(group) < 2: continue
            group = group.sort_values('R').to_dict('records')
            for i in range(len(group)-1):
                curr, next_r = group[i], group[i+1]
                pat = get_pair_pattern(curr, next_r)
                if pat:
                    label = "â— ãƒãƒ£ãƒ³ã‚¹" if any(x in pat for x in ['C','D','G','H']) else "â—‹ ç‹™ã„ç›®"
                    base_score = 4.0 if label.startswith("â—") else 3.0
                    rec_list.append({
                        'å ´å': curr['å ´å'], 'R': curr['R'], 'æ­£ç•ª': curr['æ­£ç•ª'], 'é¦¬å': curr['é¦¬å'],
                        'å˜ï½µï½¯ï½½ï¾': curr.get('å˜ï½µï½¯ï½½ï¾', np.nan),
                        'å±æ€§': f"é¨æ‰‹:{name}", 'ã‚¿ã‚¤ãƒ—': label, 'ãƒ‘ã‚¿ãƒ¼ãƒ³': pat, 
                        'æ¡ä»¶': f"[é¨æ‰‹] ãƒšã‚¢({next_r['R']}R #{next_r['æ­£ç•ª']})", 'ã‚¹ã‚³ã‚¢': base_score + 1.0
                    })
                    rec_list.append({
                        'å ´å': next_r['å ´å'], 'R': next_r['R'], 'æ­£ç•ª': next_r['æ­£ç•ª'], 'é¦¬å': next_r['é¦¬å'],
                        'å˜ï½µï½¯ï½½ï¾': next_r.get('å˜ï½µï½¯ï½½ï¾', np.nan),
                        'å±æ€§': f"é¨æ‰‹:{name}", 'ã‚¿ã‚¤ãƒ—': label, 'ãƒ‘ã‚¿ãƒ¼ãƒ³': pat, 
                        'æ¡ä»¶': f"[é¨æ‰‹] ãƒšã‚¢({curr['R']}R #{curr['æ­£ç•ª']})", 'ã‚¹ã‚³ã‚¢': base_score + 1.0
                    })

    # C. é€šå¸¸ãƒšã‚¢ (å©èˆãƒ»é¦¬ä¸»)
    for col in ['å©èˆ', 'é¦¬ä¸»']:
        if col not in df_curr.columns: continue
        for name, group in df_curr.groupby(col):
            if len(group) < 2: continue
            group = group.sort_values(['R', 'å ´å']).to_dict('records')
            for i in range(len(group)-1):
                curr, next_r = group[i], group[i+1]
                pat = get_pair_pattern(curr, next_r)
                if pat:
                    label = "â— ãƒãƒ£ãƒ³ã‚¹" if any(x in pat for x in ['C','D','G','H']) else "â—‹ ç‹™ã„ç›®"
                    base_score = 4.0 if label.startswith("â—") else 3.0
                    cond_curr = f"[{col}] ãƒšã‚¢({next_r['å ´å']}{next_r['R']}R #{next_r['æ­£ç•ª']})"
                    cond_next = f"[{col}] ãƒšã‚¢({curr['å ´å']}{curr['R']}R #{curr['æ­£ç•ª']})"
                    bonus = 0.2
                    rec_list.append({
                        'å ´å': curr['å ´å'], 'R': curr['R'], 'æ­£ç•ª': curr['æ­£ç•ª'], 'é¦¬å': curr['é¦¬å'],
                        'å˜ï½µï½¯ï½½ï¾': curr.get('å˜ï½µï½¯ï½½ï¾', np.nan),
                        'å±æ€§': f"{col}:{name}", 'ã‚¿ã‚¤ãƒ—': label, 'ãƒ‘ã‚¿ãƒ¼ãƒ³': pat, 
                        'æ¡ä»¶': cond_curr, 'ã‚¹ã‚³ã‚¢': base_score + bonus
                    })
                    rec_list.append({
                        'å ´å': next_r['å ´å'], 'R': next_r['R'], 'æ­£ç•ª': next_r['æ­£ç•ª'], 'é¦¬å': next_r['é¦¬å'],
                        'å˜ï½µï½¯ï½½ï¾': next_r.get('å˜ï½µï½¯ï½½ï¾', np.nan),
                        'å±æ€§': f"{col}:{name}", 'ã‚¿ã‚¤ãƒ—': label, 'ãƒ‘ã‚¿ãƒ¼ãƒ³': pat, 
                        'æ¡ä»¶': cond_next, 'ã‚¹ã‚³ã‚¢': base_score + bonus
                    })

    # D. å‰æ—¥åŒé…ç½®
    if df_prev is not None and not df_prev.empty:
        for idx, row in df_curr.iterrows():
            race = row['R']
            name = row['é¨æ‰‹']
            if not name: continue
            prev_rows = df_prev[(df_prev['å ´å'] == row['å ´å']) & (df_prev['R'] == race) & (df_prev['é¨æ‰‹'] == name)]
            for _, p_row in prev_rows.iterrows():
                is_seiban = (p_row['æ­£ç•ª'] == row['æ­£ç•ª'])
                is_gyaku = (p_row['è¨ˆç®—_é€†ç•ª'] == row['è¨ˆç®—_é€†ç•ª'])
                if is_seiban or is_gyaku:
                    reason = "æ­£ç•ª" if is_seiban else "é€†ç•ª"
                    prev_rank = pd.to_numeric(p_row.get('ç€é †'), errors='coerce')
                    condition_text = f"[é¨æ‰‹] å‰æ—¥{race}RåŒé…ç½®({reason})"
                    if pd.notna(prev_rank):
                        if prev_rank > 3: condition_text += " <âš ï¸å‰æ—¥å‡¡èµ°>"
                        else: condition_text += " <âœ¨å‰æ—¥å¥½èµ°>"
                    
                    rec_list.append({
                        'å ´å': row['å ´å'], 'R': race, 'æ­£ç•ª': row['æ­£ç•ª'], 'é¦¬å': row['é¦¬å'],
                        'å˜ï½µï½¯ï½½ï¾': row.get('å˜ï½µï½¯ï½½ï¾', np.nan),
                        'å±æ€§': f"é¨æ‰‹:{name}", 'ã‚¿ã‚¤ãƒ—': 'â˜… å‰æ—¥åŒé…ç½®', 
                        'ãƒ‘ã‚¿ãƒ¼ãƒ³': 'å‰æ—¥',
                        'æ¡ä»¶': condition_text, 
                        'ã‚¹ã‚³ã‚¢': 8.3
                    })

    if not rec_list:
        return pd.DataFrame()
        
    res_df = pd.DataFrame(rec_list)
    
    agg_funcs = {
        'å˜ï½µï½¯ï½½ï¾': 'min',
        'å±æ€§': lambda x: ' + '.join(sorted(set(x))),
        'ã‚¿ã‚¤ãƒ—': lambda x: ' / '.join(sorted(set(x), key=lambda s: 0 if 'â˜…' in s else 1)), 
        'ãƒ‘ã‚¿ãƒ¼ãƒ³': lambda x: ','.join(sorted(set(x))),
        'æ¡ä»¶': lambda x: ' / '.join(sorted(set(x))),
        'ã‚¹ã‚³ã‚¢': 'sum',
        'æ­£ç•ª': 'first'
    }
    
    res_df = res_df.groupby(['å ´å', 'R', 'é¦¬å'], as_index=False).agg(agg_funcs)
    res_df = res_df.sort_values(['å ´å', 'R', 'ã‚¹ã‚³ã‚¢'], ascending=[True, True, False])
    
    if 'ç€é †' not in res_df.columns: res_df['ç€é †'] = np.nan
    
    return res_df

# ==========================================
# 4. Webã‚¢ãƒ—ãƒªç”»é¢ (Streamlit)
# ==========================================

st.title("ğŸ‡ é…ç½®é¦¬åˆ¸è¡“ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ åˆ†æ")

# ã‚µã‚¤ãƒ‰ãƒãƒ¼
with st.sidebar:
    st.header("1. ãƒ‡ãƒ¼ã‚¿å…¥åŠ›")
    uploaded_file = st.file_uploader("å½“æ—¥ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ (ã¾ãŸã¯ä¿å­˜ãƒ‡ãƒ¼ã‚¿)", type=['xlsx', 'csv'])
    prev_file = st.file_uploader("å‰æ—¥ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ (ä»»æ„)", type=['xlsx', 'csv'])
    
    st.markdown("---")
    st.header("2. ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜")
    st.caption("ç€é †ã‚’å…¥åŠ›ã—ãŸçŠ¶æ…‹ã§ã“ã“ã‹ã‚‰CSVã‚’ä¿å­˜ã—ã€æ¬¡å›èª­ã¿è¾¼ã‚€ã¨ç¶šãã‹ã‚‰å†é–‹ã§ãã¾ã™ã€‚")
    
    if 'analyzed_df' in st.session_state and not st.session_state['analyzed_df'].empty:
        csv = st.session_state['analyzed_df'].to_csv(index=False).encode('utf-8-sig')
        st.download_button(
            label="ğŸ’¾ ç¾åœ¨ã®çŠ¶æ…‹ã‚’ä¿å­˜ (CSV)",
            data=csv,
            file_name="race_progress_save.csv",
            mime="text/csv"
        )
    else:
        st.button("ğŸ’¾ ç¾åœ¨ã®çŠ¶æ…‹ã‚’ä¿å­˜", disabled=True)

if uploaded_file:
    df_raw, status = load_data(uploaded_file)
    df_prev, _ = load_data(prev_file) if prev_file else (None, None)
    
    if status != "success":
        st.error(status)
    else:
        if 'analyzed_df' not in st.session_state:
            if 'ãƒ‘ã‚¿ãƒ¼ãƒ³' in df_raw.columns and 'ã‚¹ã‚³ã‚¢' in df_raw.columns:
                st.success("ğŸ“‚ ä¿å­˜ãƒ‡ãƒ¼ã‚¿ã‚’æ¤œçŸ¥ã—ã¾ã—ãŸã€‚å¾©å…ƒã—ã¾ã™ã€‚")
                result_df = df_raw
            else:
                with st.spinner('å…¨ãƒ¬ãƒ¼ã‚¹åˆ†æä¸­...'):
                    result_df = analyze_logic(df_raw, df_prev)
                    result_df = apply_ranking_logic(result_df)

            if not result_df.empty:
                result_df['id'] = result_df.index
                st.session_state['analyzed_df'] = result_df
            else:
                st.session_state['analyzed_df'] = pd.DataFrame()

        if not st.session_state['analyzed_df'].empty:
            
            st.subheader("ğŸ“ çµæœå…¥åŠ› & æ¨å¥¨é¦¬ãƒªã‚¹ãƒˆ")
            
            full_df = st.session_state['analyzed_df'].copy()
            places = sorted(full_df['å ´å'].unique())
            display_cols = ['å ´å', 'R', 'æ­£ç•ª', 'é¦¬å', 'å˜ï½µï½¯ï½½ï¾', 'å±æ€§', 'ã‚¿ã‚¤ãƒ—', 'ãƒ‘ã‚¿ãƒ¼ãƒ³', 'æ¡ä»¶', 'ã‚¹ã‚³ã‚¢', 'ç€é †']
            
            with st.form("result_entry_form"):
                place_tabs = st.tabs(places)
                edited_dfs = [] 
                
                for p_tab, place in zip(place_tabs, places):
                    with p_tab:
                        place_df = full_df[full_df['å ´å'] == place]
                        race_list = sorted(place_df['R'].unique())
                        if race_list:
                            r_tabs = st.tabs([f"{r}R" for r in race_list])
                            for r_tab, r_num in zip(r_tabs, race_list):
                                with r_tab:
                                    # --- ã‚ªãƒƒã‚ºå–å¾—æ©Ÿèƒ½ (ãƒ‡ãƒãƒƒã‚°ä»˜ã) ---
                                    with st.expander(f"ğŸŒ {place}{r_num}R ã®æœ€æ–°ã‚ªãƒƒã‚ºã‚’Webã‹ã‚‰å–å¾— (netkeiba)"):
                                        st.caption("å‡ºé¦¬è¡¨ã‚„çµæœãƒšãƒ¼ã‚¸ã®URLã‚’è²¼ã‚Šä»˜ã‘ã¦ãã ã•ã„")
                                        
                                        col_url, col_force = st.columns([3, 1])
                                        with col_url:
                                            url_input = st.text_input("URL", key=f"url_{place}_{r_num}")
                                        with col_force:
                                            force_mode = st.checkbox("URLè‡ªå‹•å¤‰æ›ã—ãªã„\n(éå»/ç¢ºå®šãƒ¬ãƒ¼ã‚¹ç”¨)", key=f"force_{place}_{r_num}")
                                        
                                        if st.form_submit_button(f"ğŸ“¥ {place}{r_num}R ã‚ªãƒƒã‚ºå–å¾—ãƒ»æ›´æ–°"):
                                            if url_input:
                                                new_odds_df, debug_msg = fetch_odds_from_web(url_input, force_mode)
                                                if new_odds_df is not None:
                                                    target_mask = (st.session_state['analyzed_df']['å ´å'] == place) & \
                                                                  (st.session_state['analyzed_df']['R'] == r_num)
                                                    
                                                    for _, o_row in new_odds_df.iterrows():
                                                        umaban = o_row['æ­£ç•ª']
                                                        odds = o_row['å˜ï½µï½¯ï½½ï¾']
                                                        mask = target_mask & (st.session_state['analyzed_df']['æ­£ç•ª'] == umaban)
                                                        st.session_state['analyzed_df'].loc[mask, 'å˜ï½µï½¯ï½½ï¾'] = odds
                                                    
                                                    st.success(f"{place}{r_num}R ã®ã‚ªãƒƒã‚ºã‚’æ›´æ–°ã—ã¾ã—ãŸï¼")
                                                    st.rerun()
                                                else:
                                                    pass # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¯fetché–¢æ•°å†…ã§è¡¨ç¤ºæ¸ˆã¿
                                                
                                    race_data = place_df[place_df['R'] == r_num][valid_cols := [c for c in display_cols if c in full_df.columns]]
                                    edited_chunk = st.data_editor(
                                        race_data,
                                        column_config={
                                            "ç€é †": st.column_config.NumberColumn("ç€é †", format="%d", min_value=1, max_value=18),
                                            "ã‚¹ã‚³ã‚¢": st.column_config.ProgressColumn("æ³¨ç›®åº¦", format="%.1f", min_value=0, max_value=20),
                                            "å˜ï½µï½¯ï½½ï¾": st.column_config.NumberColumn("ã‚ªãƒƒã‚º", format="%.1f")
                                        },
                                        disabled=["å ´å", "R", "é¦¬å", "å˜ï½µï½¯ï½½ï¾", "æ­£ç•ª", "å±æ€§", "ã‚¿ã‚¤ãƒ—", "ãƒ‘ã‚¿ãƒ¼ãƒ³", "æ¡ä»¶", "ã‚¹ã‚³ã‚¢"],
                                        hide_index=True,
                                        use_container_width=True,
                                        height=300,
                                        key=f"editor_{place}_{r_num}"
                                    )
                                    edited_dfs.append(edited_chunk)
                
                st.markdown("---")
                submit_btn = st.form_submit_button("ğŸ”„ å…¨ãƒ¬ãƒ¼ã‚¹ã®å…¥åŠ›ã‚’ç¢ºå®šã—ã¦æ›´æ–° (å†è¨ˆç®—)")

            if submit_btn:
                if edited_dfs:
                    combined_df = pd.concat(edited_dfs, ignore_index=True)
                    recalculated_df = apply_ranking_logic(combined_df)
                    recalculated_df = recalculated_df.sort_values(['å ´å', 'R', 'ç·åˆã‚¹ã‚³ã‚¢'], ascending=[True, True, False])
                    st.session_state['analyzed_df'] = recalculated_df
                    st.success("ãƒ‡ãƒ¼ã‚¿ã‚’æ›´æ–°ã—ã€ã‚¹ã‚³ã‚¢ã¨æ¨å¥¨åº¦ã‚’å†è¨ˆç®—ã—ã¾ã—ãŸï¼")
                    st.rerun()

            # ==========================================
            # 5. é›†è¨ˆ & ã‚°ãƒ©ãƒ•
            # ==========================================
            current_df = st.session_state['analyzed_df']
            df_hits = current_df[current_df['ç€é †'].notna()].copy()
            df_hits['ç€é †'] = pd.to_numeric(df_hits['ç€é †'], errors='coerce')
            df_fuku = df_hits[df_hits['ç€é †'] <= 3] 

            st.divider()
            st.subheader("ğŸ“Š ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å‚¾å‘åˆ†æ")

            if not df_hits.empty:
                c1, c2, c3 = st.columns(3)
                with c1: st.metric("æ¶ˆåŒ–ãƒ¬ãƒ¼ã‚¹", len(df_hits['R'].unique()))
                with c2: 
                    rate = len(df_fuku)/len(df_hits)*100 if len(df_hits)>0 else 0
                    st.metric("æ¨å¥¨é¦¬ è¤‡å‹ç‡", f"{rate:.1f}%")
                with c3: st.metric("çš„ä¸­æ•°", f"{len(df_fuku)} é ­")

                graph_places = sorted(df_hits['å ´å'].unique())
                if graph_places:
                    g_tabs = st.tabs(graph_places)
                    for g_tab, place in zip(g_tabs, graph_places):
                        with g_tab:
                            col_g1, col_g2 = st.columns([1, 1])
                            place_hits = df_hits[df_hits['å ´å'] == place]
                            place_fuku = df_fuku[df_fuku['å ´å'] == place]
                            
                            if not place_fuku.empty:
                                all_patterns = []
                                for p in place_fuku['ãƒ‘ã‚¿ãƒ¼ãƒ³']:
                                    if p: all_patterns.extend(str(p).split(','))
                                
                                if all_patterns:
                                    pat_counts = pd.Series(all_patterns).value_counts().reset_index()
                                    pat_counts.columns = ['ãƒ‘ã‚¿ãƒ¼ãƒ³', 'çš„ä¸­æ•°']
                                    with col_g1:
                                        fig = px.pie(pat_counts, values='çš„ä¸­æ•°', names='ãƒ‘ã‚¿ãƒ¼ãƒ³', 
                                                     title=f'ã€{place}ã€‘ çš„ä¸­ãƒ‘ã‚¿ãƒ¼ãƒ³', hole=0.4)
                                        st.plotly_chart(fig, use_container_width=True)
                                else:
                                    with col_g1: st.info("ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ‡ãƒ¼ã‚¿ãªã—")
                            else:
                                with col_g1: st.info("çš„ä¸­ãƒ‡ãƒ¼ã‚¿ã¯ã‚ã‚Šã¾ã›ã‚“")
                            
                            with col_g2:
                                st.write(f"**{place} ã®çµæœä¸€è¦§**")
                                place_hits_disp = place_hits.copy()
                                place_hits_disp['é¦¬å'] = place_hits_disp.apply(
                                    lambda x: f":blue[**{x['é¦¬å']}**]" if 'é’' in str(x['ãƒ‘ã‚¿ãƒ¼ãƒ³']) else x['é¦¬å'], 
                                    axis=1
                                )
                                st.dataframe(place_hits_disp[['R', 'é¦¬å', 'å˜ï½µï½¯ï½½ï¾', 'å±æ€§', 'ã‚¿ã‚¤ãƒ—', 'ç€é †']], use_container_width=True, hide_index=True)

                # --- å‚¾å‘ã‚¹ã‚³ã‚¢åŠ ç®— & æ¬¡ãƒ¬ãƒ¼ã‚¹è¡¨ç¤º & è²·ã„ç›® ---
                st.markdown("### ğŸ“ˆ æ¬¡ãƒ¬ãƒ¼ã‚¹ã®æ³¨ç›®é¦¬ãƒ»æ¨å¥¨è²·ã„ç›®")
                
                future_races = current_df[current_df['ç€é †'].isna()].copy()
                
                if not future_races.empty:
                    future_places = sorted(future_races['å ´å'].unique())
                    if future_places:
                        f_tabs = st.tabs(future_places)
                        
                        for tab, place in zip(f_tabs, future_places):
                            with tab:
                                place_future = future_races[future_races['å ´å'] == place]
                                if not place_future.empty:
                                    future_r_list = sorted(place_future['R'].unique())
                                    r_tabs = st.tabs([f"{r}R" for r in future_r_list])
                                    
                                    for r_tab, r_num in zip(r_tabs, future_r_list):
                                        with r_tab:
                                            target_df = place_future[place_future['R'] == r_num]
                                            target_df = target_df.sort_values('ç·åˆã‚¹ã‚³ã‚¢', ascending=False)
                                            
                                            target_df['é¦¬å'] = target_df.apply(
                                                lambda x: f":blue[**{x['é¦¬å']}**]" if 'é’' in str(x['ãƒ‘ã‚¿ãƒ¼ãƒ³']) else x['é¦¬å'], 
                                                axis=1
                                            )
                                            
                                            top_horses = target_df.head(3)
                                            if len(top_horses) >= 2:
                                                h1 = top_horses.iloc[0]
                                                h2 = top_horses.iloc[1]
                                                h1_score = h1['ç·åˆã‚¹ã‚³ã‚¢']
                                                h2_score = h2['ç·åˆã‚¹ã‚³ã‚¢']
                                                h1_name = str(h1['é¦¬å']).replace(':blue[**', '').replace('**]', '')
                                                
                                                h1_odds = h1.get('å˜ï½µï½¯ï½½ï¾', np.nan)
                                                odds_str = f"(å˜{h1_odds}å€)" if pd.notna(h1_odds) else "(ã‚ªãƒƒã‚ºä¸æ˜)"
                                                
                                                if h1_score >= 15:
                                                    if pd.notna(h1_odds):
                                                        if h1_odds >= 3.0:
                                                            st.success(f"ğŸ”¥ **{r_num}R æ¿€ã‚¢ãƒ„å‹è²  (é«˜æœŸå¾…å€¤)**: {h1['æ­£ç•ª']} ({h1_name}) {odds_str}")
                                                        elif h1_odds < 1.5:
                                                            st.warning(f"ğŸ§± **{r_num}R é‰„æ¿ (å …å®Ÿ)**: {h1['æ­£ç•ª']} ({h1_name}) {odds_str}")
                                                        else:
                                                            st.info(f"ğŸ‘‘ **{r_num}R ç›¤çŸ³ã®è»¸**: {h1['æ­£ç•ª']} ({h1_name}) {odds_str}")
                                                    else:
                                                        st.info(f"ğŸ‘‘ **{r_num}R ç›¤çŸ³ã®è»¸**: {h1['æ­£ç•ª']} ({h1_name})")
                                                elif h1_score >= 12:
                                                    st.info(f"ğŸ’¡ **{r_num}R å˜è¤‡æ¨å¥¨**: {h1['æ­£ç•ª']} ({h1_name})")
                                                else:
                                                    st.caption(f"ğŸ² {r_num}R ã¯æ··æˆ¦æ¨¡æ§˜ã§ã™ã€‚")
                                            
                                            disp_cols = ['R', 'é¦¬å', 'å˜ï½µï½¯ï½½ï¾', 'ã‚¿ã‚¤ãƒ—', 'ãƒ‘ã‚¿ãƒ¼ãƒ³', 'ã‚¹ã‚³ã‚¢', 'å‚¾å‘åŠ ç‚¹', 'ç·åˆã‚¹ã‚³ã‚¢', 'æ¨å¥¨è²·ã„ç›®']
                                            final_disp_cols = [c for c in disp_cols if c in target_df.columns]
                                            
                                            st.dataframe(
                                                target_df[final_disp_cols],
                                                use_container_width=True,
                                                hide_index=True
                                            )
                                else:
                                    st.info("æ®‹ã‚Šãƒ¬ãƒ¼ã‚¹ã¯ã‚ã‚Šã¾ã›ã‚“")
                    else:
                        st.info("å…¨ã¦ã®ãƒ¬ãƒ¼ã‚¹ãŒçµ‚äº†ã—ã¾ã—ãŸã€‚")
                else:
                    st.info("å…¨ã¦ã®ãƒ¬ãƒ¼ã‚¹ãŒçµ‚äº†ã—ã¾ã—ãŸã€‚")
            else:
                st.info("ã¾ã ç€é †ãŒå…¥åŠ›ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚çµæœã‚’å…¥åŠ›ã—ã¦æ›´æ–°ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")
