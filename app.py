import streamlit as st
import pandas as pd
import numpy as np
import re
import plotly.express as px
import openpyxl

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(page_title="é…ç½®é¦¬åˆ¸è¡“ Web", layout="wide")

# ==========================================
# 1. å…±é€šãƒ­ã‚¸ãƒƒã‚¯ & ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
# ==========================================

def to_half_width(text):
    if isinstance(text, (list, pd.Series, np.ndarray)):
        text = str(text)
    if pd.isna(text): return text
    text = str(text)
    table = str.maketrans('ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™', '0123456789')
    text = text.translate(table)
    text = re.sub(r'[^\d\.]', '', text)
    return text

def normalize_name(x):
    if pd.isna(x): return ''
    normalized_name = str(x).strip().replace('ã€€', '').replace(' ', '')
    normalized_name = re.sub(r'[â˜…â˜†â–²â–³â—‡]', '', normalized_name)
    if ',' in normalized_name: normalized_name = normalized_name.split(',')[0]
    text = re.sub(r'[0-9\.]+[Rr]', '', normalized_name)
    text = re.sub(r'\(.*?\)', '', text)
    return text.replace('/', '').strip()

@st.cache_data
def load_data(file):
    """
    ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã‚’è¡Œã†é–¢æ•°
    Windows(Shift-JIS)ã¨Mac(UTF-8)ã®ä¸¡æ–¹ã«å¯¾å¿œ
    """
    df = None
    
    # 1. Excelãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç†
    if file.name.endswith('.xlsx'):
        try:
            file.seek(0)
            df = pd.read_excel(file, engine='openpyxl')
        except Exception as e:
            return pd.DataFrame(), f"Excelèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}"
            
    # 2. CSVãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç†
    else:
        try:
            file.seek(0)
            df = pd.read_csv(file, encoding='utf-8', on_bad_lines='skip')
        except UnicodeDecodeError:
            try:
                file.seek(0)
                df = pd.read_csv(file, encoding='cp932', on_bad_lines='skip')
            except Exception as e:
                return pd.DataFrame(), f"CSVèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼(æ–‡å­—ã‚³ãƒ¼ãƒ‰åˆ¤å®šä¸èƒ½): {e}"
        except Exception as e:
            return pd.DataFrame(), f"CSVäºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼: {e}"

    # --- ãƒ‡ãƒ¼ã‚¿æ•´å½¢ ---
    df.columns = df.columns.str.strip()
    
    rename_map = {
        'å ´æ‰€': 'å ´å', 'é–‹å‚¬': 'å ´å', 'å˜ã‚ªãƒƒã‚º': 'å˜ï½µï½¯ï½½ï¾', 
        'èª¿æ•™å¸«': 'å©èˆ', 'èª¿æ•™å¸«å': 'å©èˆ', 'å©èˆå': 'å©èˆ',
        'é¨æ‰‹å': 'é¨æ‰‹',
        'ãƒ¬ãƒ¼ã‚¹': 'R', 'ï¼²': 'R', 'ãƒ¬ãƒ¼ã‚¹å': 'R',
        'ç€': 'ç€é †', 'ç€ é †': 'ç€é †', 'ç•ª': 'æ­£ç•ª', 'é¦¬ç•ª': 'æ­£ç•ª'
    }
    df = df.rename(columns=rename_map)

    # é‡è¤‡ã‚«ãƒ©ãƒ ã®å‰Šé™¤
    df = df.loc[:, ~df.columns.duplicated()]

    if 'å ´å' not in df.columns: df['å ´å'] = 'Unknown'

    target_numeric_cols = ['R', 'æ­£ç•ª', 'å˜ï½µï½¯ï½½ï¾', 'é€†ç•ª', 'æ­£å¾ªç’°', 'é€†å¾ªç’°', 'é ­æ•°']
    for col in target_numeric_cols:
        if col in df.columns:
            df[col] = df[col].apply(to_half_width)
            df[col] = pd.to_numeric(df[col], errors='coerce')

    if 'R' not in df.columns or 'æ­£ç•ª' not in df.columns:
        return pd.DataFrame(), "ã‚¨ãƒ©ãƒ¼: å¿…é ˆåˆ—ï¼ˆãƒ¬ãƒ¼ã‚¹å/Rã€é¦¬ç•ª/æ­£ç•ªï¼‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"

    df = df.dropna(subset=['R', 'æ­£ç•ª'])
    df['R'] = df['R'].astype(int)
    df['æ­£ç•ª'] = df['æ­£ç•ª'].astype(int)

    for col in ['é¨æ‰‹', 'å©èˆ', 'é¦¬ä¸»']:
        if col in df.columns:
            df[col] = df[col].apply(normalize_name)
        else:
            df[col] = ''
            
    # å¿…é ˆåˆ—ç¢ºä¿
    required_cols = ['R', 'å ´å', 'é¦¬å', 'æ­£ç•ª', 'é¨æ‰‹', 'å©èˆ', 'é¦¬ä¸»', 'å˜ï½µï½¯ï½½ï¾', 'é€†ç•ª', 'æ­£å¾ªç’°', 'é€†å¾ªç’°', 'é ­æ•°']
    for col in required_cols:
        if col not in df.columns:
            df[col] = np.nan

    # ä¿å­˜ãƒ‡ãƒ¼ã‚¿ç”¨ã®åˆ—
    save_cols = ['å±æ€§', 'ã‚¿ã‚¤ãƒ—', 'ãƒ‘ã‚¿ãƒ¼ãƒ³', 'æ¡ä»¶', 'ã‚¹ã‚³ã‚¢', 'ç€é †', 'å‚¾å‘åŠ ç‚¹', 'ç·åˆã‚¹ã‚³ã‚¢']
    existing_save_cols = [c for c in save_cols if c in df.columns]
    
    final_cols = required_cols + existing_save_cols
    
    return df[final_cols].copy(), "success"

# ==========================================
# 2. é…ç½®è¨ˆç®—ãƒ»åˆ†æãƒ­ã‚¸ãƒƒã‚¯
# ==========================================

def calc_haichi_numbers(df):
    check_cols = ['é€†ç•ª', 'æ­£å¾ªç’°', 'é€†å¾ªç’°']
    if set(check_cols).issubset(df.columns) and df[check_cols].notna().all().all():
        df['è¨ˆç®—_é€†ç•ª'] = df['é€†ç•ª']
        df['è¨ˆç®—_æ­£å¾ªç’°'] = df['æ­£å¾ªç’°']
        df['è¨ˆç®—_é€†å¾ªç’°'] = df['é€†å¾ªç’°']
        return df
    
    max_umaban = df.groupby(['å ´å', 'R'])['æ­£ç•ª'].transform('max')
    df['ä½¿ç”¨é ­æ•°'] = max_umaban.fillna(16).astype(int)
    if 'é ­æ•°' in df.columns:
        df['ä½¿ç”¨é ­æ•°'] = df['é ­æ•°'].fillna(df['ä½¿ç”¨é ­æ•°']).astype(int)
    
    df['ä½¿ç”¨é ­æ•°'] = np.maximum(df['ä½¿ç”¨é ­æ•°'], df['æ­£ç•ª'])
    
    def calc(row):
        t = int(row['ä½¿ç”¨é ­æ•°'])
        s = int(row['æ­£ç•ª'])
        g = (t + 1) - s
        sj = t + s
        gj = t + g
        return pd.Series([g, sj, gj])
    
    df[['è¨ˆç®—_é€†ç•ª', 'è¨ˆç®—_æ­£å¾ªç’°', 'è¨ˆç®—_é€†å¾ªç’°']] = df.apply(calc, axis=1)
    return df

def get_pair_pattern(row1, row2):
    def val(x):
        try: return int(float(x)) 
        except: return None
    r1 = [val(row1['æ­£ç•ª']), val(row1['è¨ˆç®—_é€†ç•ª']), val(row1['è¨ˆç®—_æ­£å¾ªç’°']), val(row1['è¨ˆç®—_é€†å¾ªç’°'])]
    r2 = [val(row2['æ­£ç•ª']), val(row2['è¨ˆç®—_é€†ç•ª']), val(row2['è¨ˆç®—_æ­£å¾ªç’°']), val(row2['è¨ˆç®—_é€†å¾ªç’°'])]
    label = list("ABCDEFGHIJKLMNOP")
    pairs = [label[i * 4 + j] for i in range(4) for j in range(4)
             if r1[i] is not None and r2[j] is not None and r1[i] == r2[j] and r1[i] != 0]
    return ",".join(pairs)

def get_common_values(group):
    cols = ['æ­£ç•ª', 'è¨ˆç®—_é€†ç•ª', 'è¨ˆç®—_æ­£å¾ªç’°', 'è¨ˆç®—_é€†å¾ªç’°']
    common_set = None
    for _, row in group.iterrows():
        current_set = set()
        for col in cols:
            val = row.get(col)
            if pd.notna(val):
                try:
                    num = int(float(val))
                    if num != 0: current_set.add(num)
                except: continue
        if common_set is None: common_set = current_set
        else: common_set = common_set.intersection(current_set)
        if not common_set: return None
    if common_set: return ','.join(map(str, sorted(list(common_set))))
    return None

def analyze_logic(df_curr, df_prev=None):
    df_curr = calc_haichi_numbers(df_curr)
    if df_prev is not None and not df_prev.empty:
        df_prev = calc_haichi_numbers(df_prev)
    
    rec_list = []
    
    # A. é’å¡— (é¨æ‰‹ãƒ»å©èˆãƒ»é¦¬ä¸»)
    blue_keys = set()
    for col in ['é¨æ‰‹', 'å©èˆ', 'é¦¬ä¸»']:
        if col not in df_curr.columns: continue
        group_keys = ['å ´å', col]
        try:
            for name, group in df_curr.groupby(group_keys):
                if len(group) < 2: continue
                target_name = name[1]
                if not target_name: continue
                
                common_vals = get_common_values(group)
                if common_vals:
                    all_races = sorted(group['R'].unique())
                    priority = 0.3 if col == 'é¨æ‰‹' else (0.2 if col == 'å©èˆ' else 0.1)
                    
                    for _, row in group.iterrows():
                        other_races = [str(r) for r in all_races if r != row['R']]
                        remark = f'[{col}] å…±é€šå€¤({common_vals}) [ä»–:{",".join(other_races)}R]'
                        rec_list.append({
                            'å ´å': row['å ´å'], 'R': row['R'], 'æ­£ç•ª': row['æ­£ç•ª'], 'é¦¬å': row['é¦¬å'],
                            'å±æ€§': f"{col}:{target_name}", 
                            'ã‚¿ã‚¤ãƒ—': f'â˜… {col}é’å¡—', 
                            'ãƒ‘ã‚¿ãƒ¼ãƒ³': 'Blue', 
                            'æ¡ä»¶': remark,
                            'ã‚¹ã‚³ã‚¢': 9.0 + priority
                        })
                        blue_keys.add((row['å ´å'], row['R'], row['é¦¬å']))
        except: continue

    # B. é’å¡—ã®éš£
    if blue_keys:
        for (place, race), group in df_curr.groupby(['å ´å', 'R']):
            group = group.sort_values('æ­£ç•ª')
            umaban_map = {int(row['æ­£ç•ª']): row for _, row in group.iterrows()}
            blue_horses = [row for _, row in group.iterrows() if (place, race, row['é¦¬å']) in blue_keys]
            for b_row in blue_horses:
                curr_num = int(b_row['æ­£ç•ª'])
                for t_num in [curr_num - 1, curr_num + 1]:
                    if t_num in umaban_map:
                        t_row = umaban_map[t_num]
                        if (place, race, t_row['é¦¬å']) not in blue_keys:
                            rec_list.append({
                                'å ´å': place, 'R': race, 'æ­£ç•ª': t_num, 'é¦¬å': t_row['é¦¬å'],
                                'å±æ€§': '(é’å¡—éš£)', 'ã‚¿ã‚¤ãƒ—': 'â–³ é’å¡—ã®éš£',
                                'ãƒ‘ã‚¿ãƒ¼ãƒ³': 'BlueNeighbor',
                                'æ¡ä»¶': f"é’å¡—é¦¬(#{curr_num})ã®éš£",
                                'ã‚¹ã‚³ã‚¢': 9.0
                            })

    # C. é€šå¸¸ãƒšã‚¢ (é¨æ‰‹)
    if 'é¨æ‰‹' in df_curr.columns:
        for name, group in df_curr.groupby('é¨æ‰‹'):
            if len(group) < 2: continue
            group = group.sort_values('R').to_dict('records')
            for i in range(len(group)-1):
                curr, next_r = group[i], group[i+1]
                if curr['å ´å'] != next_r['å ´å']: continue
                pat = get_pair_pattern(curr, next_r)
                if pat:
                    label = "â— ãƒãƒ£ãƒ³ã‚¹" if any(x in pat for x in ['C','D','G','H']) else "â—‹ ç‹™ã„ç›®"
                    base_score = 4.0 if label.startswith("â—") else 3.0
                    rec_list.append({
                        'å ´å': curr['å ´å'], 'R': curr['R'], 'æ­£ç•ª': curr['æ­£ç•ª'], 'é¦¬å': curr['é¦¬å'],
                        'å±æ€§': f"é¨æ‰‹:{name}", 'ã‚¿ã‚¤ãƒ—': label, 'ãƒ‘ã‚¿ãƒ¼ãƒ³': pat, 
                        'æ¡ä»¶': f"[é¨æ‰‹] ãƒšã‚¢({next_r['R']}R #{next_r['æ­£ç•ª']})", 'ã‚¹ã‚³ã‚¢': base_score + 0.3
                    })
                    rec_list.append({
                        'å ´å': next_r['å ´å'], 'R': next_r['R'], 'æ­£ç•ª': next_r['æ­£ç•ª'], 'é¦¬å': next_r['é¦¬å'],
                        'å±æ€§': f"é¨æ‰‹:{name}", 'ã‚¿ã‚¤ãƒ—': label, 'ãƒ‘ã‚¿ãƒ¼ãƒ³': pat, 
                        'æ¡ä»¶': f"[é¨æ‰‹] ãƒšã‚¢({curr['R']}R #{curr['æ­£ç•ª']})", 'ã‚¹ã‚³ã‚¢': base_score + 0.3
                    })

    # C. é€šå¸¸ãƒšã‚¢ (å©èˆ)
    if 'å©èˆ' in df_curr.columns:
        for (place, name), group in df_curr.groupby(['å ´å', 'å©èˆ']):
            if len(group) < 2: continue
            group = group.sort_values('R').to_dict('records')
            for i in range(len(group)):
                for j in range(i+1, len(group)):
                    curr, next_r = group[i], group[j]
                    pat = get_pair_pattern(curr, next_r)
                    if pat:
                        label = "â— ãƒãƒ£ãƒ³ã‚¹" if any(x in pat for x in ['C','D','G','H']) else "â—‹ ç‹™ã„ç›®"
                        base_score = 4.0 if label.startswith("â—") else 3.0
                        rec_list.append({
                            'å ´å': place, 'R': curr['R'], 'æ­£ç•ª': curr['æ­£ç•ª'], 'é¦¬å': curr['é¦¬å'],
                            'å±æ€§': f"å©èˆ:{name}", 'ã‚¿ã‚¤ãƒ—': label, 'ãƒ‘ã‚¿ãƒ¼ãƒ³': pat, 
                            'æ¡ä»¶': f"[å©èˆ] ãƒšã‚¢({next_r['R']}R #{next_r['æ­£ç•ª']})", 'ã‚¹ã‚³ã‚¢': base_score + 0.2
                        })
                        rec_list.append({
                            'å ´å': place, 'R': next_r['R'], 'æ­£ç•ª': next_r['æ­£ç•ª'], 'é¦¬å': next_r['é¦¬å'],
                            'å±æ€§': f"å©èˆ:{name}", 'ã‚¿ã‚¤ãƒ—': label, 'ãƒ‘ã‚¿ãƒ¼ãƒ³': pat, 
                            'æ¡ä»¶': f"[å©èˆ] ãƒšã‚¢({curr['R']}R #{curr['æ­£ç•ª']})", 'ã‚¹ã‚³ã‚¢': base_score + 0.2
                        })

    # D. å‰æ—¥åŒé…ç½® (é¨æ‰‹ã®ã¿)
    if df_prev is not None and not df_prev.empty:
        for idx, row in df_curr.iterrows():
            race = row['R']
            name = row['é¨æ‰‹']
            if not name: continue
            prev_rows = df_prev[(df_prev['R'] == race) & (df_prev['é¨æ‰‹'] == name)]
            for _, p_row in prev_rows.iterrows():
                is_seiban = (p_row['æ­£ç•ª'] == row['æ­£ç•ª'])
                is_gyaku = (p_row['è¨ˆç®—_é€†ç•ª'] == row['è¨ˆç®—_é€†ç•ª'])
                if is_seiban or is_gyaku:
                    reason = "æ­£ç•ª" if is_seiban else "é€†ç•ª"
                    rec_list.append({
                        'å ´å': row['å ´å'], 'R': race, 'æ­£ç•ª': row['æ­£ç•ª'], 'é¦¬å': row['é¦¬å'],
                        'å±æ€§': f"é¨æ‰‹:{name}", 'ã‚¿ã‚¤ãƒ—': 'â˜… å‰æ—¥åŒé…ç½®', 
                        'ãƒ‘ã‚¿ãƒ¼ãƒ³': 'PrevDay',
                        'æ¡ä»¶': f"[é¨æ‰‹] å‰æ—¥{race}RåŒé…ç½®({reason})", 
                        'ã‚¹ã‚³ã‚¢': 8.3
                    })

    if not rec_list:
        return pd.DataFrame()
        
    res_df = pd.DataFrame(rec_list)
    
    agg_funcs = {
        'å±æ€§': lambda x: ' + '.join(sorted(set(x))),
        'ã‚¿ã‚¤ãƒ—': lambda x: ' / '.join(sorted(set(x), key=lambda s: 0 if 'â˜…' in s else 1)), 
        'ãƒ‘ã‚¿ãƒ¼ãƒ³': lambda x: ','.join(sorted(set(x))),
        'æ¡ä»¶': lambda x: ' / '.join(sorted(set(x))),
        'ã‚¹ã‚³ã‚¢': 'sum',
        'æ­£ç•ª': 'first'
    }
    
    res_df = res_df.groupby(['å ´å', 'R', 'é¦¬å'], as_index=False).agg(agg_funcs)
    res_df = res_df.sort_values(['å ´å', 'R', 'ã‚¹ã‚³ã‚¢'], ascending=[True, True, False])
    
    if 'ç€é †' not in res_df.columns: res_df['ç€é †'] = np.nan
    
    return res_df

# ==========================================
# 3. Webã‚¢ãƒ—ãƒªç”»é¢ (Streamlit)
# ==========================================

st.title("ğŸ‡ é…ç½®é¦¬åˆ¸è¡“ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ åˆ†æ")

# ã‚µã‚¤ãƒ‰ãƒãƒ¼
with st.sidebar:
    st.header("1. ãƒ‡ãƒ¼ã‚¿å…¥åŠ›")
    uploaded_file = st.file_uploader("å½“æ—¥ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ (ã¾ãŸã¯ä¿å­˜ãƒ‡ãƒ¼ã‚¿)", type=['xlsx', 'csv'])
    prev_file = st.file_uploader("å‰æ—¥ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ (ä»»æ„)", type=['xlsx', 'csv'])
    
    st.markdown("---")
    st.header("2. ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜")
    st.caption("ç€é †ã‚’å…¥åŠ›ã—ãŸçŠ¶æ…‹ã§ã“ã“ã‹ã‚‰CSVã‚’ä¿å­˜ã—ã€æ¬¡å›èª­ã¿è¾¼ã‚€ã¨ç¶šãã‹ã‚‰å†é–‹ã§ãã¾ã™ã€‚")
    
    if 'analyzed_df' in st.session_state and not st.session_state['analyzed_df'].empty:
        csv = st.session_state['analyzed_df'].to_csv(index=False).encode('utf-8-sig')
        st.download_button(
            label="ğŸ’¾ ç¾åœ¨ã®çŠ¶æ…‹ã‚’ä¿å­˜ (CSV)",
            data=csv,
            file_name="race_progress_save.csv",
            mime="text/csv"
        )
    else:
        st.button("ğŸ’¾ ç¾åœ¨ã®çŠ¶æ…‹ã‚’ä¿å­˜", disabled=True)

if uploaded_file:
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    df_raw, status = load_data(uploaded_file)
    df_prev, _ = load_data(prev_file) if prev_file else (None, None)
    
    if status != "success":
        st.error(status)
    else:
        # åˆå›åˆ†æ or å¾©å…ƒ
        if 'analyzed_df' not in st.session_state:
            # å¿…é ˆåˆ—ãƒã‚§ãƒƒã‚¯ï¼ˆãƒ‘ã‚¿ãƒ¼ãƒ³ã¨ã‚¹ã‚³ã‚¢ãŒã‚ã‚‹ï¼ä¿å­˜ãƒ‡ãƒ¼ã‚¿ï¼‰
            if 'ãƒ‘ã‚¿ãƒ¼ãƒ³' in df_raw.columns and 'ã‚¹ã‚³ã‚¢' in df_raw.columns:
                st.success("ğŸ“‚ ä¿å­˜ãƒ‡ãƒ¼ã‚¿ã‚’æ¤œçŸ¥ã—ã¾ã—ãŸã€‚å¾©å…ƒã—ã¾ã™ã€‚")
                result_df = df_raw
            else:
                with st.spinner('å…¨ãƒ¬ãƒ¼ã‚¹åˆ†æä¸­...'):
                    result_df = analyze_logic(df_raw, df_prev)

            if not result_df.empty:
                result_df['id'] = result_df.index
                st.session_state['analyzed_df'] = result_df
            else:
                st.session_state['analyzed_df'] = pd.DataFrame()

        # --- ãƒ¡ã‚¤ãƒ³ã‚¨ãƒªã‚¢ ---
        if not st.session_state['analyzed_df'].empty:
            
            st.subheader("ğŸ“ çµæœå…¥åŠ› & æ¨å¥¨é¦¬ãƒªã‚¹ãƒˆ")
            st.info("ä¸‹ã®è¡¨ã§ç€é †ã‚’å…¥åŠ›ã™ã‚‹ã¨ã€å³åº§ã«é›†è¨ˆãŒæ›´æ–°ã•ã‚Œã¾ã™ã€‚")
            
            # --- é–‹å‚¬å ´ã”ã¨ã®ã‚¿ãƒ–ã‚’ä½œæˆ ---
            full_df = st.session_state['analyzed_df'].copy()
            places = sorted(full_df['å ´å'].unique())
            
            # â˜…ã“ã“ã‚’ä¿®æ­£: è¡¨ç¤ºã™ã‚‹ã‚«ãƒ©ãƒ ã‚’é™å®šã™ã‚‹
            # ã‚ªãƒƒã‚ºã‚„å©èˆãªã©ã®ä¸è¦ãªã‚«ãƒ©ãƒ ï¼ˆç‰¹ã«å…¥åŠ›æ ã«ãªã£ã¦ã—ã¾ã†ã‚‚ã®ï¼‰ã‚’é™¤å¤–
            display_cols = ['å ´å', 'R', 'æ­£ç•ª', 'é¦¬å', 'å±æ€§', 'ã‚¿ã‚¤ãƒ—', 'ãƒ‘ã‚¿ãƒ¼ãƒ³', 'æ¡ä»¶', 'ã‚¹ã‚³ã‚¢', 'ç€é †']
            
            # ã‚¿ãƒ–ã‚’ä½œæˆ (ãƒ•ã‚©ãƒ¼ãƒ ã®æ ã¯å‰Šé™¤)
            tabs = st.tabs(places)
            edited_dfs = [] 
            
            # å„ã‚¿ãƒ–ã§ã‚¨ãƒ‡ã‚£ã‚¿ã‚’è¡¨ç¤ºã—ã€ç·¨é›†çµæœã‚’ãƒªã‚¹ãƒˆã«æ ¼ç´
            for tab, place in zip(tabs, places):
                with tab:
                    # ç‰¹å®šã®ã‚«ãƒ©ãƒ ã ã‘æŠ½å‡ºã—ã¦è¡¨ç¤º
                    # â€»ã‚«ãƒ©ãƒ ãŒå­˜åœ¨ã—ãªã„å ´åˆã®ã‚¨ãƒ©ãƒ¼ã‚’é˜²ããŸã‚ã€intersectionã§å­˜åœ¨ã™ã‚‹åˆ—ã ã‘é¸ã¶
                    valid_cols = [c for c in display_cols if c in full_df.columns]
                    place_df = full_df[full_df['å ´å'] == place][valid_cols]
                    
                    # ç·¨é›†ãƒ‡ãƒ¼ã‚¿ã‚’å—ã‘å–ã‚‹
                    edited_chunk = st.data_editor(
                        place_df,
                        column_config={
                            "ç€é †": st.column_config.NumberColumn(
                                "ç€é †", help="ç¢ºå®šç€é †ã‚’å…¥åŠ› (1-18)", min_value=1, max_value=18, step=1, format="%d"
                            ),
                            "ã‚¹ã‚³ã‚¢": st.column_config.ProgressColumn(
                                "æ³¨ç›®åº¦", format="%.1f", min_value=0, max_value=20,
                            ),
                        },
                        disabled=["å ´å", "R", "é¦¬å", "æ­£ç•ª", "å±æ€§", "ã‚¿ã‚¤ãƒ—", "ãƒ‘ã‚¿ãƒ¼ãƒ³", "æ¡ä»¶", "ã‚¹ã‚³ã‚¢"],
                        hide_index=True,
                        use_container_width=True,
                        height=500,
                        key=f"editor_{place}" # ã‚­ãƒ¼ã‚’è¨­å®šã—ã¦çŠ¶æ…‹ã‚’ç®¡ç†
                    )
                    edited_dfs.append(edited_chunk)
            
            # --- ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ åæ˜ å‡¦ç† ---
            # ã™ã¹ã¦ã®ã‚¿ãƒ–ã®ç·¨é›†çµæœã‚’çµåˆã—ã¦session_stateã‚’æ›´æ–°
            if edited_dfs:
                combined_df = pd.concat(edited_dfs, ignore_index=True)
                
                # è¡¨ç¤ºã—ã¦ã„ãªã„ã‚«ãƒ©ãƒ ï¼ˆã‚ªãƒƒã‚ºç­‰ï¼‰ãŒæ¶ˆãˆã¦ã—ã¾ã†ã®ã‚’é˜²ããŸã‚ã€
                # å…ƒã®full_dfã‹ã‚‰ãã‚Œã‚‰ã®æƒ…å ±ã‚’å¾©å…ƒã—ã¦çµåˆã™ã‚‹å‡¦ç†ãŒå¿…è¦
                # ãŸã ã—ã€ä»Šå›ã¯ã€Œä¿å­˜ãƒ‡ãƒ¼ã‚¿ã€ã«ä½™è¨ˆãªã‚«ãƒ©ãƒ ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã“ã¨ãŒåŸå› ã§è¦‹ãˆã¦ã„ã‚‹ã ã‘ãªã®ã§ã€
                # ã“ã“ã§çµåˆå¾Œã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ãã®ã¾ã¾ä¿å­˜ã™ã‚Œã°ã€æ¬¡å›èª­ã¿è¾¼ã¿æ™‚ã‚‚ã‚¹ãƒƒã‚­ãƒªã—ãŸçŠ¶æ…‹ã«ãªã‚‹ã€‚
                # ä¿å­˜æ©Ÿèƒ½ã®ãŸã‚ã«å…ƒã®è©³ç´°æƒ…å ±ãŒå¿…è¦ãªå ´åˆã¯åˆ¥é€”ãƒãƒ¼ã‚¸ãŒå¿…è¦ã ãŒã€
                # ãƒ¦ãƒ¼ã‚¶ãƒ¼ä½“é¨“ã¨ã—ã¦ã¯ã€Œä½™è¨ˆãªæ ã€ãŒæ¶ˆãˆã‚‹ã“ã¨ã‚’å„ªå…ˆã™ã‚‹ã€‚
                
                combined_df = combined_df.sort_values(['å ´å', 'R', 'ã‚¹ã‚³ã‚¢'], ascending=[True, True, False])
                st.session_state['analyzed_df'] = combined_df

            # ==========================================
            # 4. é›†è¨ˆ & ã‚°ãƒ©ãƒ•
            # ==========================================
            current_df = st.session_state['analyzed_df']
            
            df_hits = current_df[current_df['ç€é †'].notna()].copy()
            df_hits['ç€é †'] = pd.to_numeric(df_hits['ç€é †'], errors='coerce')
            df_fuku = df_hits[df_hits['ç€é †'] <= 3] 

            st.divider()
            st.subheader("ğŸ“Š ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å‚¾å‘åˆ†æ")

            c1, c2, c3 = st.columns(3)
            with c1: st.metric("æ¶ˆåŒ–ãƒ¬ãƒ¼ã‚¹", len(df_hits['R'].unique()))
            with c2: 
                rate = len(df_fuku)/len(df_hits)*100 if len(df_hits)>0 else 0
                st.metric("æ¨å¥¨é¦¬ è¤‡å‹ç‡", f"{rate:.1f}%")
            with c3: st.metric("çš„ä¸­æ•°", f"{len(df_fuku)} é ­")

            if not df_fuku.empty:
                graph_places = sorted(df_fuku['å ´å'].unique())
                g_tabs = st.tabs(graph_places)
                
                for g_tab, place in zip(g_tabs, graph_places):
                    with g_tab:
                        col_g1, col_g2 = st.columns([1, 1])
                        place_data = df_fuku[df_fuku['å ´å'] == place]
                        
                        all_patterns = []
                        for p in place_data['ãƒ‘ã‚¿ãƒ¼ãƒ³']:
                            if p: all_patterns.extend(str(p).split(','))
                        
                        if all_patterns:
                            pat_counts = pd.Series(all_patterns).value_counts().reset_index()
                            pat_counts.columns = ['ãƒ‘ã‚¿ãƒ¼ãƒ³', 'çš„ä¸­æ•°']
                            
                            with col_g1:
                                fig = px.pie(pat_counts, values='çš„ä¸­æ•°', names='ãƒ‘ã‚¿ãƒ¼ãƒ³', 
                                             title=f'ã€{place}ã€‘ çš„ä¸­ãƒ‘ã‚¿ãƒ¼ãƒ³', hole=0.4)
                                st.plotly_chart(fig, use_container_width=True)
                            
                            with col_g2:
                                st.write(f"**{place} ã®çš„ä¸­è©³ç´°**")
                                st.dataframe(place_data[['R', 'é¦¬å', 'å±æ€§', 'ã‚¿ã‚¤ãƒ—', 'ç€é †']], use_container_width=True, hide_index=True)
                        else:
                            st.info("ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ‡ãƒ¼ã‚¿ãªã—")

                # --- å‚¾å‘ã‚¹ã‚³ã‚¢åŠ ç®— ---
                st.markdown("### ğŸ“ˆ æ¬¡ãƒ¬ãƒ¼ã‚¹ã®æ³¨ç›®é¦¬ (å‚¾å‘åŠ ç®—)")
                
                hit_patterns = set()
                for p in df_fuku['ãƒ‘ã‚¿ãƒ¼ãƒ³']:
                    if p: hit_patterns.update(str(p).split(','))
                
                future_races = current_df[current_df['ç€é †'].isna()].copy()
                
                if not future_races.empty:
                    def calc_bonus(row_pat):
                        if not row_pat or pd.isna(row_pat): return 0.0
                        pats = str(row_pat).split(',')
                        bonus = 0.0
                        for p in pats:
                            if p in hit_patterns and len(p) == 1: 
                                bonus += 2.0 
                        return bonus

                    future_races['å‚¾å‘åŠ ç‚¹'] = future_races['ãƒ‘ã‚¿ãƒ¼ãƒ³'].apply(calc_bonus)
                    future_races['ç·åˆã‚¹ã‚³ã‚¢'] = future_races['ã‚¹ã‚³ã‚¢'] + future_races['å‚¾å‘åŠ ç‚¹']
                    
                    hot_horses = future_races[future_races['å‚¾å‘åŠ ç‚¹'] > 0].sort_values(['å ´å', 'R', 'ç·åˆã‚¹ã‚³ã‚¢'], ascending=[True, True, False])
                    
                    if not hot_horses.empty:
                        st.success(f"æœ¬æ—¥å¥½èª¿ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æŒã¤é¦¬ãŒ {len(hot_horses)} é ­ã„ã¾ã™ï¼")
                        st.dataframe(
                            hot_horses[['å ´å', 'R', 'é¦¬å', 'ã‚¿ã‚¤ãƒ—', 'ãƒ‘ã‚¿ãƒ¼ãƒ³', 'ã‚¹ã‚³ã‚¢', 'å‚¾å‘åŠ ç‚¹', 'ç·åˆã‚¹ã‚³ã‚¢']],
                            use_container_width=True,
                            hide_index=True
                        )
                    else:
                        st.info("ç¾æ™‚ç‚¹ã§ã¯ã€ç‰¹å®šã®å‚¾å‘ã«åˆè‡´ã™ã‚‹æœªå‡ºèµ°é¦¬ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
            else:
                st.info("ã¾ã çš„ä¸­ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ç€é †ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        else:
            st.warning("æ¨å¥¨é¦¬ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
