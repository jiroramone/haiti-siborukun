import streamlit as st
import pandas as pd
import numpy as np
import re
import plotly.express as px
import openpyxl
import requests

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(page_title="é…ç½®é¦¬åˆ¸è¡“ Web", layout="wide")

# ==========================================
# 1. å…±é€šãƒ­ã‚¸ãƒƒã‚¯ & ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
# ==========================================

def to_half_width(text):
    if isinstance(text, (list, pd.Series, np.ndarray)):
        text = str(text)
    if pd.isna(text): return text
    text = str(text)
    table = str.maketrans('ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™', '0123456789')
    text = text.translate(table)
    text = re.sub(r'[^\d\.]', '', text)
    return text

def normalize_name(x):
    if pd.isna(x): return ''
    normalized_name = str(x).strip().replace('ã€€', '').replace(' ', '')
    normalized_name = re.sub(r'[â˜…â˜†â–²â–³â—‡]', '', normalized_name)
    if ',' in normalized_name: normalized_name = normalized_name.split(',')[0]
    text = re.sub(r'[0-9\.]+[Rr]', '', normalized_name)
    text = re.sub(r'\(.*?\)', '', text)
    return text.replace('/', '').strip()

@st.cache_data
def load_data(file):
    """ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã‚’è¡Œã†"""
    df = None
    if file.name.endswith('.xlsx'):
        try:
            file.seek(0)
            df = pd.read_excel(file, engine='openpyxl')
        except Exception as e:
            return pd.DataFrame(), f"Excelèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}"
    else:
        try:
            file.seek(0)
            df = pd.read_csv(file, encoding='utf-8', on_bad_lines='skip')
        except UnicodeDecodeError:
            try:
                file.seek(0)
                df = pd.read_csv(file, encoding='cp932', on_bad_lines='skip')
            except Exception as e:
                return pd.DataFrame(), f"CSVèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}"
        except Exception as e:
            return pd.DataFrame(), f"CSVäºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼: {e}"

    # ãƒ‡ãƒ¼ã‚¿æ•´å½¢
    df.columns = df.columns.str.strip()
    rename_map = {
        'å ´æ‰€': 'å ´å', 'é–‹å‚¬': 'å ´å', 
        'èª¿æ•™å¸«': 'å©èˆ', 'èª¿æ•™å¸«å': 'å©èˆ', 'å©èˆå': 'å©èˆ',
        'é¨æ‰‹å': 'é¨æ‰‹',
        'ãƒ¬ãƒ¼ã‚¹': 'R', 'ï¼²': 'R', 'ãƒ¬ãƒ¼ã‚¹å': 'R',
        'ç€': 'ç€é †', 'ç€ é †': 'ç€é †', 'ç•ª': 'æ­£ç•ª', 'é¦¬ç•ª': 'æ­£ç•ª',
        'å˜ã‚ªãƒƒã‚º': 'å˜ï½µï½¯ï½½ï¾', 'å˜å‹ã‚ªãƒƒã‚º': 'å˜ï½µï½¯ï½½ï¾', 'ã‚ªãƒƒã‚º': 'å˜ï½µï½¯ï½½ï¾', 'å˜å‹': 'å˜ï½µï½¯ï½½ï¾', 'å˜': 'å˜ï½µï½¯ï½½ï¾'
    }
    df = df.rename(columns=rename_map)
    df = df.loc[:, ~df.columns.duplicated()]

    if 'å ´å' not in df.columns: df['å ´å'] = 'Unknown'

    target_numeric_cols = ['R', 'æ­£ç•ª', 'å˜ï½µï½¯ï½½ï¾', 'é€†ç•ª', 'æ­£å¾ªç’°', 'é€†å¾ªç’°', 'é ­æ•°']
    for col in target_numeric_cols:
        if col in df.columns:
            df[col] = df[col].apply(to_half_width)
            df[col] = pd.to_numeric(df[col], errors='coerce')

    if 'R' not in df.columns or 'æ­£ç•ª' not in df.columns:
        return pd.DataFrame(), "ã‚¨ãƒ©ãƒ¼: å¿…é ˆåˆ—ï¼ˆãƒ¬ãƒ¼ã‚¹å/Rã€é¦¬ç•ª/æ­£ç•ªï¼‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"

    df = df.dropna(subset=['R', 'æ­£ç•ª'])
    df['R'] = df['R'].astype(int)
    df['æ­£ç•ª'] = df['æ­£ç•ª'].astype(int)

    for col in ['é¨æ‰‹', 'å©èˆ', 'é¦¬ä¸»']:
        if col in df.columns:
            df[col] = df[col].apply(normalize_name)
        else:
            df[col] = ''
            
    required_cols = ['R', 'å ´å', 'é¦¬å', 'æ­£ç•ª', 'é¨æ‰‹', 'å©èˆ', 'é¦¬ä¸»', 'å˜ï½µï½¯ï½½ï¾', 'é€†ç•ª', 'æ­£å¾ªç’°', 'é€†å¾ªç’°', 'é ­æ•°']
    for col in required_cols:
        if col not in df.columns:
            df[col] = np.nan

    save_cols = ['å±æ€§', 'ã‚¿ã‚¤ãƒ—', 'ãƒ‘ã‚¿ãƒ¼ãƒ³', 'æ¡ä»¶', 'ã‚¹ã‚³ã‚¢', 'ç€é †', 'å‚¾å‘åŠ ç‚¹', 'ç·åˆã‚¹ã‚³ã‚¢']
    existing_save_cols = [c for c in save_cols if c in df.columns]
    
    return df[required_cols + existing_save_cols].copy(), "success"

# â˜…ä¿®æ­£: Webã‹ã‚‰ã‚ªãƒƒã‚ºã‚’å–å¾—ã™ã‚‹é–¢æ•° (BeautifulSoupã‚’ä½¿ç”¨)
def fetch_odds_from_web(url):
    """
    æŒ‡å®šã•ã‚ŒãŸURLã‹ã‚‰ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€é¦¬ç•ªã¨å˜å‹ã‚ªãƒƒã‚ºã®ãƒšã‚¢ã‚’è¿”ã™
    """
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        }
        
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        
        # æ–‡å­—ã‚³ãƒ¼ãƒ‰ã®è‡ªå‹•åˆ¤å®šè£œæ­£
        if response.encoding == 'ISO-8859-1':
            response.encoding = response.apparent_encoding

        # â˜…å¤‰æ›´ç‚¹: bs4 ã¾ãŸã¯ html5lib ã‚’ä½¿ç”¨ã—ã¦èª­ã¿è¾¼ã‚€ï¼ˆlxmlã¯ä½¿ã‚ãªã„ï¼‰
        # ã“ã‚Œã«ã‚ˆã‚Šä¾å­˜é–¢ä¿‚ã®ã‚¨ãƒ©ãƒ¼ã‚’å›é¿ã—ã‚„ã™ãã™ã‚‹
        try:
            dfs = pd.read_html(response.text, flavor='bs4')
        except ImportError:
            try:
                dfs = pd.read_html(response.text, flavor='html5lib')
            except ImportError:
                # ä½•ã‚‚æŒ‡å®šã›ãšpandasã«ãŠä»»ã›
                dfs = pd.read_html(response.text)
        
        target_df = None
        for df in dfs:
            # ã‚«ãƒ©ãƒ åã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
            cols = [str(c).replace(' ', '').replace('\n', '') for c in df.columns]
            
            # ãƒãƒ«ãƒã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å¯¾å¿œ
            if isinstance(df.columns, pd.MultiIndex):
                flat_cols = []
                for c in df.columns:
                    flat_cols.append(''.join([str(x) for x in c if 'Unnamed' not in str(x)]))
                cols = flat_cols
                df.columns = cols

            # ã€Œé¦¬ç•ªã€ã¨ã€Œå˜å‹ã€ãŒå«ã¾ã‚Œã‚‹ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æ¢ã™
            if any('é¦¬ç•ª' in c for c in cols) and any('å˜å‹' in c for c in cols):
                col_map = {}
                for c, original_c in zip(cols, df.columns):
                    if 'é¦¬ç•ª' in c: col_map[original_c] = 'æ­£ç•ª'
                    elif 'å˜å‹' in c and 'ã‚ªãƒƒã‚º' in c: col_map[original_c] = 'å˜ï½µï½¯ï½½ï¾'
                    elif 'å˜å‹' in c: col_map[original_c] = 'å˜ï½µï½¯ï½½ï¾'
                
                if 'æ­£ç•ª' in col_map.values() and 'å˜ï½µï½¯ï½½ï¾' in col_map.values():
                    target_df = df.rename(columns=col_map)
                    break
        
        if target_df is not None:
            res = target_df[['æ­£ç•ª', 'å˜ï½µï½¯ï½½ï¾']].copy()
            res['æ­£ç•ª'] = pd.to_numeric(res['æ­£ç•ª'], errors='coerce')
            
            def clean_odds(x):
                try: return float(x)
                except: return np.nan
            
            res['å˜ï½µï½¯ï½½ï¾'] = res['å˜ï½µï½¯ï½½ï¾'].apply(clean_odds)
            res = res.dropna(subset=['æ­£ç•ª'])
            return res
        else:
            st.error("ãƒšãƒ¼ã‚¸å†…ã«ã‚ªãƒƒã‚ºè¡¨ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚URLãŒæ­£ã—ã„ã‹ç¢ºèªã—ã¦ãã ã•ã„ï¼ˆnetkeibaã®å‡ºé¦¬è¡¨ãƒšãƒ¼ã‚¸ãªã©ã‚’æ¨å¥¨ï¼‰ã€‚")
            return None

    except Exception as e:
        st.error(f"ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼: {e}")
        return None

# ==========================================
# 2. é…ç½®è¨ˆç®—ãƒ»åˆ†æãƒ­ã‚¸ãƒƒã‚¯
# ==========================================

def calc_haichi_numbers(df):
    check_cols = ['é€†ç•ª', 'æ­£å¾ªç’°', 'é€†å¾ªç’°']
    if set(check_cols).issubset(df.columns) and df[check_cols].notna().all().all():
        df['è¨ˆç®—_é€†ç•ª'] = df['é€†ç•ª']
        df['è¨ˆç®—_æ­£å¾ªç’°'] = df['æ­£å¾ªç’°']
        df['è¨ˆç®—_é€†å¾ªç’°'] = df['é€†å¾ªç’°']
        return df
    
    max_umaban = df.groupby(['å ´å', 'R'])['æ­£ç•ª'].transform('max')
    df['ä½¿ç”¨é ­æ•°'] = max_umaban.fillna(16).astype(int)
    if 'é ­æ•°' in df.columns:
        df['ä½¿ç”¨é ­æ•°'] = df['é ­æ•°'].fillna(df['ä½¿ç”¨é ­æ•°']).astype(int)
    df['ä½¿ç”¨é ­æ•°'] = np.maximum(df['ä½¿ç”¨é ­æ•°'], df['æ­£ç•ª'])
    
    def calc(row):
        t = int(row['ä½¿ç”¨é ­æ•°'])
        s = int(row['æ­£ç•ª'])
        g = (t + 1) - s
        sj = t + s
        gj = t + g
        return pd.Series([g, sj, gj])
    
    df[['è¨ˆç®—_é€†ç•ª', 'è¨ˆç®—_æ­£å¾ªç’°', 'è¨ˆç®—_é€†å¾ªç’°']] = df.apply(calc, axis=1)
    return df

def get_pair_pattern(row1, row2):
    def val(x):
        try: return int(float(x)) 
        except: return None
    r1 = [val(row1['æ­£ç•ª']), val(row1['è¨ˆç®—_é€†ç•ª']), val(row1['è¨ˆç®—_æ­£å¾ªç’°']), val(row1['è¨ˆç®—_é€†å¾ªç’°'])]
    r2 = [val(row2['æ­£ç•ª']), val(row2['è¨ˆç®—_é€†ç•ª']), val(row2['è¨ˆç®—_æ­£å¾ªç’°']), val(row2['è¨ˆç®—_é€†å¾ªç’°'])]
    label = list("ABCDEFGHIJKLMNOP")
    pairs = [label[i * 4 + j] for i in range(4) for j in range(4)
             if r1[i] is not None and r2[j] is not None and r1[i] == r2[j] and r1[i] != 0]
    return ",".join(pairs)

def get_common_values(group):
    cols = ['æ­£ç•ª', 'è¨ˆç®—_é€†ç•ª', 'è¨ˆç®—_æ­£å¾ªç’°', 'è¨ˆç®—_é€†å¾ªç’°']
    common_set = None
    for _, row in group.iterrows():
        current_set = set()
        for col in cols:
            val = row.get(col)
            if pd.notna(val):
                try:
                    num = int(float(val))
                    if num != 0: current_set.add(num)
                except: continue
        if common_set is None: common_set = current_set
        else: common_set = common_set.intersection(current_set)
        if not common_set: return None
    if common_set: return ','.join(map(str, sorted(list(common_set))))
    return None

def analyze_logic(df_curr, df_prev=None):
    df_curr = calc_haichi_numbers(df_curr)
    if df_prev is not None and not df_prev.empty:
        df_prev = calc_haichi_numbers(df_prev)
    
    rec_list = []
    
    # A. é’å¡—
    blue_keys = set()
    for col in ['é¨æ‰‹', 'å©èˆ', 'é¦¬ä¸»']:
        if col not in df_curr.columns: continue
        if col == 'é¨æ‰‹': group_keys = ['å ´å', col]
        else: group_keys = [col]
        try:
            for name_key, group in df_curr.groupby(group_keys):
                if len(group) < 2: continue
                target_name = name_key[1] if col == 'é¨æ‰‹' else name_key
                if not target_name: continue
                common_vals = get_common_values(group)
                if common_vals:
                    all_races_display = [f"{r['å ´å']}{r['R']}" for _, r in group.iterrows()]
                    priority = 1.0 if col == 'é¨æ‰‹' else 0.2
                    for _, row in group.iterrows():
                        current_race_str = f"{row['å ´å']}{row['R']}"
                        other_races = [s for s in all_races_display if s != current_race_str]
                        other_races = sorted(list(set(other_races)))
                        remark = f'[{col}] å…±é€šå€¤({common_vals}) [ä»–:{",".join(other_races)}]'
                        odds_val = row.get('å˜ï½µï½¯ï½½ï¾', np.nan)
                        rec_list.append({
                            'å ´å': row['å ´å'], 'R': row['R'], 'æ­£ç•ª': row['æ­£ç•ª'], 'é¦¬å': row['é¦¬å'],
                            'å˜ï½µï½¯ï½½ï¾': odds_val,
                            'å±æ€§': f"{col}:{target_name}", 
                            'ã‚¿ã‚¤ãƒ—': f'â˜… {col}é’å¡—', 
                            'ãƒ‘ã‚¿ãƒ¼ãƒ³': 'é’', 
                            'æ¡ä»¶': remark,
                            'ã‚¹ã‚³ã‚¢': 9.0 + priority
                        })
                        blue_keys.add((row['å ´å'], row['R'], row['é¦¬å'], row['å±æ€§']))
        except: continue

    # B. é’å¡—ã®éš£
    if blue_keys:
        blue_lookup = {}
        for b in blue_keys:
            key = (b[0], b[1]) 
            if key not in blue_lookup: blue_lookup[key] = []
            blue_lookup[key].append({'é¦¬å': b[2], 'å±æ€§': b[3]})

        for (place, race), group in df_curr.groupby(['å ´å', 'R']):
            key = (place, race)
            if key not in blue_lookup: continue
            blue_horses_info = blue_lookup[key]
            group = group.sort_values('æ­£ç•ª')
            umaban_map = {int(row['æ­£ç•ª']): row for _, row in group.iterrows()}
            blue_horse_names = [b['é¦¬å'] for b in blue_horses_info]

            for b_info in blue_horses_info:
                b_row = group[group['é¦¬å'] == b_info['é¦¬å']]
                if b_row.empty: continue
                b_row = b_row.iloc[0]
                curr_num = int(b_row['æ­£ç•ª'])
                source_attr = b_info['å±æ€§']
                blue_odds = pd.to_numeric(b_row.get('å˜ï½µï½¯ï½½ï¾'), errors='coerce')
                
                for t_num in [curr_num - 1, curr_num + 1]:
                    if t_num in umaban_map:
                        t_row = umaban_map[t_num]
                        if t_row['é¦¬å'] not in blue_horse_names:
                            neighbor_odds = pd.to_numeric(t_row.get('å˜ï½µï½¯ï½½ï¾'), errors='coerce')
                            neighbor_score = 9.0
                            
                            # éš£ã®ã‚ªãƒƒã‚º < æœ¬ä½“ã®ã‚ªãƒƒã‚º ãªã‚‰ã‚¹ã‚³ã‚¢åŠ ç®— (é€†è»¢)
                            if pd.notna(blue_odds) and pd.notna(neighbor_odds):
                                if neighbor_odds < blue_odds:
                                    neighbor_score += 2.0
                            
                            rec_list.append({
                                'å ´å': place, 'R': race, 'æ­£ç•ª': t_num, 'é¦¬å': t_row['é¦¬å'],
                                'å˜ï½µï½¯ï½½ï¾': neighbor_odds,
                                'å±æ€§': f"(é’å¡—éš£) <{source_attr}>", 
                                'ã‚¿ã‚¤ãƒ—': 'â–³ é’å¡—ã®éš£',
                                'ãƒ‘ã‚¿ãƒ¼ãƒ³': 'é’éš£',
                                'æ¡ä»¶': f"é’å¡—#{curr_num}({source_attr})ã®éš£",
                                'ã‚¹ã‚³ã‚¢': neighbor_score
                            })

    # C. é€šå¸¸ãƒšã‚¢ (é¨æ‰‹)
    if 'é¨æ‰‹' in df_curr.columns:
        for (place, name), group in df_curr.groupby(['å ´å', 'é¨æ‰‹']):
            if len(group) < 2: continue
            group = group.sort_values('R').to_dict('records')
            for i in range(len(group)-1):
                curr, next_r = group[i], group[i+1]
                pat = get_pair_pattern(curr, next_r)
                if pat:
                    label = "â— ãƒãƒ£ãƒ³ã‚¹" if any(x in pat for x in ['C','D','G','H']) else "â—‹ ç‹™ã„ç›®"
                    base_score = 4.0 if label.startswith("â—") else 3.0
                    rec_list.append({
                        'å ´å': curr['å ´å'], 'R': curr['R'], 'æ­£ç•ª': curr['æ­£ç•ª'], 'é¦¬å': curr['é¦¬å'],
                        'å˜ï½µï½¯ï½½ï¾': curr.get('å˜ï½µï½¯ï½½ï¾', np.nan),
                        'å±æ€§': f"é¨æ‰‹:{name}", 'ã‚¿ã‚¤ãƒ—': label, 'ãƒ‘ã‚¿ãƒ¼ãƒ³': pat, 
                        'æ¡ä»¶': f"[é¨æ‰‹] ãƒšã‚¢({next_r['R']}R #{next_r['æ­£ç•ª']})", 'ã‚¹ã‚³ã‚¢': base_score + 1.0
                    })
                    rec_list.append({
                        'å ´å': next_r['å ´å'], 'R': next_r['R'], 'æ­£ç•ª': next_r['æ­£ç•ª'], 'é¦¬å': next_r['é¦¬å'],
                        'å˜ï½µï½¯ï½½ï¾': next_r.get('å˜ï½µï½¯ï½½ï¾', np.nan),
                        'å±æ€§': f"é¨æ‰‹:{name}", 'ã‚¿ã‚¤ãƒ—': label, 'ãƒ‘ã‚¿ãƒ¼ãƒ³': pat, 
                        'æ¡ä»¶': f"[é¨æ‰‹] ãƒšã‚¢({curr['R']}R #{curr['æ­£ç•ª']})", 'ã‚¹ã‚³ã‚¢': base_score + 1.0
                    })

    # C. é€šå¸¸ãƒšã‚¢ (å©èˆãƒ»é¦¬ä¸»)
    for col in ['å©èˆ', 'é¦¬ä¸»']:
        if col not in df_curr.columns: continue
        for name, group in df_curr.groupby(col):
            if len(group) < 2: continue
            group = group.sort_values(['R', 'å ´å']).to_dict('records')
            for i in range(len(group)-1):
                curr, next_r = group[i], group[i+1]
                pat = get_pair_pattern(curr, next_r)
                if pat:
                    label = "â— ãƒãƒ£ãƒ³ã‚¹" if any(x in pat for x in ['C','D','G','H']) else "â—‹ ç‹™ã„ç›®"
                    base_score = 4.0 if label.startswith("â—") else 3.0
                    cond_curr = f"[{col}] ãƒšã‚¢({next_r['å ´å']}{next_r['R']}R #{next_r['æ­£ç•ª']})"
                    cond_next = f"[{col}] ãƒšã‚¢({curr['å ´å']}{curr['R']}R #{curr['æ­£ç•ª']})"
                    bonus = 0.2
                    rec_list.append({
                        'å ´å': curr['å ´å'], 'R': curr['R'], 'æ­£ç•ª': curr['æ­£ç•ª'], 'é¦¬å': curr['é¦¬å'],
                        'å˜ï½µï½¯ï½½ï¾': curr.get('å˜ï½µï½¯ï½½ï¾', np.nan),
                        'å±æ€§': f"{col}:{name}", 'ã‚¿ã‚¤ãƒ—': label, 'ãƒ‘ã‚¿ãƒ¼ãƒ³': pat, 
                        'æ¡ä»¶': cond_curr, 'ã‚¹ã‚³ã‚¢': base_score + bonus
                    })
                    rec_list.append({
                        'å ´å': next_r['å ´å'], 'R': next_r['R'], 'æ­£ç•ª': next_r['æ­£ç•ª'], 'é¦¬å': next_r['é¦¬å'],
                        'å˜ï½µï½¯ï½½ï¾': next_r.get('å˜ï½µï½¯ï½½ï¾', np.nan),
                        'å±æ€§': f"{col}:{name}", 'ã‚¿ã‚¤ãƒ—': label, 'ãƒ‘ã‚¿ãƒ¼ãƒ³': pat, 
                        'æ¡ä»¶': cond_next, 'ã‚¹ã‚³ã‚¢': base_score + bonus
                    })

    # D. å‰æ—¥åŒé…ç½®
    if df_prev is not None and not df_prev.empty:
        for idx, row in df_curr.iterrows():
            race = row['R']
            name = row['é¨æ‰‹']
            if not name: continue
            prev_rows = df_prev[(df_prev['å ´å'] == row['å ´å']) & (df_prev['R'] == race) & (df_prev['é¨æ‰‹'] == name)]
            for _, p_row in prev_rows.iterrows():
                is_seiban = (p_row['æ­£ç•ª'] == row['æ­£ç•ª'])
                is_gyaku = (p_row['è¨ˆç®—_é€†ç•ª'] == row['è¨ˆç®—_é€†ç•ª'])
                if is_seiban or is_gyaku:
                    reason = "æ­£ç•ª" if is_seiban else "é€†ç•ª"
                    prev_rank = pd.to_numeric(p_row.get('ç€é †'), errors='coerce')
                    condition_text = f"[é¨æ‰‹] å‰æ—¥{race}RåŒé…ç½®({reason})"
                    if pd.notna(prev_rank):
                        if prev_rank > 3: condition_text += " <âš ï¸å‰æ—¥å‡¡èµ°>"
                        else: condition_text += " <âœ¨å‰æ—¥å¥½èµ°>"
                    
                    rec_list.append({
                        'å ´å': row['å ´å'], 'R': race, 'æ­£ç•ª': row['æ­£ç•ª'], 'é¦¬å': row['é¦¬å'],
                        'å˜ï½µï½¯ï½½ï¾': row.get('å˜ï½µï½¯ï½½ï¾', np.nan),
                        'å±æ€§': f"é¨æ‰‹:{name}", 'ã‚¿ã‚¤ãƒ—': 'â˜… å‰æ—¥åŒé…ç½®', 
                        'ãƒ‘ã‚¿ãƒ¼ãƒ³': 'å‰æ—¥',
                        'æ¡ä»¶': condition_text, 
                        'ã‚¹ã‚³ã‚¢': 8.3
                    })

    if not rec_list:
        return pd.DataFrame()
        
    res_df = pd.DataFrame(rec_list)
    
    agg_funcs = {
        'å˜ï½µï½¯ï½½ï¾': 'min',
        'å±æ€§': lambda x: ' + '.join(sorted(set(x))),
        'ã‚¿ã‚¤ãƒ—': lambda x: ' / '.join(sorted(set(x), key=lambda s: 0 if 'â˜…' in s else 1)), 
        'ãƒ‘ã‚¿ãƒ¼ãƒ³': lambda x: ','.join(sorted(set(x))),
        'æ¡ä»¶': lambda x: ' / '.join(sorted(set(x))),
        'ã‚¹ã‚³ã‚¢': 'sum',
        'æ­£ç•ª': 'first'
    }
    
    res_df = res_df.groupby(['å ´å', 'R', 'é¦¬å'], as_index=False).agg(agg_funcs)
    res_df = res_df.sort_values(['å ´å', 'R', 'ã‚¹ã‚³ã‚¢'], ascending=[True, True, False])
    
    if 'ç€é †' not in res_df.columns: res_df['ç€é †'] = np.nan
    
    return res_df

# ==========================================
# 3. ç·åˆè©•ä¾¡ãƒ»å†è¨ˆç®—ãƒ­ã‚¸ãƒƒã‚¯
# ==========================================

def apply_ranking_logic(df_in):
    """æœ€æ–°ã®ã‚ªãƒƒã‚ºã‚„ãƒˆãƒ¬ãƒ³ãƒ‰ã«åŸºã¥ã„ã¦ã‚¹ã‚³ã‚¢ã¨æ¨å¥¨åº¦ã‚’å†è¨ˆç®—ã™ã‚‹"""
    if df_in.empty: return df_in
    df = df_in.copy()
    
    df['ç€é †'] = pd.to_numeric(df['ç€é †'], errors='coerce')
    df_hits = df[df['ç€é †'] <= 3]
    
    hit_patterns = set()
    downgraded_attrs = set()
    
    for _, row in df_hits.iterrows():
        pats = str(row.get('ãƒ‘ã‚¿ãƒ¼ãƒ³', '')).split(',')
        hit_patterns.update(pats)
        if 'é’éš£' in str(row.get('ãƒ‘ã‚¿ãƒ¼ãƒ³', '')):
            found = re.findall(r'<(.*?)>', str(row.get('å±æ€§', '')))
            downgraded_attrs.update(found)

    def calc_bonus(row):
        row_pat = row.get('ãƒ‘ã‚¿ãƒ¼ãƒ³', '')
        if not row_pat or pd.isna(row_pat): return 0.0
        pats = str(row_pat).split(',')
        bonus = 0.0
        
        # 1. ãƒ’ãƒƒãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³åŠ ç‚¹ (ãƒˆãƒ¬ãƒ³ãƒ‰) +4.0
        for p in pats:
            if p in hit_patterns and len(p) == 1: 
                bonus += 4.0 
        
        # 2. é’å¡—å‡¦ç†
        if 'é’' in pats:
            my_attrs = str(row.get('å±æ€§', ''))
            for bad_attr in downgraded_attrs:
                if bad_attr in my_attrs:
                    bonus -= 3.0
                    break
        
        # 3. é«˜ã‚ªãƒƒã‚ºã«ã‚ˆã‚‹æ¸›ç‚¹ (50å€ä»¥ä¸Šã¯åœå¤–)
        odds = pd.to_numeric(row.get('å˜ï½µï½¯ï½½ï¾'), errors='coerce')
        if pd.notna(odds) and odds > 49.9:
            bonus -= 30.0
                
        return bonus

    def get_bet_recommendation(row):
        score = row['ç·åˆã‚¹ã‚³ã‚¢']
        rank_in_race = row['ãƒ¬ãƒ¼ã‚¹å†…é †ä½']
        pat_str = str(row.get('ãƒ‘ã‚¿ãƒ¼ãƒ³', ''))
        my_pats = pat_str.split(',')
        matched = [p for p in my_pats if p in hit_patterns]
        is_trend_horse = len(matched) > 0
        is_blue = 'é’' in my_pats

        if score >= 15: rank = "S"
        elif score >= 12: rank = "A"
        elif score >= 10: rank = "B"
        elif is_blue: rank = "C"
        else: rank = "D"

        if rank_in_race > 1:
            if rank == "S": rank = "A"
            elif rank == "A": rank = "B"
        
        if rank == "S":
            return "ğŸ‘‘ ç›¤çŸ³ã®è»¸" if is_trend_horse else "ğŸ‘‘ é‰„æ¿ç´š"
        elif rank == "A":
            return "âœ¨ å‚¾å‘è»¸" if is_trend_horse else "â— è»¸å€™è£œ"
        elif rank == "B":
            return "ğŸ”¥ æ¿€ç†±ç›¸æ‰‹" if is_trend_horse else "â—‹ ç›¸æ‰‹ç­†é ­"
        elif rank == "C":
            return "â˜… å‚¾å‘åˆè‡´ç©´" if is_trend_horse else "â–² é’å¡—ç©´"
        else: 
            if is_trend_horse: return "æ³¨ å‚¾å‘åˆè‡´"
            return "â–³ ç´"

    df['å‚¾å‘åŠ ç‚¹'] = df.apply(calc_bonus, axis=1)
    df['ç·åˆã‚¹ã‚³ã‚¢'] = df['ã‚¹ã‚³ã‚¢'] + df['å‚¾å‘åŠ ç‚¹']
    df['ãƒ¬ãƒ¼ã‚¹å†…é †ä½'] = df.groupby(['å ´å', 'R'])['ç·åˆã‚¹ã‚³ã‚¢'].rank(method='min', ascending=False)
    df['æ¨å¥¨è²·ã„ç›®'] = df.apply(get_bet_recommendation, axis=1)
    
    return df

# ==========================================
# 4. Webã‚¢ãƒ—ãƒªç”»é¢ (Streamlit)
# ==========================================

st.title("ğŸ‡ é…ç½®é¦¬åˆ¸è¡“ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ åˆ†æ")

# ã‚µã‚¤ãƒ‰ãƒãƒ¼
with st.sidebar:
    st.header("1. ãƒ‡ãƒ¼ã‚¿å…¥åŠ›")
    uploaded_file = st.file_uploader("å½“æ—¥ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ (ã¾ãŸã¯ä¿å­˜ãƒ‡ãƒ¼ã‚¿)", type=['xlsx', 'csv'])
    prev_file = st.file_uploader("å‰æ—¥ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ (ä»»æ„)", type=['xlsx', 'csv'])
    
    st.markdown("---")
    st.header("2. ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜")
    st.caption("ç€é †ã‚’å…¥åŠ›ã—ãŸçŠ¶æ…‹ã§ã“ã“ã‹ã‚‰CSVã‚’ä¿å­˜ã—ã€æ¬¡å›èª­ã¿è¾¼ã‚€ã¨ç¶šãã‹ã‚‰å†é–‹ã§ãã¾ã™ã€‚")
    
    if 'analyzed_df' in st.session_state and not st.session_state['analyzed_df'].empty:
        csv = st.session_state['analyzed_df'].to_csv(index=False).encode('utf-8-sig')
        st.download_button(
            label="ğŸ’¾ ç¾åœ¨ã®çŠ¶æ…‹ã‚’ä¿å­˜ (CSV)",
            data=csv,
            file_name="race_progress_save.csv",
            mime="text/
