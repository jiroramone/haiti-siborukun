import streamlit as st
import pandas as pd
import numpy as np
import re
import requests
from bs4 import BeautifulSoup

# --- 1. åŸºæœ¬è¨­å®š ---
st.set_page_config(page_title="é…ç½®é¦¬åˆ¸è¡“ åˆ†æã‚·ã‚¹ãƒ†ãƒ ", layout="wide")

def to_half_width(text):
    if pd.isna(text): return text
    text = str(text)
    table = str.maketrans('ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™ï¼', '0123456789.')
    return re.sub(r'[^\d\.]', '', text.translate(table))

def normalize_name(x):
    if pd.isna(x): return ''
    return re.sub(r'[â˜…â˜†â–²â–³â—‡]', '', str(x).strip().replace('ã€€', '').replace(' ', ''))

# --- 2. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆãƒ˜ãƒƒãƒ€ãƒ¼è‡ªå‹•æ¢ç´¢æ©Ÿèƒ½ä»˜ãï¼‰ ---
@st.cache_data
def load_data(file):
    try:
        if file.name.endswith('.xlsx'):
            df = pd.read_excel(file, engine='openpyxl')
        else:
            try: df = pd.read_csv(file, encoding='utf-8')
            except: df = pd.read_csv(file, encoding='cp932')
        
        # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚’è‡ªå‹•ã§æ¢ã™ï¼ˆ1è¡Œç›®ãŒç©ºç™½ãªã©ã®å ´åˆã«å¯¾å¿œï¼‰
        if not any(col in str(df.columns) for col in ['é¦¬', 'ç•ª', 'R', 'é¨']):
            for i in range(min(len(df), 10)):
                row_vals = str(df.iloc[i].values)
                if any(x in row_vals for x in ['é¦¬', 'ç•ª', 'R']):
                    df.columns = df.iloc[i]
                    df = df.iloc[i+1:].reset_index(drop=True)
                    break

        df.columns = df.columns.astype(str).str.strip()
        name_map = {
            'å ´æ‰€': 'å ´å', 'é–‹å‚¬': 'å ´å', 'ç«¶é¦¬å ´': 'å ´å',
            'èª¿æ•™å¸«': 'å©èˆ', 'èª¿æ•™å¸«å': 'å©èˆ', 'å©èˆå': 'å©èˆ',
            'é¨æ‰‹å': 'é¨æ‰‹', 'ãƒ¬ãƒ¼ã‚¹': 'R', 'ç•ª': 'æ­£ç•ª', 'é¦¬ç•ª': 'æ­£ç•ª',
            'å˜ã‚ªãƒƒã‚º': 'å˜ï½µï½¯ï½½ï¾', 'å˜å‹ã‚ªãƒƒã‚º': 'å˜ï½µï½¯ï½½ï¾', 'ã‚ªãƒƒã‚º': 'å˜ï½µï½¯ï½½ï¾', 'å˜å‹': 'å˜ï½µï½¯ï½½ï¾'
        }
        df = df.rename(columns=name_map)
        
        # å¿…é ˆåˆ—ã®å­˜åœ¨ç¢ºèªã¨ä½œæˆ
        ensure_cols = ['R', 'æ­£ç•ª', 'é¦¬å', 'é¨æ‰‹', 'å©èˆ', 'é¦¬ä¸»', 'å ´å', 'å˜ï½µï½¯ï½½ï¾', 'ç€é †']
        for col in ensure_cols:
            if col not in df.columns: df[col] = np.nan

        # æ•°å€¤åŒ–ã¨ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
        df['R'] = pd.to_numeric(df['R'].apply(to_half_width), errors='coerce')
        df['æ­£ç•ª'] = pd.to_numeric(df['æ­£ç•ª'].apply(to_half_width), errors='coerce')
        df = df.dropna(subset=['R', 'æ­£ç•ª'])
        df['R'] = df['R'].astype(int); df['æ­£ç•ª'] = df['æ­£ç•ª'].astype(int)

        for col in ['é¨æ‰‹', 'å©èˆ', 'é¦¬ä¸»', 'é¦¬å', 'å ´å']:
            df[col] = df[col].apply(normalize_name)
        
        df['å˜ï½µï½¯ï½½ï¾'] = pd.to_numeric(df['å˜ï½µï½¯ï½½ï¾'].apply(to_half_width), errors='coerce')
        
        return df.copy(), "success"
    except Exception as e: return pd.DataFrame(), str(e)

# --- 3. é…ç½®è¨ˆç®—ã‚¨ãƒ³ã‚¸ãƒ³ (è¿½è¨˜å‹ãƒ»ç¢ºå®Ÿç‰ˆ) ---
def analyze_haichi(df):
    df = df.copy()
    
    # åŸºç¤æ•°å€¤è¨ˆç®—
    max_umaban = df.groupby(['å ´å', 'R'])['æ­£ç•ª'].transform('max')
    df['é ­æ•°'] = max_umaban.fillna(16).astype(int)
    if 'é ­æ•°' in df.columns and df['é ­æ•°'].notna().any():
         df['é ­æ•°'] = pd.to_numeric(df['é ­æ•°'], errors='coerce').fillna(df['é ­æ•°']).astype(int)
         
    df['é€†ç•ª'] = (df['é ­æ•°'] + 1) - df['æ­£ç•ª']
    df['æ­£å¾ªç’°'] = df['é ­æ•°'] + df['æ­£ç•ª']
    df['é€†å¾ªç’°'] = df['é ­æ•°'] + df['é€†ç•ª']

    # çµæœæ›¸ãè¾¼ã¿ç”¨ã®åˆ—ã‚’åˆæœŸåŒ–ï¼ˆã“ã“ãŒé‡è¦ï¼‰
    # ãƒªã‚¹ãƒˆå‹ã«ã—ã¦ãŠãã“ã¨ã§ã€è¤‡æ•°ã®ã‚¿ã‚°ï¼ˆé’å¡—ã‹ã¤ãƒšã‚¢ãªã©ï¼‰ã‚’åŒå±…ã•ã›ã‚‹
    df['ã‚¿ã‚¤ãƒ—_list'] = [[] for _ in range(len(df))]
    df['ãƒ‘ã‚¿ãƒ¼ãƒ³_list'] = [[] for _ in range(len(df))]
    df['æ¡ä»¶_list'] = [[] for _ in range(len(df))]
    df['ã‚¹ã‚³ã‚¢'] = 0.0

    # é«˜é€Ÿæ¤œç´¢ç”¨ã®ãƒãƒƒãƒ”ãƒ³ã‚°è¾æ›¸ã‚’ä½œæˆ (Key: (å ´å, R, æ­£ç•ª) -> Value: DataFrameã®index)
    # ã“ã‚Œã«ã‚ˆã‚Šã€ç¢ºå®Ÿã«ãã®é¦¬ã®è¡Œã‚’ç‰¹å®šã—ã¦æ›¸ãè¾¼ã‚ã‚‹
    idx_map = {}
    for idx, row in df.iterrows():
        idx_map[(row['å ´å'], row['R'], row['æ­£ç•ª'])] = idx

    # --- A. é’å¡—åˆ†æ (é¨æ‰‹ãƒ»å©èˆãƒ»é¦¬ä¸») ---
    blue_horses = [] # éš£åˆ¤å®šç”¨ã«è¨˜éŒ²
    
    for col in ['é¨æ‰‹', 'å©èˆ', 'é¦¬ä¸»']:
        if df[col].isna().all(): continue
        
        group_keys = ['å ´å', col] if col == 'é¨æ‰‹' else [col]
        # ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã—ã¦å…±é€šå€¤ã‚’æ¢ã™
        for name, group in df.groupby(group_keys):
            if len(group) < 2 or not name: continue
            
            # 4ã¤ã®æ•°å­—ã®ã‚»ãƒƒãƒˆã‚’å–å¾—
            cols_val = ['æ­£ç•ª', 'é€†ç•ª', 'æ­£å¾ªç’°', 'é€†å¾ªç’°']
            common = None
            for _, row in group.iterrows():
                cur_v = {int(row[c]) for c in cols_val if pd.notna(row[c])}
                common = cur_v if common is None else common.intersection(cur_v)
            
            # å…±é€šå€¤ãŒã‚ã‚Œã°æ›¸ãè¾¼ã¿
            if common:
                priority = 1.0 if col == 'é¨æ‰‹' else 0.2
                c_text = ','.join(map(str, sorted(list(common))))
                
                for _, row in group.iterrows():
                    idx = idx_map.get((row['å ´å'], row['R'], row['æ­£ç•ª']))
                    if idx is not None:
                        df.at[idx, 'ã‚¿ã‚¤ãƒ—_list'].append(f'â˜…{col}é’å¡—')
                        df.at[idx, 'ãƒ‘ã‚¿ãƒ¼ãƒ³_list'].append('é’')
                        df.at[idx, 'æ¡ä»¶_list'].append(f'å…±é€š({c_text})')
                        df.at[idx, 'ã‚¹ã‚³ã‚¢'] += 9.0 + priority
                        
                        # é’å¡—ãƒªã‚¹ãƒˆã«è¿½åŠ 
                        blue_horses.append({
                            'å ´å': row['å ´å'], 'R': row['R'], 'æ­£ç•ª': row['æ­£ç•ª'],
                            'å±æ€§': f"{col}:{name}", 'å˜ï½µï½¯ï½½ï¾': row['å˜ï½µï½¯ï½½ï¾']
                        })

    # --- B. é’å¡—ã®éš£ (é€†è»¢åˆ¤å®š) ---
    for b in blue_horses:
        for t_num in [b['æ­£ç•ª']-1, b['æ­£ç•ª']+1]:
            key = (b['å ´å'], b['R'], t_num)
            if key in idx_map:
                idx = idx_map[key]
                n_score = 9.0
                is_reverse = False
                
                b_odds = b['å˜ï½µï½¯ï½½ï¾']
                t_odds = df.at[idx, 'å˜ï½µï½¯ï½½ï¾']
                
                if pd.notna(b_odds) and pd.notna(t_odds):
                    if t_odds < b_odds:
                        n_score += 2.0
                        is_reverse = True
                
                # é‡è¤‡å›é¿
                if not any('é’å¡—éš£' in x for x in df.at[idx, 'ã‚¿ã‚¤ãƒ—_list']):
                    df.at[idx, 'ã‚¿ã‚¤ãƒ—_list'].append('â–³é’å¡—éš£' + ('(é€†è»¢)' if is_reverse else ''))
                    df.at[idx, 'ãƒ‘ã‚¿ãƒ¼ãƒ³_list'].append('é’éš£')
                    df.at[idx, 'æ¡ä»¶_list'].append(f"#{b['æ­£ç•ª']}ã®éš£")
                    df.at[idx, 'ã‚¹ã‚³ã‚¢'] += n_score

    # --- C. ãƒšã‚¢åˆ†æ ---
    pair_labels = list("ABCDEFGHIJKLMNOP")
    for col in ['é¨æ‰‹', 'å©èˆ', 'é¦¬ä¸»']:
        if df[col].isna().all(): continue
        
        for name, group in df.groupby(['å ´å', col] if col=='é¨æ‰‹' else col):
            if len(group) < 2 or not name: continue
            sorted_rows = group.sort_values('R').to_dict('records')
            
            for i in range(len(sorted_rows)-1):
                r1 = sorted_rows[i]
                r2 = sorted_rows[i+1]
                
                v1 = [r1[c] for c in ['æ­£ç•ª', 'é€†ç•ª', 'æ­£å¾ªç’°', 'é€†å¾ªç’°']]
                v2 = [r2[c] for c in ['æ­£ç•ª', 'é€†ç•ª', 'æ­£å¾ªç’°', 'é€†å¾ªç’°']]
                
                pats = []
                for x in range(4):
                    for y in range(4):
                        if v1[x] == v2[y] and v1[x] != 0:
                            pats.append(pair_labels[x*4+y])
                
                if pats:
                    p_str = "".join(pats)
                    is_chance = any(x in pats for x in ['C','D','G','H'])
                    type_str = 'â—ãƒãƒ£ãƒ³ã‚¹' if is_chance else 'â—‹ç‹™ã„ç›®'
                    score_add = 4.0 if is_chance else 3.0
                    
                    # R1ã¸ã®æ›¸ãè¾¼ã¿
                    idx1 = idx_map.get((r1['å ´å'], r1['R'], r1['æ­£ç•ª']))
                    if idx1 is not None:
                        df.at[idx1, 'ã‚¿ã‚¤ãƒ—_list'].append(type_str)
                        df.at[idx1, 'ãƒ‘ã‚¿ãƒ¼ãƒ³_list'].append(p_str)
                        df.at[idx1, 'æ¡ä»¶_list'].append(f"ãƒšã‚¢({r2['R']}R)")
                        df.at[idx1, 'ã‚¹ã‚³ã‚¢'] += score_add
                    
                    # R2ã¸ã®æ›¸ãè¾¼ã¿
                    idx2 = idx_map.get((r2['å ´å'], r2['R'], r2['æ­£ç•ª']))
                    if idx2 is not None:
                        df.at[idx2, 'ã‚¿ã‚¤ãƒ—_list'].append(type_str)
                        df.at[idx2, 'ãƒ‘ã‚¿ãƒ¼ãƒ³_list'].append(p_str)
                        df.at[idx2, 'æ¡ä»¶_list'].append(f"ãƒšã‚¢({r1['R']}R)")
                        df.at[idx2, 'ã‚¹ã‚³ã‚¢'] += score_add

    # ãƒªã‚¹ãƒˆã‚’æ–‡å­—åˆ—ã«æˆ»ã™
    df['ã‚¿ã‚¤ãƒ—'] = df['ã‚¿ã‚¤ãƒ—_list'].apply(lambda x: ' / '.join(x))
    df['ãƒ‘ã‚¿ãƒ¼ãƒ³'] = df['ãƒ‘ã‚¿ãƒ¼ãƒ³_list'].apply(lambda x: ','.join(x))
    df['æ¡ä»¶'] = df['æ¡ä»¶_list'].apply(lambda x: ' '.join(x))
    
    return df

# --- 4. Webã‚ªãƒƒã‚ºå–å¾— ---
def fetch_odds(url):
    try:
        headers = {"User-Agent": "Mozilla/5.0"}
        resp = requests.get(url, headers=headers, timeout=10)
        resp.encoding = 'euc-jp'
        soup = BeautifulSoup(resp.content, 'html.parser')
        rows = soup.select('tr.HorseList')
        data = []
        for r in rows:
            u = r.select_one('td[class*="Umaban"]')
            o = r.select_one('td[class*="Popular"]')
            if u:
                u_n = u.get_text(strip=True)
                o_v = re.sub(r'\(.*?\)', '', o.get_text(strip=True)) if o else 'nan'
                try: dv = float(o_v)
                except: dv = np.nan
                data.append({'æ­£ç•ª': int(u_n), 'å˜ï½µï½¯ï½½ï¾': dv})
        return pd.DataFrame(data) if data else None
    except: return None

# --- 5. UIæ§‹æˆ ---
st.title("ğŸ‡ é…ç½®é¦¬åˆ¸è¡“ åˆ†æã‚·ã‚¹ãƒ†ãƒ ")

with st.sidebar:
    up_file = st.file_uploader("å½“æ—¥ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type=['xlsx', 'csv'])
    if 'analyzed_df' in st.session_state:
        csv = st.session_state['analyzed_df'].to_csv(index=False).encode('utf-8-sig')
        st.download_button("ğŸ’¾ ç¾åœ¨ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜", csv, "race_result.csv")

if up_file:
    df_raw, status = load_data(up_file)
    if status == "success":
        if 'analyzed_df' not in st.session_state:
            st.session_state['analyzed_df'] = analyze_haichi(df_raw)

        full_df = st.session_state['analyzed_df']
        
        # ã‚¨ãƒ©ãƒ¼å›é¿: ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ãªã„ã‹ç¢ºèª
        if full_df.empty:
            st.error("æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãŒèª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸã€‚")
        else:
            places = sorted(full_df['å ´å'].unique())
            p_tabs = st.tabs(places)
            
            for p_tab, place in zip(p_tabs, places):
                with p_tab:
                    p_df = full_df[full_df['å ´å'] == place]
                    r_list = sorted(p_df['R'].unique())
                    r_tabs = st.tabs([f"{r}R" for r in r_list])
                    for r_tab, r_num in zip(r_tabs, r_list):
                        with r_tab:
                            # ã‚ªãƒƒã‚ºæ›´æ–°
                            with st.expander("ğŸŒ ãƒãƒƒãƒˆç«¶é¦¬ã‹ã‚‰æœ€æ–°ã‚ªãƒƒã‚ºã‚’å–å¾—"):
                                u_in = st.text_input("URLã‚’è²¼ã‚Šä»˜ã‘", key=f"u_{place}_{r_num}")
                                if st.button("ã‚ªãƒƒã‚ºæ›´æ–°å®Ÿè¡Œ", key=f"b_{place}_{r_num}"):
                                    new_o = fetch_odds(u_in)
                                    if new_o is not None:
                                        curr_df = st.session_state['analyzed_df']
                                        for _, row in new_o.iterrows():
                                            mask = (curr_df['å ´å']==place) & (curr_df['R']==r_num) & (curr_df['æ­£ç•ª']==row['æ­£ç•ª'])
                                            curr_df.loc[mask, 'å˜ï½µï½¯ï½½ï¾'] = row['å˜ï½µï½¯ï½½ï¾']
                                        
                                        st.session_state['analyzed_df'] = analyze_haichi(curr_df)
                                        st.success("æ›´æ–°å®Œäº†ï¼å†è¨ˆç®—ã—ã¾ã—ãŸã€‚")
                                        st.rerun()
                                    else:
                                        st.error("å–å¾—å¤±æ•—ã€‚URLã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

                            # ãƒ‡ãƒ¼ã‚¿è¡¨ç¤º
                            disp_df = st.session_state['analyzed_df']
                            disp_df = disp_df[(disp_df['å ´å']==place) & (disp_df['R']==r_num)].sort_values('æ­£ç•ª')
                            
                            # è¡¨ç¤ºåˆ—ã®å®šç¾©ï¼ˆå­˜åœ¨ã™ã‚‹ã‚‚ã®ã ã‘è¡¨ç¤ºï¼‰
                            cols_to_show = ['æ­£ç•ª', 'é¦¬å', 'é¨æ‰‹', 'å˜ï½µï½¯ï½½ï¾', 'ã‚¿ã‚¤ãƒ—', 'ãƒ‘ã‚¿ãƒ¼ãƒ³', 'æ¡ä»¶', 'ã‚¹ã‚³ã‚¢']
                            final_cols = [c for c in cols_to_show if c in disp_df.columns]

                            def highlight_row(row):
                                styles = [''] * len(row)
                                sc = row.get('ã‚¹ã‚³ã‚¢', 0)
                                tp = str(row.get('ã‚¿ã‚¤ãƒ—', ''))
                                if sc >= 10: return ['background-color: #ffcccc'] * len(row)
                                elif 'é’' in tp: return ['background-color: #e6f3ff'] * len(row)
                                return styles

                            st.dataframe(
                                disp_df[final_cols].style.apply(highlight_row, axis=1),
                                use_container_width=True,
                                hide_index=True
                            )
