import streamlit as st
import pandas as pd
import numpy as np
import re
import plotly.express as px

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(page_title="é…ç½®é¦¬åˆ¸è¡“ Web", layout="wide")

# ==========================================
# 1. å…±é€šãƒ­ã‚¸ãƒƒã‚¯ (æ—¢å­˜ã®é–¢æ•°ã‚’ç§»æ¤)
# ==========================================

def to_half_width(text):
    if pd.isna(text): return text
    text = str(text)
    table = str.maketrans('ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™', '0123456789')
    text = text.translate(table)
    text = re.sub(r'[^\d\.]', '', text)
    return text

def normalize_name(x):
    if pd.isna(x): return ''
    normalized_name = str(x).strip().replace('ã€€', '').replace(' ', '')
    normalized_name = re.sub(r'[â˜…â˜†â–²â–³â—‡]', '', normalized_name)
    if ',' in normalized_name: normalized_name = normalized_name.split(',')[0]
    text = re.sub(r'[0-9\.]+[Rr]', '', normalized_name)
    text = re.sub(r'\(.*?\)', '', text)
    return text.replace('/', '').strip()

@st.cache_data
def load_data(file):
    try:
        if file.name.endswith('.csv'):
            df = pd.read_csv(file, encoding='cp932', on_bad_lines='skip')
        else:
            df = pd.read_excel(file, engine='openpyxl')
    except:
        return pd.DataFrame(), "ã‚¨ãƒ©ãƒ¼"

    df.columns = df.columns.str.strip()
    
    rename_map = {
        'å ´æ‰€': 'å ´å', 'é–‹å‚¬': 'å ´å', 'å˜ã‚ªãƒƒã‚º': 'å˜ï½µï½¯ï½½ï¾', 
        'èª¿æ•™å¸«': 'å©èˆ', 'èª¿æ•™å¸«å': 'å©èˆ', 'é¨æ‰‹å': 'é¨æ‰‹',
        'ãƒ¬ãƒ¼ã‚¹': 'R', 'ï¼²': 'R', 'ãƒ¬ãƒ¼ã‚¹å': 'R',
        'ç€': 'ç€é †', 'ç€ é †': 'ç€é †', 'ç•ª': 'æ­£ç•ª', 'é¦¬ç•ª': 'æ­£ç•ª'
    }
    df = df.rename(columns=rename_map)
    if 'å ´å' not in df.columns: df['å ´å'] = 'Unknown'

    target_numeric_cols = ['R', 'æ­£ç•ª', 'å˜ï½µï½¯ï½½ï¾', 'é€†ç•ª', 'æ­£å¾ªç’°', 'é€†å¾ªç’°', 'é ­æ•°']
    for col in target_numeric_cols:
        if col in df.columns:
            df[col] = df[col].apply(to_half_width)
            df[col] = pd.to_numeric(df[col], errors='coerce')

    df = df.dropna(subset=['R', 'æ­£ç•ª'])
    df['R'] = df['R'].astype(int)
    df['æ­£ç•ª'] = df['æ­£ç•ª'].astype(int)

    for col in ['é¨æ‰‹', 'å©èˆ', 'é¦¬ä¸»']:
        if col in df.columns:
            df[col] = df[col].apply(normalize_name)
        else:
            df[col] = ''
            
    # åˆ†æç”¨ãƒ‡ãƒ¼ã‚¿ã®å¿…é ˆåˆ—ç¢ºä¿
    potential_cols = ['R', 'å ´å', 'é¦¬å', 'æ­£ç•ª', 'é¨æ‰‹', 'å©èˆ', 'é¦¬ä¸»', 'å˜ï½µï½¯ï½½ï¾', 'é€†ç•ª', 'æ­£å¾ªç’°', 'é€†å¾ªç’°', 'é ­æ•°']
    for col in potential_cols:
        if col not in df.columns: df[col] = np.nan

    return df[potential_cols].copy(), "success"

# --- é…ç½®è¨ˆç®—ãƒ­ã‚¸ãƒƒã‚¯ (çœç•¥ã›ãšãã®ã¾ã¾ä½¿ç”¨) ---
def calc_haichi_numbers(df):
    if df[['é€†ç•ª', 'æ­£å¾ªç’°', 'é€†å¾ªç’°']].notna().all().all():
        df['è¨ˆç®—_é€†ç•ª'] = df['é€†ç•ª']
        return df
    max_umaban = df.groupby(['å ´å', 'R'])['æ­£ç•ª'].transform('max')
    df['ä½¿ç”¨é ­æ•°'] = max_umaban.fillna(16).astype(int)
    if 'é ­æ•°' in df.columns:
        df['ä½¿ç”¨é ­æ•°'] = df['é ­æ•°'].fillna(df['ä½¿ç”¨é ­æ•°']).astype(int)
    
    df['ä½¿ç”¨é ­æ•°'] = np.maximum(df['ä½¿ç”¨é ­æ•°'], df['æ­£ç•ª'])
    
    def calc(row):
        t = int(row['ä½¿ç”¨é ­æ•°'])
        s = int(row['æ­£ç•ª'])
        g = (t + 1) - s
        sj = t + s
        gj = t + g
        return pd.Series([g, sj, gj])
    
    df[['è¨ˆç®—_é€†ç•ª', 'è¨ˆç®—_æ­£å¾ªç’°', 'è¨ˆç®—_é€†å¾ªç’°']] = df.apply(calc, axis=1)
    return df

def get_pair_pattern(row1, row2):
    def val(x):
        try: return int(float(x)) 
        except: return None
    r1 = [val(row1['æ­£ç•ª']), val(row1['è¨ˆç®—_é€†ç•ª']), val(row1['è¨ˆç®—_æ­£å¾ªç’°']), val(row1['è¨ˆç®—_é€†å¾ªç’°'])]
    r2 = [val(row2['æ­£ç•ª']), val(row2['è¨ˆç®—_é€†ç•ª']), val(row2['è¨ˆç®—_æ­£å¾ªç’°']), val(row2['è¨ˆç®—_é€†å¾ªç’°'])]
    label = list("ABCDEFGHIJKLMNOP")
    pairs = [label[i * 4 + j] for i in range(4) for j in range(4)
             if r1[i] is not None and r2[j] is not None and r1[i] == r2[j] and r1[i] != 0]
    return ",".join(pairs)

def analyze_logic(df, df_prev=None):
    # ã“ã“ã«ã“ã‚Œã¾ã§ã® find_all_pairs, get_blue_recommendations ãªã©ã®ãƒ­ã‚¸ãƒƒã‚¯ã‚’é›†ç´„
    # (ã‚³ãƒ¼ãƒ‰ãŒé•·ããªã‚‹ãŸã‚è¦ç´„ã—ã¾ã™ãŒã€æ—¢å­˜ã®ãƒ­ã‚¸ãƒƒã‚¯ã‚’ãã®ã¾ã¾ã‚³ãƒ”ãƒ¼ã—ã¦ä½¿ãˆã¾ã™)
    
    # ç°¡æ˜“ç‰ˆã®å®Ÿè£…ï¼ˆå®Ÿéš›ã¯æ—¢å­˜ã®é•·ã„é–¢æ•°ç¾¤ã‚’ã“ã“ã«è²¼ã‚Šã¾ã™ï¼‰
    df = calc_haichi_numbers(df)
    
    recommendations = []
    
    # ä¾‹: é¨æ‰‹ãƒšã‚¢ã®ç°¡æ˜“æ¢ç´¢
    df = df.sort_values(['å ´å', 'R'])
    for name, group in df.groupby('é¨æ‰‹'):
        if len(group) < 2: continue
        group = group.sort_values('R').to_dict('records')
        for i in range(len(group)-1):
            curr, next_r = group[i], group[i+1]
            pat = get_pair_pattern(curr, next_r)
            if pat:
                recommendations.append({
                    'å ´å': curr['å ´å'], 'R': curr['R'], 'é¦¬å': curr['é¦¬å'], 'æ­£ç•ª': curr['æ­£ç•ª'],
                    'é¨æ‰‹': name, 'ã‚¿ã‚¤ãƒ—': 'é¨æ‰‹ãƒšã‚¢', 'ãƒ‘ã‚¿ãƒ¼ãƒ³': pat, 'ç›¸æ‰‹R': next_r['R'], 'ã‚¹ã‚³ã‚¢': 3.3
                })
                recommendations.append({
                    'å ´å': next_r['å ´å'], 'R': next_r['R'], 'é¦¬å': next_r['é¦¬å'], 'æ­£ç•ª': next_r['æ­£ç•ª'],
                    'é¨æ‰‹': name, 'ã‚¿ã‚¤ãƒ—': 'é¨æ‰‹ãƒšã‚¢', 'ãƒ‘ã‚¿ãƒ¼ãƒ³': pat, 'ç›¸æ‰‹R': curr['R'], 'ã‚¹ã‚³ã‚¢': 3.3
                })

    if not recommendations:
        return pd.DataFrame()
        
    res_df = pd.DataFrame(recommendations)
    # é‡è¤‡å‰Šé™¤ã‚„ãƒãƒ¼ã‚¸
    res_df = res_df.drop_duplicates(subset=['å ´å', 'R', 'é¦¬å'])
    
    # å¿…é ˆåˆ—ã®æ•´å‚™
    if 'ç€é †' not in res_df.columns: res_df['ç€é †'] = np.nan
    
    return res_df

# ==========================================
# 2. Webã‚¢ãƒ—ãƒªç”»é¢ (Streamlit)
# ==========================================

st.title("ğŸ‡ é…ç½®é¦¬åˆ¸è¡“ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ åˆ†æ")

# ã‚µã‚¤ãƒ‰ãƒãƒ¼: ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
with st.sidebar:
    st.header("ãƒ‡ãƒ¼ã‚¿å…¥åŠ›")
    uploaded_file = st.file_uploader("å½“æ—¥ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type=['xlsx', 'csv'])
    prev_file = st.file_uploader("å‰æ—¥ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ (ä»»æ„)", type=['xlsx', 'csv'])

if uploaded_file:
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    df_raw, status = load_data(uploaded_file)
    
    if status == "success":
        # åˆå›åˆ†æ (ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½¿ã£ã¦é«˜é€ŸåŒ–ã‚‚å¯èƒ½)
        if 'analyzed_df' not in st.session_state:
            with st.spinner('åˆ†æä¸­...'):
                # â˜…ã“ã“ã§æœ¬æ¥ã¯å…¨ãƒ­ã‚¸ãƒƒã‚¯ã‚’å®Ÿè¡Œ
                # ã“ã“ã§ã¯ãƒ‡ãƒ¢ç”¨ã«ç°¡æ˜“ãƒ­ã‚¸ãƒƒã‚¯ã‚’å‘¼ã¶
                result_df = analyze_logic(df_raw)
                
                # è¡¨ç¤ºç”¨ã«æ•´ç†
                if not result_df.empty:
                    disp_cols = ['å ´å', 'R', 'æ­£ç•ª', 'é¦¬å', 'é¨æ‰‹', 'ã‚¿ã‚¤ãƒ—', 'ãƒ‘ã‚¿ãƒ¼ãƒ³', 'ã‚¹ã‚³ã‚¢', 'ç€é †']
                    # åˆ—ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèªã—ã¦ãƒ•ã‚£ãƒ«ã‚¿
                    cols = [c for c in disp_cols if c in result_df.columns]
                    st.session_state['analyzed_df'] = result_df[cols].copy()
                else:
                    st.session_state['analyzed_df'] = pd.DataFrame()

        # ==========================================
        # 3. ç·¨é›†å¯èƒ½ãªãƒ†ãƒ¼ãƒ–ãƒ« (ã“ã‚ŒãŒé‡è¦ï¼)
        # ==========================================
        st.subheader("ğŸ“ çµæœå…¥åŠ›ãƒ»åˆ†æ")
        st.info("ä¸‹ã®è¡¨ã®ã€Œç€é †ã€åˆ—ã‚’ãƒ€ãƒ–ãƒ«ã‚¯ãƒªãƒƒã‚¯ã—ã¦å…¥åŠ›ã™ã‚‹ã¨ã€ã‚°ãƒ©ãƒ•ã¨ã‚¹ã‚³ã‚¢ãŒå³åº§ã«æ›´æ–°ã•ã‚Œã¾ã™ã€‚")

        if not st.session_state['analyzed_df'].empty:
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒç·¨é›†ã§ãã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
            edited_df = st.data_editor(
                st.session_state['analyzed_df'],
                column_config={
                    "ç€é †": st.column_config.NumberColumn(
                        "ç€é †",
                        help="1ã€œ18ã®æ•°å€¤ã‚’å…¥åŠ›",
                        min_value=1,
                        max_value=18,
                        step=1,
                        format="%d"
                    )
                },
                disabled=["å ´å", "R", "é¦¬å", "é¨æ‰‹", "ã‚¿ã‚¤ãƒ—", "ãƒ‘ã‚¿ãƒ¼ãƒ³"], # ç€é †ä»¥å¤–ã¯ç·¨é›†ä¸å¯
                hide_index=True,
                use_container_width=True,
                height=500
            )

            # ==========================================
            # 4. ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é›†è¨ˆ & ã‚°ãƒ©ãƒ•
            # ==========================================
            
            # ç€é †ãŒå…¥åŠ›ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã ã‘æŠ½å‡º
            df_hits = edited_df[edited_df['ç€é †'].notna()].copy()
            df_hits['ç€é †'] = pd.to_numeric(df_hits['ç€é †'], errors='coerce')
            
            # çš„ä¸­ï¼ˆ3ç€å†…ï¼‰ãƒ‡ãƒ¼ã‚¿
            df_fuku = df_hits[df_hits['ç€é †'] <= 3]

            st.divider()
            
            # --- ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æŒ‡æ¨™ ---
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("æ¶ˆåŒ–ãƒ¬ãƒ¼ã‚¹æ•°", len(df_hits['R'].unique()))
            with col2:
                # è¤‡å‹ç‡
                fuku_rate = len(df_fuku) / len(df_hits) * 100 if len(df_hits) > 0 else 0
                st.metric("æ¨å¥¨é¦¬ è¤‡å‹ç‡", f"{fuku_rate:.1f}%")
            with col3:
                # çš„ä¸­ãƒ‘ã‚¿ãƒ¼ãƒ³æ•°
                st.metric("çš„ä¸­æ•°", len(df_fuku))

            # --- é–‹å‚¬å ´ãƒ»å±æ€§åˆ¥ã®å††ã‚°ãƒ©ãƒ• ---
            st.subheader("ğŸ“Š ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å‚¾å‘ã‚°ãƒ©ãƒ•")
            
            if not df_fuku.empty:
                # é–‹å‚¬å ´ã”ã¨ã«ã‚¿ãƒ–åˆ†ã‘
                places = df_fuku['å ´å'].unique()
                tabs = st.tabs(list(places))
                
                for tab, place in zip(tabs, places):
                    with tab:
                        place_data = df_fuku[df_fuku['å ´å'] == place]
                        
                        # ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥é›†è¨ˆ
                        pat_counts = place_data['ãƒ‘ã‚¿ãƒ¼ãƒ³'].value_counts().reset_index()
                        pat_counts.columns = ['ãƒ‘ã‚¿ãƒ¼ãƒ³', 'çš„ä¸­æ•°']
                        
                        # å††ã‚°ãƒ©ãƒ• (Plotly)
                        fig = px.pie(pat_counts, values='çš„ä¸­æ•°', names='ãƒ‘ã‚¿ãƒ¼ãƒ³', 
                                     title=f'{place} ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥çš„ä¸­ã‚·ã‚§ã‚¢',
                                     hole=0.4)
                        st.plotly_chart(fig, use_container_width=True)
                        
                        # ãƒ‡ãƒ¼ã‚¿ãƒ†ãƒ¼ãƒ–ãƒ«è¡¨ç¤º
                        st.dataframe(place_data[['R', 'é¦¬å', 'é¨æ‰‹', 'ãƒ‘ã‚¿ãƒ¼ãƒ³', 'ç€é †']], use_container_width=True)
            else:
                st.warning("ã¾ã 3ç€ä»¥å†…ã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ç€é †ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")

            # --- å‚¾å‘ã‚’åŠ å‘³ã—ãŸã‚¹ã‚³ã‚¢å†è¨ˆç®— ---
            # (ã“ã“ã§ã¯ç°¡æ˜“çš„ã«ã€å½“ãŸã£ã¦ã„ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã®é¦¬ã« +1.0 ã™ã‚‹ãƒ­ã‚¸ãƒƒã‚¯ä¾‹)
            if not df_fuku.empty:
                hit_patterns = df_fuku['ãƒ‘ã‚¿ãƒ¼ãƒ³'].unique()
                
                # ã¾ã ç€é †ãŒå…¥ã£ã¦ã„ãªã„é¦¬ï¼ˆã“ã‚Œã‹ã‚‰èµ°ã‚‹é¦¬ï¼‰
                future_races = edited_df[edited_df['ç€é †'].isna()].copy()
                
                if not future_races.empty:
                    # å½“ãŸã‚Šãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æŒã¤é¦¬ã®ã‚¹ã‚³ã‚¢ã‚’ã‚¢ãƒƒãƒ—
                    future_races['å‚¾å‘åŠ ç‚¹'] = future_races['ãƒ‘ã‚¿ãƒ¼ãƒ³'].apply(lambda x: 2.0 if x in hit_patterns else 0.0)
                    future_races['äºˆæƒ³ã‚¹ã‚³ã‚¢'] = future_races['ã‚¹ã‚³ã‚¢'] + future_races['å‚¾å‘åŠ ç‚¹']
                    
                    st.subheader("ğŸ“ˆ å‚¾å‘ã‚’åŠ å‘³ã—ãŸæ¨å¥¨é¦¬ï¼ˆã“ã‚Œã‹ã‚‰èµ°ã‚‹ãƒ¬ãƒ¼ã‚¹ï¼‰")
                    st.dataframe(
                        future_races.sort_values('äºˆæƒ³ã‚¹ã‚³ã‚¢', ascending=False)[['å ´å', 'R', 'é¦¬å', 'ãƒ‘ã‚¿ãƒ¼ãƒ³', 'ã‚¹ã‚³ã‚¢', 'äºˆæƒ³ã‚¹ã‚³ã‚¢']],
                        use_container_width=True
                    )

        else:
            st.warning("æ¨å¥¨é¦¬ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")