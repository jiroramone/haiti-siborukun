import streamlit as st
import pandas as pd
import numpy as np
import re
import plotly.express as px

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(page_title="é…ç½®é¦¬åˆ¸è¡“ Web", layout="wide")

# ==========================================
# 1. å…±é€šãƒ­ã‚¸ãƒƒã‚¯
# ==========================================

def to_half_width(text):
    if pd.isna(text): return text
    text = str(text)
    table = str.maketrans('ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™', '0123456789')
    text = text.translate(table)
    text = re.sub(r'[^\d\.]', '', text)
    return text

def normalize_name(x):
    if pd.isna(x): return ''
    normalized_name = str(x).strip().replace('ã€€', '').replace(' ', '')
    normalized_name = re.sub(r'[â˜…â˜†â–²â–³â—‡]', '', normalized_name)
    if ',' in normalized_name: normalized_name = normalized_name.split(',')[0]
    text = re.sub(r'[0-9\.]+[Rr]', '', normalized_name)
    text = re.sub(r'\(.*?\)', '', text)
    return text.replace('/', '').strip()

@st.cache_data
def load_data(file):
    try:
        if file.name.endswith('.csv'):
            df = pd.read_csv(file, encoding='cp932', on_bad_lines='skip')
        else:
            df = pd.read_excel(file, engine='openpyxl')
    except:
        return pd.DataFrame(), "ã‚¨ãƒ©ãƒ¼"

    df.columns = df.columns.str.strip()
    
    rename_map = {
        'å ´æ‰€': 'å ´å', 'é–‹å‚¬': 'å ´å', 'å˜ã‚ªãƒƒã‚º': 'å˜ï½µï½¯ï½½ï¾', 
        'èª¿æ•™å¸«': 'å©èˆ', 'èª¿æ•™å¸«å': 'å©èˆ', 'é¨æ‰‹å': 'é¨æ‰‹',
        'ãƒ¬ãƒ¼ã‚¹': 'R', 'ï¼²': 'R', 'ãƒ¬ãƒ¼ã‚¹å': 'R',
        'ç€': 'ç€é †', 'ç€ é †': 'ç€é †', 'ç•ª': 'æ­£ç•ª', 'é¦¬ç•ª': 'æ­£ç•ª'
    }
    df = df.rename(columns=rename_map)
    if 'å ´å' not in df.columns: df['å ´å'] = 'Unknown'

    target_numeric_cols = ['R', 'æ­£ç•ª', 'å˜ï½µï½¯ï½½ï¾', 'é€†ç•ª', 'æ­£å¾ªç’°', 'é€†å¾ªç’°', 'é ­æ•°']
    for col in target_numeric_cols:
        if col in df.columns:
            df[col] = df[col].apply(to_half_width)
            df[col] = pd.to_numeric(df[col], errors='coerce')

    df = df.dropna(subset=['R', 'æ­£ç•ª'])
    df['R'] = df['R'].astype(int)
    df['æ­£ç•ª'] = df['æ­£ç•ª'].astype(int)

    for col in ['é¨æ‰‹', 'å©èˆ', 'é¦¬ä¸»']:
        if col in df.columns:
            df[col] = df[col].apply(normalize_name)
        else:
            df[col] = ''
            
    potential_cols = ['R', 'å ´å', 'é¦¬å', 'æ­£ç•ª', 'é¨æ‰‹', 'å©èˆ', 'é¦¬ä¸»', 'å˜ï½µï½¯ï½½ï¾', 'é€†ç•ª', 'æ­£å¾ªç’°', 'é€†å¾ªç’°', 'é ­æ•°']
    for col in potential_cols:
        if col not in df.columns: df[col] = np.nan

    return df[potential_cols].copy(), "success"

def calc_haichi_numbers(df):
    if df[['é€†ç•ª', 'æ­£å¾ªç’°', 'é€†å¾ªç’°']].notna().all().all():
        df['è¨ˆç®—_é€†ç•ª'] = df['é€†ç•ª']
        df['è¨ˆç®—_æ­£å¾ªç’°'] = df['æ­£å¾ªç’°']
        df['è¨ˆç®—_é€†å¾ªç’°'] = df['é€†å¾ªç’°']
        return df
    
    # é ­æ•°è‡ªå‹•è¨ˆç®— (åœ°æ–¹ç«¶é¦¬å¯¾å¿œ)
    max_umaban = df.groupby(['å ´å', 'R'])['æ­£ç•ª'].transform('max')
    df['ä½¿ç”¨é ­æ•°'] = max_umaban.fillna(16).astype(int)
    if 'é ­æ•°' in df.columns:
        df['ä½¿ç”¨é ­æ•°'] = df['é ­æ•°'].fillna(df['ä½¿ç”¨é ­æ•°']).astype(int)
    
    df['ä½¿ç”¨é ­æ•°'] = np.maximum(df['ä½¿ç”¨é ­æ•°'], df['æ­£ç•ª'])
    
    def calc(row):
        t = int(row['ä½¿ç”¨é ­æ•°'])
        s = int(row['æ­£ç•ª'])
        g = (t + 1) - s
        sj = t + s
        gj = t + g
        return pd.Series([g, sj, gj])
    
    df[['è¨ˆç®—_é€†ç•ª', 'è¨ˆç®—_æ­£å¾ªç’°', 'è¨ˆç®—_é€†å¾ªç’°']] = df.apply(calc, axis=1)
    return df

def get_pair_pattern(row1, row2):
    def val(x):
        try: return int(float(x)) 
        except: return None
    r1 = [val(row1['æ­£ç•ª']), val(row1['è¨ˆç®—_é€†ç•ª']), val(row1['è¨ˆç®—_æ­£å¾ªç’°']), val(row1['è¨ˆç®—_é€†å¾ªç’°'])]
    r2 = [val(row2['æ­£ç•ª']), val(row2['è¨ˆç®—_é€†ç•ª']), val(row2['è¨ˆç®—_æ­£å¾ªç’°']), val(row2['è¨ˆç®—_é€†å¾ªç’°'])]
    label = list("ABCDEFGHIJKLMNOP")
    pairs = [label[i * 4 + j] for i in range(4) for j in range(4)
             if r1[i] is not None and r2[j] is not None and r1[i] == r2[j] and r1[i] != 0]
    return ",".join(pairs)

# ==========================================
# 2. åˆ†æãƒ­ã‚¸ãƒƒã‚¯ (åˆ¤å®šå‡¦ç†)
# ==========================================
def analyze_logic(df):
    df = calc_haichi_numbers(df)
    recommendations = []
    
    # ãƒ‡ãƒ¼ã‚¿ã‚’ã‚½ãƒ¼ãƒˆã—ã¦ãŠã
    df = df.sort_values(['å ´å', 'R', 'æ­£ç•ª'])
    
    # --- 1. é¨æ‰‹ãƒšã‚¢ ---
    for name, group in df.groupby('é¨æ‰‹'):
        if len(group) < 2: continue
        group = group.sort_values('R').to_dict('records')
        for i in range(len(group)-1):
            curr, next_r = group[i], group[i+1]
            if curr['å ´å'] != next_r['å ´å']: continue # åŒå ´ã®ã¿
            pat = get_pair_pattern(curr, next_r)
            if pat:
                # ç›¸äº’ã«ç™»éŒ²
                recommendations.append({
                    'å ´å': curr['å ´å'], 'R': curr['R'], 'æ­£ç•ª': curr['æ­£ç•ª'], 'é¦¬å': curr['é¦¬å'],
                    'é¨æ‰‹': name, 'ã‚¿ã‚¤ãƒ—': 'é¨æ‰‹ãƒšã‚¢', 'ãƒ‘ã‚¿ãƒ¼ãƒ³': pat, 'ç›¸æ‰‹R': next_r['R'], 'ã‚¹ã‚³ã‚¢': 3.3
                })
                recommendations.append({
                    'å ´å': next_r['å ´å'], 'R': next_r['R'], 'æ­£ç•ª': next_r['æ­£ç•ª'], 'é¦¬å': next_r['é¦¬å'],
                    'é¨æ‰‹': name, 'ã‚¿ã‚¤ãƒ—': 'é¨æ‰‹ãƒšã‚¢', 'ãƒ‘ã‚¿ãƒ¼ãƒ³': pat, 'ç›¸æ‰‹R': curr['R'], 'ã‚¹ã‚³ã‚¢': 3.3
                })

    # --- 2. å©èˆãƒšã‚¢ ---
    if 'å©èˆ' in df.columns:
        for (place, name), group in df.groupby(['å ´å', 'å©èˆ']):
            if len(group) < 2: continue
            group = group.sort_values('R').to_dict('records')
            for i in range(len(group)):
                for j in range(i+1, len(group)):
                    curr, next_r = group[i], group[j]
                    pat = get_pair_pattern(curr, next_r)
                    if pat:
                        recommendations.append({
                            'å ´å': curr['å ´å'], 'R': curr['R'], 'æ­£ç•ª': curr['æ­£ç•ª'], 'é¦¬å': curr['é¦¬å'],
                            'é¨æ‰‹': f"(å©èˆ:{name})", 'ã‚¿ã‚¤ãƒ—': 'å©èˆãƒšã‚¢', 'ãƒ‘ã‚¿ãƒ¼ãƒ³': pat, 'ç›¸æ‰‹R': next_r['R'], 'ã‚¹ã‚³ã‚¢': 3.2
                        })
                        recommendations.append({
                            'å ´å': next_r['å ´å'], 'R': next_r['R'], 'æ­£ç•ª': next_r['æ­£ç•ª'], 'é¦¬å': next_r['é¦¬å'],
                            'é¨æ‰‹': f"(å©èˆ:{name})", 'ã‚¿ã‚¤ãƒ—': 'å©èˆãƒšã‚¢', 'ãƒ‘ã‚¿ãƒ¼ãƒ³': pat, 'ç›¸æ‰‹R': curr['R'], 'ã‚¹ã‚³ã‚¢': 3.2
                        })

    if not recommendations:
        return pd.DataFrame()
        
    res_df = pd.DataFrame(recommendations)
    
    # é‡è¤‡å‰Šé™¤ (åŒã˜é¦¬ãŒè¤‡æ•°ã®ç†ç”±ã§é¸ã°ã‚ŒãŸå ´åˆã€ã‚¹ã‚³ã‚¢ã‚’åŠ ç®—ã—ã¦çµ±åˆ)
    # ã¾ãšé¦¬ã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
    agg_funcs = {
        'é¨æ‰‹': 'first',
        'ã‚¿ã‚¤ãƒ—': lambda x: '/'.join(sorted(set(x))),
        'ãƒ‘ã‚¿ãƒ¼ãƒ³': lambda x: ','.join(sorted(set(x))),
        'ã‚¹ã‚³ã‚¢': 'sum',
        'æ­£ç•ª': 'first' # ã‚½ãƒ¼ãƒˆç”¨
    }
    
    # å¿…è¦ãªåˆ—ã ã‘ã§ã‚°ãƒ«ãƒ¼ãƒ”ãƒ³ã‚°
    res_df = res_df.groupby(['å ´å', 'R', 'é¦¬å'], as_index=False).agg(agg_funcs)
    
    # â˜…é‡è¦: ãƒ¬ãƒ¼ã‚¹é †ã«ä¸¦ã¹æ›¿ãˆ (ã“ã“ã‚’è¿½åŠ ï¼)
    res_df = res_df.sort_values(['å ´å', 'R', 'æ­£ç•ª'], ascending=[True, True, True])
    
    # ç€é †åˆ—ã®åˆæœŸåŒ–
    res_df['ç€é †'] = np.nan
    
    return res_df

# ==========================================
# 3. Webã‚¢ãƒ—ãƒªç”»é¢ (Streamlit)
# ==========================================

st.title("ğŸ‡ é…ç½®é¦¬åˆ¸è¡“ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ åˆ†æ")
st.caption("ç€é †ã‚’å…¥åŠ›ã™ã‚‹ã¨ã€çµ±è¨ˆã¨ã‚¹ã‚³ã‚¢ãŒå³åº§ã«æ›´æ–°ã•ã‚Œã¾ã™ã€‚")

# ã‚µã‚¤ãƒ‰ãƒãƒ¼
with st.sidebar:
    st.header("ãƒ‡ãƒ¼ã‚¿å…¥åŠ›")
    uploaded_file = st.file_uploader("å½“æ—¥ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type=['xlsx', 'csv'])
    st.markdown("---")
    st.write("â€»Excelãƒ•ã‚¡ã‚¤ãƒ«(.xlsx)ã¾ãŸã¯CSVãƒ•ã‚¡ã‚¤ãƒ«(.csv)ã«å¯¾å¿œã—ã¦ã„ã¾ã™ã€‚")

if uploaded_file:
    df_raw, status = load_data(uploaded_file)
    
    if status == "success":
        # åˆå›åˆ†æ (session_stateã§ä¿æŒ)
        if 'analyzed_df' not in st.session_state:
            with st.spinner('åˆ†æä¸­...'):
                result_df = analyze_logic(df_raw)
                if not result_df.empty:
                    # ç·¨é›†ç”¨IDã‚’ä½œæˆ (Streamlitã®ä»•æ§˜å¯¾ç­–)
                    result_df['id'] = result_df.index
                    st.session_state['analyzed_df'] = result_df
                else:
                    st.session_state['analyzed_df'] = pd.DataFrame()

        # --- ãƒ¡ã‚¤ãƒ³ã‚¨ãƒªã‚¢ ---
        if not st.session_state['analyzed_df'].empty:
            
            # --- 1. ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ‡ã‚£ã‚¿ (ç€é †å…¥åŠ›) ---
            st.subheader("ğŸ“ çµæœå…¥åŠ›ãƒ»æ¨å¥¨é¦¬ãƒªã‚¹ãƒˆ")
            
            # è¡¨ç¤ºã™ã‚‹åˆ—ã‚’æ•´ç†
            display_df = st.session_state['analyzed_df'].copy()
            
            edited_df = st.data_editor(
                display_df,
                column_config={
                    "ç€é †": st.column_config.NumberColumn(
                        "ç€é † (å…¥åŠ›)",
                        help="ç¢ºå®šã—ãŸç€é †ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ (1ã€œ18)",
                        min_value=1,
                        max_value=18,
                        step=1,
                        format="%d"
                    ),
                    "ã‚¹ã‚³ã‚¢": st.column_config.ProgressColumn(
                        "é‡è¦åº¦",
                        format="%.1f",
                        min_value=0,
                        max_value=15,
                    ),
                },
                disabled=["å ´å", "R", "é¦¬å", "æ­£ç•ª", "é¨æ‰‹", "ã‚¿ã‚¤ãƒ—", "ãƒ‘ã‚¿ãƒ¼ãƒ³", "ã‚¹ã‚³ã‚¢"],
                hide_index=True,
                use_container_width=True,
                height=500,
                key="editor" # ã‚­ãƒ¼ã‚’æŒ‡å®šã—ã¦çŠ¶æ…‹ã‚’ç¶­æŒ
            )
            
            # å…¥åŠ›ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã«åæ˜ 
            # (data_editorã¯è‡ªå‹•ã§stateã‚’æ›´æ–°ã—ãªã„å ´åˆãŒã‚ã‚‹ãŸã‚å¿µã®ãŸã‚)
            if edited_df is not None:
                st.session_state['analyzed_df'] = edited_df

            # ==========================================
            # 4. ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é›†è¨ˆ & ã‚°ãƒ©ãƒ•
            # ==========================================
            
            df_hits = edited_df[edited_df['ç€é †'].notna()].copy()
            df_hits['ç€é †'] = pd.to_numeric(df_hits['ç€é †'], errors='coerce')
            df_fuku = df_hits[df_hits['ç€é †'] <= 3]

            st.markdown("---")
            st.subheader("ğŸ“Š ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ åˆ†æãƒ¬ãƒãƒ¼ãƒˆ")

            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("æ¶ˆåŒ–ãƒ¬ãƒ¼ã‚¹æ•°", len(df_hits['R'].unique()))
            with col2:
                # è¤‡å‹ç‡
                fuku_rate = len(df_fuku) / len(df_hits) * 100 if len(df_hits) > 0 else 0
                st.metric("æ¨å¥¨é¦¬ è¤‡å‹ç‡", f"{fuku_rate:.1f}%")
            with col3:
                # å˜å‹å›åç‡ (ã‚ªãƒƒã‚ºãƒ‡ãƒ¼ã‚¿ãŒãªã„ã®ã§ä»®)
                # å®Ÿè£…ã™ã‚‹ãªã‚‰load_dataã§å˜ã‚ªãƒƒã‚ºã‚’èª­ã¿è¾¼ã¿è¨ˆç®—ã™ã‚‹
                st.metric("çš„ä¸­æ•°", f"{len(df_fuku)} é ­")

            if not df_fuku.empty:
                # é–‹å‚¬å ´ã”ã¨ã«ã‚¿ãƒ–åˆ†ã‘
                places = sorted(df_fuku['å ´å'].unique())
                tabs = st.tabs(places)
                
                for tab, place in zip(tabs, places):
                    with tab:
                        col_g1, col_g2 = st.columns(2)
                        
                        place_data = df_fuku[df_fuku['å ´å'] == place]
                        
                        # ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥é›†è¨ˆ (ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã‚’å±•é–‹)
                        all_patterns = []
                        for p in place_data['ãƒ‘ã‚¿ãƒ¼ãƒ³']:
                            if p: all_patterns.extend(p.split(','))
                        
                        if all_patterns:
                            pat_counts = pd.Series(all_patterns).value_counts().reset_index()
                            pat_counts.columns = ['ãƒ‘ã‚¿ãƒ¼ãƒ³', 'çš„ä¸­æ•°']
                            
                            with col_g1:
                                fig = px.pie(pat_counts, values='çš„ä¸­æ•°', names='ãƒ‘ã‚¿ãƒ¼ãƒ³', 
                                            title=f'ã€{place}ã€‘ ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥ çš„ä¸­ã‚·ã‚§ã‚¢',
                                            hole=0.4)
                                st.plotly_chart(fig, use_container_width=True)
                            
                            with col_g2:
                                st.write(f"**{place} ã®çš„ä¸­é¦¬ä¸€è¦§**")
                                st.dataframe(place_data[['R', 'é¦¬å', 'é¨æ‰‹', 'ãƒ‘ã‚¿ãƒ¼ãƒ³', 'ç€é †']], use_container_width=True, hide_index=True)
                        else:
                            st.info("ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")

                # --- å‚¾å‘ã‚¹ã‚³ã‚¢åŠ ç®— ---
                st.markdown("### ğŸ“ˆ æ¬¡ã®ãƒ¬ãƒ¼ã‚¹ã®æ³¨ç›®é¦¬ (å‚¾å‘åŠ å‘³)")
                hit_patterns = set()
                for p in df_fuku['ãƒ‘ã‚¿ãƒ¼ãƒ³']:
                    if p: hit_patterns.update(p.split(','))
                
                # ã¾ã ç€é †ãŒå…¥ã£ã¦ã„ãªã„é¦¬
                future_races = edited_df[edited_df['ç€é †'].isna()].copy()
                
                if not future_races.empty:
                    # å½“ãŸã‚Šãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æŒã£ã¦ã„ã‚‹ã‹åˆ¤å®š
                    def calc_trend_bonus(row_pat):
                        if not row_pat: return 0
                        pats = row_pat.split(',')
                        bonus = 0
                        for p in pats:
                            if p in hit_patterns: bonus += 1.0 # ãƒ’ãƒƒãƒˆã—ãŸãƒ‘ã‚¿ãƒ¼ãƒ³1ã¤ã«ã¤ã+1ç‚¹
                        return bonus

                    future_races['å‚¾å‘åŠ ç‚¹'] = future_races['ãƒ‘ã‚¿ãƒ¼ãƒ³'].apply(calc_trend_bonus)
                    future_races['ç·åˆã‚¹ã‚³ã‚¢'] = future_races['ã‚¹ã‚³ã‚¢'] + future_races['å‚¾å‘åŠ ç‚¹']
                    
                    # å‚¾å‘åŠ ç‚¹ãŒã‚ã‚‹é¦¬ã®ã¿ã€ã¾ãŸã¯ã‚¹ã‚³ã‚¢ä¸Šä½ã‚’è¡¨ç¤º
                    hot_horses = future_races[future_races['å‚¾å‘åŠ ç‚¹'] > 0].sort_values(['å ´å', 'R', 'ç·åˆã‚¹ã‚³ã‚¢'], ascending=[True, True, False])
                    
                    if not hot_horses.empty:
                        st.success("æœ¬æ—¥ã®å½“ãŸã‚Šãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æŒã¤é¦¬ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸï¼")
                        st.dataframe(
                            hot_horses[['å ´å', 'R', 'é¦¬å', 'é¨æ‰‹', 'ãƒ‘ã‚¿ãƒ¼ãƒ³', 'ã‚¹ã‚³ã‚¢', 'å‚¾å‘åŠ ç‚¹', 'ç·åˆã‚¹ã‚³ã‚¢']],
                            use_container_width=True,
                            hide_index=True
                        )
                    else:
                        st.info("ç¾åœ¨ã€å‚¾å‘ã¨åˆè‡´ã™ã‚‹æœªå‡ºèµ°é¦¬ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
            else:
                st.info("çš„ä¸­ãƒ‡ãƒ¼ã‚¿ãŒã¾ã ã‚ã‚Šã¾ã›ã‚“ã€‚ãƒ¬ãƒ¼ã‚¹çµæœã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")

        else:
            st.warning("æ¨å¥¨å¯¾è±¡ã¨ãªã‚‹é¦¬ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
    else:
        st.error("ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚å½¢å¼ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
