import streamlit as st
import pandas as pd
import numpy as np
import re
import plotly.express as px
import openpyxl

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(page_title="é…ç½®é¦¬åˆ¸è¡“ Web", layout="wide")

# ==========================================
# 1. å…±é€šãƒ­ã‚¸ãƒƒã‚¯
# ==========================================

def to_half_width(text):
    if pd.isna(text): return text
    text = str(text)
    table = str.maketrans('ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™', '0123456789')
    text = text.translate(table)
    text = re.sub(r'[^\d\.]', '', text)
    return text

def normalize_name(x):
    if pd.isna(x): return ''
    normalized_name = str(x).strip().replace('ã€€', '').replace(' ', '')
    normalized_name = re.sub(r'[â˜…â˜†â–²â–³â—‡]', '', normalized_name)
    if ',' in normalized_name: normalized_name = normalized_name.split(',')[0]
    text = re.sub(r'[0-9\.]+[Rr]', '', normalized_name)
    text = re.sub(r'\(.*?\)', '', text)
    return text.replace('/', '').strip()

@st.cache_data
def load_data(file):
    """
    ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã‚’è¡Œã†é–¢æ•°
    Windows(Shift-JIS)ã¨Mac(UTF-8)ã®ä¸¡æ–¹ã«å¯¾å¿œ
    """
    df = None
    
    # 1. Excelãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç†
    if file.name.endswith('.xlsx'):
        try:
            file.seek(0)
            df = pd.read_excel(file, engine='openpyxl')
        except Exception as e:
            return pd.DataFrame(), f"Excelèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}"
            
    # 2. CSVãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç†ï¼ˆå …ç‰¢ãªãƒ­ã‚¸ãƒƒã‚¯ï¼‰
    else:
        # ãƒ‘ã‚¿ãƒ¼ãƒ³A: UTF-8 ã§è©¦è¡Œ
        try:
            file.seek(0)
            df = pd.read_csv(file, encoding='utf-8', on_bad_lines='skip')
        except UnicodeDecodeError:
            # ãƒ‘ã‚¿ãƒ¼ãƒ³B: å¤±æ•—ã—ãŸã‚‰ CP932 (Shift-JIS) ã§å†è©¦è¡Œ
            try:
                file.seek(0) # å¿…é ˆ: ãƒ•ã‚¡ã‚¤ãƒ«ãƒã‚¤ãƒ³ã‚¿ã‚’å…ˆé ­ã«æˆ»ã™
                df = pd.read_csv(file, encoding='cp932', on_bad_lines='skip')
            except Exception as e:
                return pd.DataFrame(), f"CSVèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼(æ–‡å­—ã‚³ãƒ¼ãƒ‰åˆ¤å®šä¸èƒ½): {e}"
        except Exception as e:
            return pd.DataFrame(), f"CSVäºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼: {e}"

    # ãƒ‡ãƒ¼ã‚¿æ•´å½¢ãƒ­ã‚¸ãƒƒã‚¯
    df.columns = df.columns.str.strip()
    
    rename_map = {
        'å ´æ‰€': 'å ´å', 'é–‹å‚¬': 'å ´å', 'å˜ã‚ªãƒƒã‚º': 'å˜ï½µï½¯ï½½ï¾', 
        'èª¿æ•™å¸«': 'å©èˆ', 'èª¿æ•™å¸«å': 'å©èˆ', 'é¨æ‰‹å': 'é¨æ‰‹',
        'ãƒ¬ãƒ¼ã‚¹': 'R', 'ï¼²': 'R', 'ãƒ¬ãƒ¼ã‚¹å': 'R',
        'ç€': 'ç€é †', 'ç€ é †': 'ç€é †', 'ç•ª': 'æ­£ç•ª', 'é¦¬ç•ª': 'æ­£ç•ª'
    }
    df = df.rename(columns=rename_map)
    if 'å ´å' not in df.columns: df['å ´å'] = 'Unknown'

    target_numeric_cols = ['R', 'æ­£ç•ª', 'å˜ï½µï½¯ï½½ï¾', 'é€†ç•ª', 'æ­£å¾ªç’°', 'é€†å¾ªç’°', 'é ­æ•°']
    for col in target_numeric_cols:
        if col in df.columns:
            df[col] = df[col].apply(to_half_width)
            df[col] = pd.to_numeric(df[col], errors='coerce')

    # å¿…é ˆåˆ—ï¼ˆR, æ­£ç•ªï¼‰ãŒãªã„å ´åˆã¯ã‚¨ãƒ©ãƒ¼
    if 'R' not in df.columns or 'æ­£ç•ª' not in df.columns:
        return pd.DataFrame(), "ã‚¨ãƒ©ãƒ¼: å¿…é ˆåˆ—ï¼ˆãƒ¬ãƒ¼ã‚¹å/Rã€é¦¬ç•ª/æ­£ç•ªï¼‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"

    df = df.dropna(subset=['R', 'æ­£ç•ª'])
    df['R'] = df['R'].astype(int)
    df['æ­£ç•ª'] = df['æ­£ç•ª'].astype(int)

    for col in ['é¨æ‰‹', 'å©èˆ', 'é¦¬ä¸»']:
        if col in df.columns:
            df[col] = df[col].apply(normalize_name)
        else:
            df[col] = ''
            
    # åˆ†æç”¨ãƒ‡ãƒ¼ã‚¿ã®å¿…é ˆåˆ—ç¢ºä¿
    potential_cols = ['R', 'å ´å', 'é¦¬å', 'æ­£ç•ª', 'é¨æ‰‹', 'å©èˆ', 'é¦¬ä¸»', 'å˜ï½µï½¯ï½½ï¾', 'é€†ç•ª', 'æ­£å¾ªç’°', 'é€†å¾ªç’°', 'é ­æ•°']
    for col in potential_cols:
        if col not in df.columns: df[col] = np.nan

    return df[potential_cols].copy(), "success"

# --- é…ç½®è¨ˆç®—ãƒ­ã‚¸ãƒƒã‚¯ ---
def calc_haichi_numbers(df):
    if df[['é€†ç•ª', 'æ­£å¾ªç’°', 'é€†å¾ªç’°']].notna().all().all():
        df['è¨ˆç®—_é€†ç•ª'] = df['é€†ç•ª']
        return df
    max_umaban = df.groupby(['å ´å', 'R'])['æ­£ç•ª'].transform('max')
    df['ä½¿ç”¨é ­æ•°'] = max_umaban.fillna(16).astype(int)
    if 'é ­æ•°' in df.columns:
        df['ä½¿ç”¨é ­æ•°'] = df['é ­æ•°'].fillna(df['ä½¿ç”¨é ­æ•°']).astype(int)
    
    df['ä½¿ç”¨é ­æ•°'] = np.maximum(df['ä½¿ç”¨é ­æ•°'], df['æ­£ç•ª'])
    
    def calc(row):
        t = int(row['ä½¿ç”¨é ­æ•°'])
        s = int(row['æ­£ç•ª'])
        g = (t + 1) - s
        sj = t + s
        gj = t + g
        return pd.Series([g, sj, gj])
    
    df[['è¨ˆç®—_é€†ç•ª', 'è¨ˆç®—_æ­£å¾ªç’°', 'è¨ˆç®—_é€†å¾ªç’°']] = df.apply(calc, axis=1)
    return df

def get_pair_pattern(row1, row2):
    def val(x):
        try: return int(float(x)) 
        except: return None
    r1 = [val(row1['æ­£ç•ª']), val(row1['è¨ˆç®—_é€†ç•ª']), val(row1['è¨ˆç®—_æ­£å¾ªç’°']), val(row1['è¨ˆç®—_é€†å¾ªç’°'])]
    r2 = [val(row2['æ­£ç•ª']), val(row2['è¨ˆç®—_é€†ç•ª']), val(row2['è¨ˆç®—_æ­£å¾ªç’°']), val(row2['è¨ˆç®—_é€†å¾ªç’°'])]
    label = list("ABCDEFGHIJKLMNOP")
    pairs = [label[i * 4 + j] for i in range(4) for j in range(4)
             if r1[i] is not None and r2[j] is not None and r1[i] == r2[j] and r1[i] != 0]
    return ",".join(pairs)

def analyze_logic(df):
    # ãƒ­ã‚¸ãƒƒã‚¯å®Ÿè£…éƒ¨åˆ†
    df = calc_haichi_numbers(df)
    recommendations = []
    
    df = df.sort_values(['å ´å', 'R'])
    for name, group in df.groupby('é¨æ‰‹'):
        if len(group) < 2: continue
        group = group.sort_values('R').to_dict('records')
        for i in range(len(group)-1):
            curr, next_r = group[i], group[i+1]
            pat = get_pair_pattern(curr, next_r)
            if pat:
                recommendations.append({
                    'å ´å': curr['å ´å'], 'R': curr['R'], 'é¦¬å': curr['é¦¬å'], 'æ­£ç•ª': curr['æ­£ç•ª'],
                    'é¨æ‰‹': name, 'ã‚¿ã‚¤ãƒ—': 'é¨æ‰‹ãƒšã‚¢', 'ãƒ‘ã‚¿ãƒ¼ãƒ³': pat, 'ç›¸æ‰‹R': next_r['R'], 'ã‚¹ã‚³ã‚¢': 3.3
                })
                recommendations.append({
                    'å ´å': next_r['å ´å'], 'R': next_r['R'], 'é¦¬å': next_r['é¦¬å'], 'æ­£ç•ª': next_r['æ­£ç•ª'],
                    'é¨æ‰‹': name, 'ã‚¿ã‚¤ãƒ—': 'é¨æ‰‹ãƒšã‚¢', 'ãƒ‘ã‚¿ãƒ¼ãƒ³': pat, 'ç›¸æ‰‹R': curr['R'], 'ã‚¹ã‚³ã‚¢': 3.3
                })

    if not recommendations:
        return pd.DataFrame()
        
    res_df = pd.DataFrame(recommendations)
    res_df = res_df.drop_duplicates(subset=['å ´å', 'R', 'é¦¬å'])
    
    if 'ç€é †' not in res_df.columns: res_df['ç€é †'] = np.nan
    
    return res_df

# ==========================================
# 2. Webã‚¢ãƒ—ãƒªç”»é¢ (Streamlit)
# ==========================================

st.title("ğŸ‡ é…ç½®é¦¬åˆ¸è¡“ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ åˆ†æ")

# ã‚µã‚¤ãƒ‰ãƒãƒ¼: ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
with st.sidebar:
    st.header("ãƒ‡ãƒ¼ã‚¿å…¥åŠ›")
    uploaded_file = st.file_uploader("å½“æ—¥ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type=['xlsx', 'csv'])
    prev_file = st.file_uploader("å‰æ—¥ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ (ä»»æ„)", type=['xlsx', 'csv'])

if uploaded_file:
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆå¼·åŒ–ç‰ˆï¼‰
    df_raw, status = load_data(uploaded_file)
    
    if status != "success":
        st.error(status)
    else:
        # åˆå›åˆ†æ (ã‚­ãƒ£ãƒƒã‚·ãƒ¥åˆ¶å¾¡)
        if 'analyzed_df' not in st.session_state:
            with st.spinner('åˆ†æä¸­...'):
                result_df = analyze_logic(df_raw)
                
                if not result_df.empty:
                    disp_cols = ['å ´å', 'R', 'æ­£ç•ª', 'é¦¬å', 'é¨æ‰‹', 'ã‚¿ã‚¤ãƒ—', 'ãƒ‘ã‚¿ãƒ¼ãƒ³', 'ã‚¹ã‚³ã‚¢', 'ç€é †']
                    cols = [c for c in disp_cols if c in result_df.columns]
                    st.session_state['analyzed_df'] = result_df[cols].copy()
                else:
                    st.session_state['analyzed_df'] = pd.DataFrame()

        # ----------------------------------------------------
        # 3. ç·¨é›†å¯èƒ½ãªãƒ†ãƒ¼ãƒ–ãƒ«
        # ----------------------------------------------------
        st.subheader("ğŸ“ çµæœå…¥åŠ›ãƒ»åˆ†æ")
        st.info("ä¸‹ã®è¡¨ã®ã€Œç€é †ã€åˆ—ã‚’ãƒ€ãƒ–ãƒ«ã‚¯ãƒªãƒƒã‚¯ã—ã¦å…¥åŠ›ã™ã‚‹ã¨ã€ã‚°ãƒ©ãƒ•ã¨ã‚¹ã‚³ã‚¢ãŒå³åº§ã«æ›´æ–°ã•ã‚Œã¾ã™ã€‚")

        if not st.session_state['analyzed_df'].empty:
            edited_df = st.data_editor(
                st.session_state['analyzed_df'],
                column_config={
                    "ç€é †": st.column_config.NumberColumn(
                        "ç€é †",
                        help="1ã€œ18ã®æ•°å€¤ã‚’å…¥åŠ›",
                        min_value=1,
                        max_value=18,
                        step=1,
                        format="%d"
                    )
                },
                disabled=["å ´å", "R", "é¦¬å", "é¨æ‰‹", "ã‚¿ã‚¤ãƒ—", "ãƒ‘ã‚¿ãƒ¼ãƒ³"], 
                hide_index=True,
                use_container_width=True,
                height=500,
                key='editor'
            )

            # ----------------------------------------------------
            # 4. ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é›†è¨ˆ & ã‚°ãƒ©ãƒ• (é–‹å‚¬å ´åˆ¥å¯¾å¿œç‰ˆ)
            # ----------------------------------------------------
            
            # ç€é †ãŒå…¥åŠ›ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ï¼ˆå…¨ãƒ¬ãƒ¼ã‚¹ï¼‰
            df_hits = edited_df[edited_df['ç€é †'].notna()].copy()
            df_hits['ç€é †'] = pd.to_numeric(df_hits['ç€é †'], errors='coerce')
            
            # çš„ä¸­ï¼ˆ3ç€å†…ï¼‰ãƒ‡ãƒ¼ã‚¿
            df_fuku = df_hits[df_hits['ç€é †'] <= 3]

            st.divider()
            
            # --- 1. å…¨ä½“ã®æ¦‚è¦ï¼ˆåˆç®—ï¼‰ ---
            st.markdown("### ğŸ“Š å…¨ä½“ãƒã‚¤ãƒ©ã‚¤ãƒˆ")
            col1, col2, col3 = st.columns(3)
            with col1:
                total_races = len(df_hits[['å ´å', 'R']].drop_duplicates())
                st.metric("ç·æ¶ˆåŒ–ãƒ¬ãƒ¼ã‚¹æ•°", total_races)
            with col2:
                fuku_rate = len(df_fuku) / len(df_hits) * 100 if len(df_hits) > 0 else 0
                st.metric("å…¨ä½“è¤‡å‹ç‡", f"{fuku_rate:.1f}%")
            with col3:
                st.metric("ç·çš„ä¸­æ•°", len(df_fuku))

            # --- 2. é–‹å‚¬å ´ã”ã¨ã®è©³ç´°é›†è¨ˆ ---
            st.markdown("### ğŸŸï¸ é–‹å‚¬å ´åˆ¥ãƒ¬ãƒãƒ¼ãƒˆ")
            
            if not df_hits.empty:
                # ç€é †å…¥åŠ›ãŒã‚ã‚‹é–‹å‚¬å ´ã‚’å–å¾—
                places = sorted(df_hits['å ´å'].unique())
                
                # é–‹å‚¬å ´ã”ã¨ã«ã‚¿ãƒ–ã‚’ä½œæˆ
                tabs = st.tabs(list(places))
                
                for tab, place in zip(tabs, places):
                    with tab:
                        # --- ãã®å ´æ‰€ã ã‘ã®ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º ---
                        local_hits = df_hits[df_hits['å ´å'] == place]       
                        local_fuku = df_fuku[df_fuku['å ´å'] == place]       
                        local_races_count = len(local_hits['R'].unique())    
                        
                        # --- ãã®å ´æ‰€ã®æŒ‡æ¨™ã‚’è¡¨ç¤º ---
                        c1, c2, c3 = st.columns(3)
                        with c1:
                            c1.metric(f"{place} æ¶ˆåŒ–R", local_races_count)
                        with c2:
                            local_rate = len(local_fuku) / len(local_hits) * 100 if len(local_hits) > 0 else 0
                            c2.metric("è¤‡å‹ç‡", f"{local_rate:.1f}%")
                        with c3:
                            c3.metric("çš„ä¸­æ•°", len(local_fuku))
                        
                        st.divider()

                        # --- ãã®å ´æ‰€ã®å††ã‚°ãƒ©ãƒ•ã¨ãƒªã‚¹ãƒˆ ---
                        if not local_fuku.empty:
                            col_graph, col_list = st.columns([1, 1])
                            
                            with col_graph:
                                pat_counts = local_fuku['ãƒ‘ã‚¿ãƒ¼ãƒ³'].value_counts().reset_index()
                                pat_counts.columns = ['ãƒ‘ã‚¿ãƒ¼ãƒ³', 'çš„ä¸­æ•°']
                                
                                fig = px.pie(pat_counts, values='çš„ä¸­æ•°', names='ãƒ‘ã‚¿ãƒ¼ãƒ³', 
                                             title=f'{place} ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥å‚¾å‘',
                                             hole=0.4)
                                st.plotly_chart(fig, use_container_width=True)
                            
                            with col_list:
                                st.caption(f"ğŸ¯ {place} ã®çš„ä¸­ãƒªã‚¹ãƒˆ")
                                st.dataframe(
                                    local_fuku[['R', 'é¦¬å', 'é¨æ‰‹', 'ãƒ‘ã‚¿ãƒ¼ãƒ³', 'ç€é †']], 
                                    use_container_width=True,
                                    height=300
                                )
                        else:
                            st.info(f"{place} ã§ã¯ã¾ã 3ç€ä»¥å†…ã®çš„ä¸­ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
            
            else:
                st.info("ä¸Šã®è¡¨ã§ã€Œç€é †ã€ã‚’å…¥åŠ›ã™ã‚‹ã¨ã€ã“ã“ã«é–‹å‚¬å ´ã”ã¨ã®åˆ†æãŒè¡¨ç¤ºã•ã‚Œã¾ã™ã€‚")

            # --- 3. å‚¾å‘ã‚’åŠ å‘³ã—ãŸã‚¹ã‚³ã‚¢å†è¨ˆç®— ---
            if not df_fuku.empty:
                hit_patterns = df_fuku['ãƒ‘ã‚¿ãƒ¼ãƒ³'].unique()
                future_races = edited_df[edited_df['ç€é †'].isna()].copy()
                
                if not future_races.empty:
                    future_races['å‚¾å‘åŠ ç‚¹'] = future_races['ãƒ‘ã‚¿ãƒ¼ãƒ³'].apply(lambda x: 2.0 if x in hit_patterns else 0.0)
                    future_races['äºˆæƒ³ã‚¹ã‚³ã‚¢'] = future_races['ã‚¹ã‚³ã‚¢'] + future_races['å‚¾å‘åŠ ç‚¹']
                    
                    st.subheader("ğŸ“ˆ å‚¾å‘ã‚’åŠ å‘³ã—ãŸæ¨å¥¨é¦¬ï¼ˆã“ã‚Œã‹ã‚‰èµ°ã‚‹ãƒ¬ãƒ¼ã‚¹ï¼‰")
                    st.dataframe(
                        future_races.sort_values('äºˆæƒ³ã‚¹ã‚³ã‚¢', ascending=False)[['å ´å', 'R', 'é¦¬å', 'ãƒ‘ã‚¿ãƒ¼ãƒ³', 'ã‚¹ã‚³ã‚¢', 'äºˆæƒ³ã‚¹ã‚³ã‚¢']],
                        use_container_width=True
                    )

        else:
            st.warning("æ¨å¥¨é¦¬ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

