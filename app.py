import streamlit as st
import pandas as pd
import numpy as np
import re
import requests
from bs4 import BeautifulSoup

st.set_page_config(page_title="é…ç½®é¦¬åˆ¸è¡“ ç©¶æ¥µåˆ†æ", layout="wide")

# ==========================================
# 1. ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
# ==========================================

def to_half_width(text):
    if pd.isna(text): return text
    text = str(text)
    table = str.maketrans('ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™ï¼', '0123456789.')
    text = text.translate(table)
    return re.sub(r'[^\d\.]', '', text)

def normalize_name(x):
    if pd.isna(x): return ''
    name = str(x).strip().replace('ã€€', '').replace(' ', '')
    return re.sub(r'[â˜…â˜†â–²â–³â—‡]', '', name)

@st.cache_data
def load_data(file):
    try:
        if file.name.endswith('.xlsx'):
            df = pd.read_excel(file, engine='openpyxl')
        else:
            try: df = pd.read_csv(file, encoding='utf-8')
            except: df = pd.read_csv(file, encoding='cp932')
        
        # ãƒ˜ãƒƒãƒ€ãƒ¼ä½ç½®è‡ªå‹•ç‰¹å®š
        if not any(col in str(df.columns) for col in ['é¦¬', 'ç•ª', 'R', 'é¨']):
            for i in range(min(len(df), 10)):
                vals = df.iloc[i].astype(str).values
                if any('é¦¬' in v or 'ç•ª' in v or 'R' in v or 'é¨' in v for v in vals):
                    df.columns = df.iloc[i]; df = df.iloc[i+1:].reset_index(drop=True); break

        df.columns = df.columns.astype(str).str.strip()
        name_map = {
            'å ´æ‰€': 'å ´å', 'é–‹å‚¬': 'å ´å', 'ç«¶é¦¬å ´': 'å ´å',
            'èª¿æ•™å¸«': 'å©èˆ', 'èª¿æ•™å¸«å': 'å©èˆ', 'å©èˆå': 'å©èˆ',
            'é¨æ‰‹å': 'é¨æ‰‹', 'ãƒ¬ãƒ¼ã‚¹': 'R', 'ç•ª': 'æ­£ç•ª', 'é¦¬ç•ª': 'æ­£ç•ª',
            'å˜ã‚ªãƒƒã‚º': 'å˜ï½µï½¯ï½½ï¾', 'å˜å‹ã‚ªãƒƒã‚º': 'å˜ï½µï½¯ï½½ï¾', 'ã‚ªãƒƒã‚º': 'å˜ï½µï½¯ï½½ï¾', 'å˜å‹': 'å˜ï½µï½¯ï½½ï¾'
        }
        df = df.rename(columns=name_map)
        df = df.loc[:, ~df.columns.duplicated()]

        # å¿…é ˆåˆ—å¤‰æ›
        df['R'] = pd.to_numeric(df['R'].apply(to_half_width), errors='coerce')
        df['æ­£ç•ª'] = pd.to_numeric(df['æ­£ç•ª'].apply(to_half_width), errors='coerce')
        df = df.dropna(subset=['R', 'æ­£ç•ª'])
        df['R'] = df['R'].astype(int); df['æ­£ç•ª'] = df['æ­£ç•ª'].astype(int)

        for col in ['é¨æ‰‹', 'å©èˆ', 'é¦¬ä¸»', 'é¦¬å', 'å ´å']:
            if col in df.columns: df[col] = df[col].apply(normalize_name)
            else: df[col] = ''
        
        if 'å˜ï½µï½¯ï½½ï¾' in df.columns:
            df['å˜ï½µï½¯ï½½ï¾'] = pd.to_numeric(df['å˜ï½µï½¯ï½½ï¾'].apply(to_half_width), errors='coerce')
        else: df['å˜ï½µï½¯ï½½ï¾'] = np.nan

        return df.copy(), "success"
    except Exception as e: return pd.DataFrame(), str(e)

# ==========================================
# 2. é…ç½®è¨ˆç®—ã‚¨ãƒ³ã‚¸ãƒ³ï¼ˆæ ¸å¿ƒéƒ¨ï¼‰
# ==========================================

def get_haichi_df(df):
    """æ­£ãƒ»é€†ãƒ»æ­£å¾ªç’°ãƒ»é€†å¾ªç’°ã®4ã¤ã®æ•°å€¤ã‚’å…¨é ­è¨ˆç®—"""
    df = df.copy()
    max_umaban = df.groupby(['å ´å', 'R'])['æ­£ç•ª'].transform('max')
    df['ä½¿ç”¨é ­æ•°'] = max_umaban.fillna(16).astype(int)
    if 'é ­æ•°' in df.columns:
        df['ä½¿ç”¨é ­æ•°'] = pd.to_numeric(df['é ­æ•°'], errors='coerce').fillna(df['ä½¿ç”¨é ­æ•°']).astype(int)

    df['é€†ç•ª'] = (df['ä½¿ç”¨é ­æ•°'] + 1) - df['æ­£ç•ª']
    df['æ­£å¾ªç’°'] = df['ä½¿ç”¨é ­æ•°'] + df['æ­£ç•ª']
    df['é€†å¾ªç’°'] = df['ä½¿ç”¨é ­æ•°'] + df['é€†ç•ª']
    return df

def get_16_pattern(r1, r2):
    """ç†è«–Aã€œPã®16ãƒ‘ã‚¿ãƒ¼ãƒ³è¡Œåˆ—åˆ¤å®š"""
    v1 = [r1['æ­£ç•ª'], r1['é€†ç•ª'], r1['æ­£å¾ªç’°'], r1['é€†å¾ªç’°']]
    v2 = [r2['æ­£ç•ª'], r2['é€†ç•ª'], r2['æ­£å¾ªç’°'], r2['é€†å¾ªç’°']]
    labels = list("ABCDEFGHIJKLMNOP")
    found = []
    for i in range(4):
        for j in range(4):
            if v1[i] == v2[j] and v1[i] != 0:
                found.append(labels[i*4 + j])
    return ",".join(found)

def run_analysis(df):
    """é…ç½®é¦¬åˆ¸ãƒ­ã‚¸ãƒƒã‚¯ã®å®Ÿè¡Œ"""
    df = get_haichi_df(df)
    results = []
    blue_info = set()

    # 1. é’å¡—åˆ†æ (é¨æ‰‹ãƒ»å©èˆãƒ»é¦¬ä¸»)
    for col in ['é¨æ‰‹', 'å©èˆ', 'é¦¬ä¸»']:
        group_keys = ['å ´å', col] if col == 'é¨æ‰‹' else [col]
        for name, group in df.groupby(group_keys):
            if len(group) < 2 or not name: continue
            
            # å…±é€šå€¤(Blue Paint)ã®æ¢ç´¢
            cols = ['æ­£ç•ª', 'é€†ç•ª', 'æ­£å¾ªç’°', 'é€†å¾ªç’°']
            common = None
            for _, r in group.iterrows():
                cur = {int(r[c]) for c in cols if pd.notna(r[c])}
                common = cur if common is None else common.intersection(cur)
            
            if common:
                priority = 1.0 if col == 'é¨æ‰‹' else 0.2
                c_vals = ','.join(map(str, sorted(list(common))))
                for _, row in group.iterrows():
                    results.append({
                        'å ´å': row['å ´å'], 'R': row['R'], 'æ­£ç•ª': row['æ­£ç•ª'], 'é¦¬å': row['é¦¬å'],
                        'å˜ï½µï½¯ï½½ï¾': row.get('å˜ï½µï½¯ï½½ï¾'), 'å±æ€§': f"{col}:{name}", 
                        'ã‚¿ã‚¤ãƒ—': f'â˜…{col}é’å¡—', 'ãƒ‘ã‚¿ãƒ¼ãƒ³': 'é’', 'æ¡ä»¶': f'å…±é€š({c_vals})', 'ã‚¹ã‚³ã‚¢': 9.0 + priority
                    })
                    blue_info.add((row['å ´å'], row['R'], row['æ­£ç•ª'], f"{col}:{name}", row.get('å˜ï½µï½¯ï½½ï¾')))

    # 2. é’å¡—ã®éš£åˆ†æ (é€†è»¢ç¾è±¡)
    if blue_info:
        for (place, race), group in df.groupby(['å ´å', 'R']):
            umaban_map = {int(r['æ­£ç•ª']): r for _, r in group.iterrows()}
            for b_place, b_race, b_num, b_attr, b_odds in blue_info:
                if b_place == place and b_race == race:
                    for side in [b_num-1, b_num+1]:
                        if side in umaban_map:
                            s_row = umaban_map[side]
                            n_score = 9.0
                            s_odds = pd.to_numeric(s_row.get('å˜ï½µï½¯ï½½ï¾'), errors='coerce')
                            if pd.notna(b_odds) and pd.notna(s_odds) and s_odds < b_odds: n_score += 2.0
                            results.append({
                                'å ´å': place, 'R': race, 'æ­£ç•ª': side, 'é¦¬å': s_row['é¦¬å'],
                                'å˜ï½µï½¯ï½½ï¾': s_odds, 'å±æ€§': f"(éš£) <{b_attr}>", 
                                'ã‚¿ã‚¤ãƒ—': 'â–³é’å¡—ã®éš£', 'ãƒ‘ã‚¿ãƒ¼ãƒ³': 'é’éš£', 'æ¡ä»¶': f"#{b_num}ã®éš£", 'ã‚¹ã‚³ã‚¢': n_score
                            })

    # 3. é€šå¸¸ãƒšã‚¢åˆ†æ (ç†è«–A-P)
    for col in ['é¨æ‰‹', 'å©èˆ', 'é¦¬ä¸»']:
        for name, group in df.groupby(['å ´å', col] if col=='é¨æ‰‹' else col):
            if len(group) < 2 or not name: continue
            sorted_group = group.sort_values('R').to_dict('records')
            for i in range(len(sorted_group)-1):
                r1, r2 = sorted_group[i], sorted_group[i+1]
                pat = get_16_pattern(r1, r2)
                if pat:
                    is_chanse = any(x in pat for x in ['C','D','G','H'])
                    score = 4.0 if is_chanse else 3.0
                    for row in [r1, r2]:
                        results.append({
                            'å ´å': row['å ´å'], 'R': row['R'], 'æ­£ç•ª': row['æ­£ç•ª'], 'é¦¬å': row['é¦¬å'],
                            'å˜ï½µï½¯ï½½ï¾': row.get('å˜ï½µï½¯ï½½ï¾'), 'å±æ€§': f"{col}:{name}", 
                            'ã‚¿ã‚¤ãƒ—': 'â—ãƒãƒ£ãƒ³ã‚¹' if is_chanse else 'â—‹ç‹™ã„ç›®', 
                            'ãƒ‘ã‚¿ãƒ¼ãƒ³': pat, 'æ¡ä»¶': f'ãƒšã‚¢({r1["R"]}R-{r2["R"]}R)', 'ã‚¹ã‚³ã‚¢': score
                        })

    if not results: return pd.DataFrame()
    res_df = pd.DataFrame(results)
    agg_funcs = {'å˜ï½µï½¯ï½½ï¾': 'min', 'å±æ€§': lambda x: ' + '.join(sorted(set(x))), 'ã‚¿ã‚¤ãƒ—': lambda x: ' / '.join(sorted(set(x))), 'ãƒ‘ã‚¿ãƒ¼ãƒ³': lambda x: ','.join(sorted(set(x))), 'æ¡ä»¶': lambda x: ' / '.join(sorted(set(x))), 'ã‚¹ã‚³ã‚¢': 'sum', 'æ­£ç•ª': 'first'}
    return res_df.groupby(['å ´å', 'R', 'é¦¬å'], as_index=False).agg(agg_funcs)

# ==========================================
# 3. ç·åˆåˆ¤å®šï¼ˆæ¨å¥¨ãƒãƒ¼ã‚¯ï¼‰
# ==========================================

def apply_ranking(df):
    if df.empty: return df
    df = df.copy()
    if 'ç€é †' not in df.columns: df['ç€é †'] = np.nan
    df['ç€é †'] = pd.to_numeric(df['ç€é †'], errors='coerce')
    
    # çš„ä¸­ãƒˆãƒ¬ãƒ³ãƒ‰ã®å­¦ç¿’
    hit_patterns = set()
    if not df[df['ç€é †']<=3].empty:
        p_str = ','.join(df[df['ç€é †']<=3]['ãƒ‘ã‚¿ãƒ¼ãƒ³'].dropna().astype(str))
        hit_patterns = set(p_str.split(','))

    def get_recommendation(row):
        score = row.get('ã‚¹ã‚³ã‚¢', 0)
        odds = pd.to_numeric(row.get('å˜ï½µï½¯ï½½ï¾'), errors='coerce')
        pats = str(row.get('ãƒ‘ã‚¿ãƒ¼ãƒ³', '')).split(',')
        
        bonus = 0.0
        if any(p in hit_patterns and len(p)==1 for p in pats): bonus += 4.0 # ãƒˆãƒ¬ãƒ³ãƒ‰åŠ ç‚¹
        if pd.notna(odds) and odds > 49.9: bonus -= 30.0 # å¤§ç©´é™¤å¤–
        
        total = score + bonus
        row['ç·åˆã‚¹ã‚³ã‚¢'] = total
        
        if total >= 15: return "ğŸ‘‘ ç›¤çŸ³ã®è»¸"
        if total >= 12: return "âœ¨ æ¨å¥¨è»¸"
        if total >= 10: return "ğŸ”¥ æ¿€ç†±ç›¸æ‰‹"
        if 'é’' in pats: return "â–² é’å¡—ç©´"
        return "â–³ ç´"

    df['æ¨å¥¨è²·ã„ç›®'] = df.apply(get_recommendation, axis=1)
    return df

# ==========================================
# 4. Webå–å¾—
# ==========================================

def fetch_netkeiba_odds(url):
    try:
        headers = {"User-Agent": "Mozilla/5.0"}
        resp = requests.get(url, headers=headers, timeout=10)
        resp.encoding = 'euc-jp'
        soup = BeautifulSoup(resp.content, 'html.parser')
        rows = soup.select('tr.HorseList')
        data = []
        for r in rows:
            u = r.select_one('td[class*="Umaban"]')
            o = r.select_one('td[class*="Popular"]')
            if u:
                u_n = u.get_text(strip=True)
                o_v = re.sub(r'\(.*?\)', '', o.get_text(strip=True)) if o else 'nan'
                try: dv = float(o_v)
                except: dv = np.nan
                data.append({'æ­£ç•ª': int(u_n), 'å˜ï½µï½¯ï½½ï¾': dv})
        return pd.DataFrame(data) if data else None
    except: return None

# ==========================================
# 5. UI
# ==========================================

st.title("ğŸ‡ é…ç½®é¦¬åˆ¸è¡“ ç©¶æ¥µåˆ†æã‚·ã‚¹ãƒ†ãƒ ")

with st.sidebar:
    up_file = st.file_uploader("å‡ºé¦¬è¡¨ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type=['xlsx', 'csv'])
    if 'analyzed_df' in st.session_state:
        st.download_button("ğŸ’¾ ä¿å­˜", st.session_state['analyzed_df'].to_csv(index=False).encode('utf-8-sig'), "race_result.csv")

if up_file:
    df_raw, status = load_data(up_file)
    if status == "success":
        if 'analyzed_df' not in st.session_state:
            with st.spinner('é…ç½®ãƒ­ã‚¸ãƒƒã‚¯è¨ˆç®—ä¸­...'):
                res = run_analysis(df_raw)
                st.session_state['analyzed_df'] = apply_ranking(res)

        full_df = st.session_state['analyzed_df']
        places = sorted(full_df['å ´å'].unique())
        p_tabs = st.tabs(places)
        
        for p_tab, place in zip(p_tabs, places):
            with p_tab:
                p_df = full_df[full_df['å ´å'] == place]
                r_list = sorted(p_df['R'].unique())
                r_tabs = st.tabs([f"{r}R" for r in r_list])
                for r_tab, r_num in zip(r_tabs, r_list):
                    with r_tab:
                        # --- ã‚ªãƒƒã‚ºæ›´æ–° ---
                        with st.expander("ğŸŒ æœ€æ–°ã‚ªãƒƒã‚ºå–å¾—"):
                            u_in = st.text_input("URL", key=f"u_{place}_{r_num}")
                            if st.button("æ›´æ–°", key=f"b_{place}_{r_num}"):
                                new_o = fetch_netkeiba_odds(u_in)
                                if new_o is not None:
                                    for _, row in new_o.iterrows():
                                        mask = (st.session_state['analyzed_df']['å ´å']==place) & (st.session_state['analyzed_df']['R']==r_num) & (st.session_state['analyzed_df']['æ­£ç•ª']==row['æ­£ç•ª'])
                                        st.session_state['analyzed_df'].loc[mask, 'å˜ï½µï½¯ï½½ï¾'] = row['å˜ï½µï½¯ï½½ï¾']
                                    st.session_state['analyzed_df'] = apply_ranking(st.session_state['analyzed_df'])
                                    st.rerun()

                        # --- è¡¨ç¤º ---
                        disp = p_df[p_df['R'] == r_num].sort_values('ç·åˆã‚¹ã‚³ã‚¢', ascending=False)
                        st.dataframe(disp[['æ­£ç•ª', 'é¦¬å', 'å˜ï½µï½¯ï½½ï¾', 'ã‚¿ã‚¤ãƒ—', 'ãƒ‘ã‚¿ãƒ¼ãƒ³', 'æ¡ä»¶', 'æ¨å¥¨è²·ã„ç›®']], use_container_width=True, hide_index=True)
