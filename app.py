import streamlit as st
import pandas as pd
import numpy as np
import re
import plotly.express as px
import openpyxl
import requests
from bs4 import BeautifulSoup

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(page_title="é…ç½®é¦¬åˆ¸è¡“ Web", layout="wide")

# ==========================================
# 1. å…±é€šãƒ­ã‚¸ãƒƒã‚¯ & ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
# ==========================================

def to_half_width(text):
    if isinstance(text, (list, pd.Series, np.ndarray)):
        text = str(text)
    if pd.isna(text): return text
    text = str(text)
    table = str.maketrans('ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™', '0123456789')
    text = text.translate(table)
    text = re.sub(r'[^\d\.]', '', text)
    return text

def normalize_name(x):
    if pd.isna(x): return ''
    normalized_name = str(x).strip().replace('ã€€', '').replace(' ', '')
    normalized_name = re.sub(r'[â˜…â˜†â–²â–³â—‡]', '', normalized_name)
    if ',' in normalized_name: normalized_name = normalized_name.split(',')[0]
    text = re.sub(r'[0-9\.]+[Rr]', '', normalized_name)
    text = re.sub(r'\(.*?\)', '', text)
    return text.replace('/', '').strip()

@st.cache_data
def load_data(file):
    df = None
    if file.name.endswith('.xlsx'):
        try:
            file.seek(0)
            df = pd.read_excel(file, engine='openpyxl')
        except Exception as e:
            return pd.DataFrame(), f"Excelèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}"
    else:
        try:
            file.seek(0)
            df = pd.read_csv(file, encoding='utf-8', on_bad_lines='skip')
        except UnicodeDecodeError:
            try:
                file.seek(0)
                df = pd.read_csv(file, encoding='cp932', on_bad_lines='skip')
            except Exception as e:
                return pd.DataFrame(), f"CSVèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}"
        except Exception as e:
            return pd.DataFrame(), f"CSVäºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼: {e}"

    df.columns = df.columns.str.strip()
    rename_map = {
        'å ´æ‰€': 'å ´å', 'é–‹å‚¬': 'å ´å', 
        'èª¿æ•™å¸«': 'å©èˆ', 'èª¿æ•™å¸«å': 'å©èˆ', 'å©èˆå': 'å©èˆ',
        'é¨æ‰‹å': 'é¨æ‰‹',
        'ãƒ¬ãƒ¼ã‚¹': 'R', 'ï¼²': 'R', 'ãƒ¬ãƒ¼ã‚¹å': 'R',
        'ç€': 'ç€é †', 'ç€ é †': 'ç€é †', 'ç•ª': 'æ­£ç•ª', 'é¦¬ç•ª': 'æ­£ç•ª',
        'å˜ã‚ªãƒƒã‚º': 'å˜ï½µï½¯ï½½ï¾', 'å˜å‹ã‚ªãƒƒã‚º': 'å˜ï½µï½¯ï½½ï¾', 'ã‚ªãƒƒã‚º': 'å˜ï½µï½¯ï½½ï¾', 'å˜å‹': 'å˜ï½µï½¯ï½½ï¾', 'å˜': 'å˜ï½µï½¯ï½½ï¾'
    }
    df = df.rename(columns=rename_map)
    df = df.loc[:, ~df.columns.duplicated()]

    if 'å ´å' not in df.columns: df['å ´å'] = 'Unknown'

    target_numeric_cols = ['R', 'æ­£ç•ª', 'å˜ï½µï½¯ï½½ï¾', 'é€†ç•ª', 'æ­£å¾ªç’°', 'é€†å¾ªç’°', 'é ­æ•°']
    for col in target_numeric_cols:
        if col in df.columns:
            df[col] = df[col].apply(to_half_width)
            df[col] = pd.to_numeric(df[col], errors='coerce')

    if 'R' not in df.columns or 'æ­£ç•ª' not in df.columns:
        return pd.DataFrame(), "ã‚¨ãƒ©ãƒ¼: å¿…é ˆåˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"

    df = df.dropna(subset=['R', 'æ­£ç•ª'])
    df['R'] = df['R'].astype(int); df['æ­£ç•ª'] = df['æ­£ç•ª'].astype(int)

    for col in ['é¨æ‰‹', 'å©èˆ', 'é¦¬ä¸»']:
        if col in df.columns: df[col] = df[col].apply(normalize_name)
        else: df[col] = ''
            
    required_cols = ['R', 'å ´å', 'é¦¬å', 'æ­£ç•ª', 'é¨æ‰‹', 'å©èˆ', 'é¦¬ä¸»', 'å˜ï½µï½¯ï½½ï¾', 'é€†ç•ª', 'æ­£å¾ªç’°', 'é€†å¾ªç’°', 'é ­æ•°']
    for col in required_cols:
        if col not in df.columns: df[col] = np.nan

    save_cols = ['å±æ€§', 'ã‚¿ã‚¤ãƒ—', 'ãƒ‘ã‚¿ãƒ¼ãƒ³', 'æ¡ä»¶', 'ã‚¹ã‚³ã‚¢', 'ç€é †', 'å‚¾å‘åŠ ç‚¹', 'ç·åˆã‚¹ã‚³ã‚¢']
    existing_save_cols = [c for c in save_cols if c in df.columns]
    
    return df[required_cols + existing_save_cols].copy(), "success"

# â˜…ä¿®æ­£: HTMLæ§‹é€ ã‚’ç›´æ¥è§£æã™ã‚‹æœ€å¼·ã®Webå–å¾—é–¢æ•°
def fetch_odds_from_web(url, force_mode=False):
    try:
        t_url = url
        if not force_mode and "shutuba.html" in t_url:
            t_url = t_url.replace("shutuba.html", "odds.html")

        headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"}
        response = requests.get(t_url, headers=headers, timeout=15)
        response.raise_for_status()
        
        # æ–‡å­—ã‚³ãƒ¼ãƒ‰å¼·åˆ¶ãƒ‡ã‚³ãƒ¼ãƒ‰
        if "netkeiba" in t_url: content = response.content.decode('euc-jp', errors='ignore')
        else: content = response.text

        soup = BeautifulSoup(content, 'html.parser')
        
        # ãƒãƒƒãƒˆç«¶é¦¬ã®è¡¨ã®è¡Œï¼ˆHorseListã‚¯ãƒ©ã‚¹ï¼‰ã‚’æ¢ã™
        rows = soup.select('tr.HorseList')
        
        # æ•‘æ¸ˆæªç½®: ã‚‚ã—odds.htmlã§å–ã‚Œãªã‹ã£ãŸã‚‰shutuba.htmlï¼ˆå…ƒã®URLï¼‰ã§å†ãƒˆãƒ©ã‚¤
        if not rows and t_url != url:
            response = requests.get(url, headers=headers, timeout=15)
            content = response.content.decode('euc-jp', errors='ignore')
            soup = BeautifulSoup(content, 'html.parser')
            rows = soup.select('tr.HorseList')

        if not rows:
            return None, "å¯¾è±¡ã®é¦¬ãƒªã‚¹ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"

        data = []
        for row in rows:
            # é¦¬ç•ªã®æŠ½å‡º (Umabanã‚¯ãƒ©ã‚¹)
            umaban_td = row.select_one('td[class*="Umaban"]')
            # ã‚ªãƒƒã‚ºã®æŠ½å‡º (Popularã‚¯ãƒ©ã‚¹)
            odds_td = row.select_one('td[class*="Popular"]')
            
            if umaban_td:
                u_num = umaban_td.get_text(strip=True)
                o_val = np.nan
                if odds_td:
                    # spanã®ä¸­ã®æ•°å€¤ã€ã¾ãŸã¯ãƒ†ã‚­ã‚¹ãƒˆã‚’ç›´æ¥å–å¾—
                    o_text = odds_td.get_text(strip=True)
                    # ä½™è¨ˆãªæ–‡å­—ï¼ˆã‚«ãƒƒã‚³æ›¸ããªã©ï¼‰ã‚’é™¤å»
                    o_text = re.sub(r'\(.*?\)', '', o_text)
                    try: o_val = float(o_text)
                    except: o_val = np.nan
                
                data.append({'æ­£ç•ª': int(u_num), 'å˜ï½µï½¯ï½½ï¾': o_val})

        if data:
            res_df = pd.DataFrame(data)
            return res_df, "Success"
        return None, "ãƒ‡ãƒ¼ã‚¿è§£æã«å¤±æ•—ã—ã¾ã—ãŸã€‚"

    except Exception as e:
        return None, str(e)

# ==========================================
# 2. é…ç½®è¨ˆç®—ãƒ»åˆ†æãƒ­ã‚¸ãƒƒã‚¯
# ==========================================

def calc_haichi_numbers(df):
    max_umaban = df.groupby(['å ´å', 'R'])['æ­£ç•ª'].transform('max')
    df['ä½¿ç”¨é ­æ•°'] = max_umaban.fillna(16).astype(int)
    if 'é ­æ•°' in df.columns: df['ä½¿ç”¨é ­æ•°'] = df['é ­æ•°'].fillna(df['ä½¿ç”¨é ­æ•°']).astype(int)
    df['ä½¿ç”¨é ­æ•°'] = np.maximum(df['ä½¿ç”¨é ­æ•°'], df['æ­£ç•ª'])
    def calc(row):
        t=int(row['ä½¿ç”¨é ­æ•°']); s=int(row['æ­£ç•ª']); g=(t+1)-s; sj=t+s; gj=t+g
        return pd.Series([g, sj, gj])
    df[['è¨ˆç®—_é€†ç•ª', 'è¨ˆç®—_æ­£å¾ªç’°', 'è¨ˆç®—_é€†å¾ªç’°']] = df.apply(calc, axis=1)
    return df

def get_pair_pattern(row1, row2):
    r1 = [row1['æ­£ç•ª'], row1['è¨ˆç®—_é€†ç•ª'], row1['è¨ˆç®—_æ­£å¾ªç’°'], row1['è¨ˆç®—_é€†å¾ªç’°']]
    r2 = [row2['æ­£ç•ª'], row2['è¨ˆç®—_é€†ç•ª'], row2['è¨ˆç®—_æ­£å¾ªç’°'], row2['è¨ˆç®—_é€†å¾ªç’°']]
    label = list("ABCDEFGHIJKLMNOP")
    pairs = [label[i*4+j] for i in range(4) for j in range(4) if r1[i]==r2[j] and r1[i]!=0]
    return ",".join(pairs)

def get_common_values(group):
    cols = ['æ­£ç•ª', 'è¨ˆç®—_é€†ç•ª', 'è¨ˆç®—_æ­£å¾ªç’°', 'è¨ˆç®—_é€†å¾ªç’°']
    common_set = None
    for _, row in group.iterrows():
        cur = set()
        for c in cols:
            val = row.get(c)
            if pd.notna(val): cur.add(int(val))
        if common_set is None: common_set = cur
        else: common_set = common_set.intersection(cur)
    if common_set: return ','.join(map(str, sorted(list(common_set))))
    return None

def analyze_logic(df_curr, df_prev=None):
    df_curr = calc_haichi_numbers(df_curr)
    if df_prev is not None and not df_prev.empty: df_prev = calc_haichi_numbers(df_prev)
    rec_list = []
    
    blue_keys = set()
    for col in ['é¨æ‰‹', 'å©èˆ', 'é¦¬ä¸»']:
        if col not in df_curr.columns: continue
        group_keys = ['å ´å', col] if col == 'é¨æ‰‹' else [col]
        for name_key, group in df_curr.groupby(group_keys):
            if len(group) < 2: continue
            target_name = name_key[1] if col == 'é¨æ‰‹' else name_key
            if not target_name: continue
            common_vals = get_common_values(group)
            if common_vals:
                priority = 1.0 if col == 'é¨æ‰‹' else 0.2
                for _, row in group.iterrows():
                    rec_list.append({
                        'å ´å': row['å ´å'], 'R': row['R'], 'æ­£ç•ª': row['æ­£ç•ª'], 'é¦¬å': row['é¦¬å'],
                        'å˜ï½µï½¯ï½½ï¾': row.get('å˜ï½µï½¯ï½½ï¾'), 'å±æ€§': f"{col}:{target_name}", 
                        'ã‚¿ã‚¤ãƒ—': f'â˜… {col}é’å¡—', 'ãƒ‘ã‚¿ãƒ¼ãƒ³': 'é’', 'æ¡ä»¶': f'å…±é€šå€¤({common_vals})', 'ã‚¹ã‚³ã‚¢': 9.0 + priority
                    })
                    blue_keys.add((row['å ´å'], row['R'], row['é¦¬å'], f"{col}:{target_name}"))

    if blue_keys:
        blue_lookup = {}
        for b in blue_keys:
            key = (b[0], b[1])
            if key not in blue_lookup: blue_lookup[key] = []
            blue_lookup[key].append({'é¦¬å': b[2], 'å±æ€§': b[3]})
        for (place, race), group in df_curr.groupby(['å ´å', 'R']):
            if (place, race) not in blue_lookup: continue
            group = group.sort_values('æ­£ç•ª')
            umaban_map = {int(row['æ­£ç•ª']): row for _, row in group.iterrows()}
            for b_info in blue_lookup[(place, race)]:
                b_row = group[group['é¦¬å'] == b_info['é¦¬å']]
                if b_row.empty: continue
                b_row = b_row.iloc[0]; curr_num = int(b_row['æ­£ç•ª'])
                for t_num in [curr_num - 1, curr_num + 1]:
                    if t_num in umaban_map:
                        t_row = umaban_map[t_num]
                        neighbor_score = 9.0
                        b_odds = pd.to_numeric(b_row.get('å˜ï½µï½¯ï½½ï¾'), errors='coerce')
                        n_odds = pd.to_numeric(t_row.get('å˜ï½µï½¯ï½½ï¾'), errors='coerce')
                        if pd.notna(b_odds) and pd.notna(n_odds) and n_odds < b_odds: neighbor_score += 2.0
                        rec_list.append({
                            'å ´å': place, 'R': race, 'æ­£ç•ª': t_num, 'é¦¬å': t_row['é¦¬å'],
                            'å˜ï½µï½¯ï½½ï¾': n_odds, 'å±æ€§': f"(é’å¡—éš£) <{b_info['å±æ€§']}>", 
                            'ã‚¿ã‚¤ãƒ—': 'â–³ é’å¡—ã®éš£', 'ãƒ‘ã‚¿ãƒ¼ãƒ³': 'é’éš£', 'æ¡ä»¶': f"#{curr_num}ã®éš£", 'ã‚¹ã‚³ã‚¢': neighbor_score
                        })

    for col in ['é¨æ‰‹', 'å©èˆ', 'é¦¬ä¸»']:
        if col not in df_curr.columns: continue
        for name, group in df_curr.groupby(['å ´å', col] if col=='é¨æ‰‹' else col):
            if len(group) < 2: continue
            group = group.sort_values('R').to_dict('records')
            for i in range(len(group)-1):
                curr, nxt = group[i], group[i+1]
                pat = get_pair_pattern(curr, nxt)
                if pat:
                    lbl = "â— ãƒãƒ£ãƒ³ã‚¹" if any(x in pat for x in ['C','D','G','H']) else "â—‹ ç‹™ã„ç›®"
                    base = 4.0 if lbl.startswith("â—") else 3.0
                    rec_list.append({'å ´å': curr['å ´å'], 'R': curr['R'], 'æ­£ç•ª': curr['æ­£ç•ª'], 'é¦¬å': curr['é¦¬å'], 'å˜ï½µï½¯ï½½ï¾': curr.get('å˜ï½µï½¯ï½½ï¾'), 'å±æ€§': f"{col}:{name}", 'ã‚¿ã‚¤ãƒ—': lbl, 'ãƒ‘ã‚¿ãƒ¼ãƒ³': pat, 'æ¡ä»¶': f'ãƒšã‚¢({nxt["R"]}R)', 'ã‚¹ã‚³ã‚¢': base + 1.0})
                    rec_list.append({'å ´å': nxt['å ´å'], 'R': nxt['R'], 'æ­£ç•ª': nxt['æ­£ç•ª'], 'é¦¬å': nxt['é¦¬å'], 'å˜ï½µï½¯ï½½ï¾': nxt.get('å˜ï½µï½¯ï½½ï¾'), 'å±æ€§': f"{col}:{name}", 'ã‚¿ã‚¤ãƒ—': lbl, 'ãƒ‘ã‚¿ãƒ¼ãƒ³': pat, 'æ¡ä»¶': f'ãƒšã‚¢({curr["R"]}R)', 'ã‚¹ã‚³ã‚¢': base + 1.0})

    if not rec_list: return pd.DataFrame()
    res_df = pd.DataFrame(rec_list)
    agg_funcs = {'å˜ï½µï½¯ï½½ï¾': 'min', 'å±æ€§': lambda x: ' + '.join(sorted(set(x))), 'ã‚¿ã‚¤ãƒ—': lambda x: ' / '.join(sorted(set(x))), 'ãƒ‘ã‚¿ãƒ¼ãƒ³': lambda x: ','.join(sorted(set(x))), 'æ¡ä»¶': lambda x: ' / '.join(sorted(set(x))), 'ã‚¹ã‚³ã‚¢': 'sum', 'æ­£ç•ª': 'first'}
    res_df = res_df.groupby(['å ´å', 'R', 'é¦¬å'], as_index=False).agg(agg_funcs)
    if 'ç€é †' not in res_df.columns: res_df['ç€é †'] = np.nan
    return res_df

# ==========================================
# 3. åˆ¤å®šãƒ­ã‚¸ãƒƒã‚¯ (ç·åˆã‚¹ã‚³ã‚¢é‡è¦–)
# ==========================================

def apply_ranking_logic(df_in):
    if df_in.empty: return df_in
    df = df_in.copy()
    df['ç€é †'] = pd.to_numeric(df['ç€é †'], errors='coerce')
    df_hits = df[df['ç€é †'] <= 3]
    hit_patterns = set(); downgraded_attrs = set()
    for _, row in df_hits.iterrows():
        hit_patterns.update(str(row.get('ãƒ‘ã‚¿ãƒ¼ãƒ³', '')).split(','))
        if 'é’éš£' in str(row.get('ãƒ‘ã‚¿ãƒ¼ãƒ³', '')):
            found = re.findall(r'<(.*?)>', str(row.get('å±æ€§', '')))
            downgraded_attrs.update(found)

    def calc_bonus(row):
        bonus = 0.0; pats = str(row.get('ãƒ‘ã‚¿ãƒ¼ãƒ³', '')).split(',')
        for p in pats:
            if p in hit_patterns and len(p) == 1: bonus += 4.0
        if 'é’' in pats:
            my_attrs = str(row.get('å±æ€§', ''))
            if any(bad in my_attrs for bad in downgraded_attrs): bonus -= 3.0
        odds = pd.to_numeric(row.get('å˜ï½µï½¯ï½½ï¾'), errors='coerce')
        if pd.notna(odds) and odds > 49.9: bonus -= 30.0
        return bonus

    def get_bet_recommendation(row):
        score = row['ç·åˆã‚¹ã‚³ã‚¢']; rank = row['ãƒ¬ãƒ¼ã‚¹å†…é †ä½']; pats = str(row.get('ãƒ‘ã‚¿ãƒ¼ãƒ³', '')).split(',')
        is_trend = any(p in hit_patterns for p in pats)
        if score >= 15: r_label = "S"
        elif score >= 12: r_label = "A"
        elif score >= 10: r_label = "B"
        elif 'é’' in pats: r_label = "C"
        else: r_label = "D"
        if rank > 1:
            if r_label == "S": r_label = "A"
            elif r_label == "A": r_label = "B"
        if r_label == "S": return "ğŸ‘‘ ç›¤çŸ³ã®è»¸" if is_trend else "ğŸ‘‘ é‰„æ¿ç´š"
        if r_label == "A": return "âœ¨ å‚¾å‘è»¸" if is_trend else "â— è»¸å€™è£œ"
        if r_label == "B": return "ğŸ”¥ æ¿€ç†±ç›¸æ‰‹" if is_trend else "â—‹ ç›¸æ‰‹ç­†é ­"
        if r_label == "C": return "â˜… å‚¾å‘ç©´" if is_trend else "â–² é’å¡—ç©´"
        return "æ³¨ å‚¾å‘åˆè‡´" if is_trend else "â–³ ç´"

    df['å‚¾å‘åŠ ç‚¹'] = df.apply(calc_bonus, axis=1)
    df['ç·åˆã‚¹ã‚³ã‚¢'] = df['ã‚¹ã‚³ã‚¢'].fillna(0) + df['å‚¾å‘åŠ ç‚¹'].fillna(0)
    df['ãƒ¬ãƒ¼ã‚¹å†…é †ä½'] = df.groupby(['å ´å', 'R'])['ç·åˆã‚¹ã‚³ã‚¢'].rank(method='min', ascending=False)
    df['æ¨å¥¨è²·ã„ç›®'] = df.apply(get_bet_recommendation, axis=1)
    return df

# ==========================================
# 4. ç”»é¢è¡¨ç¤º
# ==========================================

st.title("ğŸ‡ é…ç½®é¦¬åˆ¸è¡“ åˆ†æãƒ„ãƒ¼ãƒ«")

with st.sidebar:
    st.header("1. ãƒ‡ãƒ¼ã‚¿å…¥åŠ›")
    uploaded_file = st.file_uploader("å½“æ—¥ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type=['xlsx', 'csv'])
    prev_file = st.file_uploader("å‰æ—¥ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type=['xlsx', 'csv'])
    if 'analyzed_df' in st.session_state and not st.session_state['analyzed_df'].empty:
        csv = st.session_state['analyzed_df'].to_csv(index=False).encode('utf-8-sig')
        st.download_button("ğŸ’¾ ä¿å­˜ (CSV)", data=csv, file_name="race_save.csv", mime="text/csv")

if uploaded_file:
    df_raw, status = load_data(uploaded_file)
    df_prev, _ = load_data(prev_file) if prev_file else (None, None)
    
    if status != "success": st.error(status)
    else:
        if 'analyzed_df' not in st.session_state:
            with st.spinner('å…¨ãƒ¬ãƒ¼ã‚¹åˆ†æä¸­...'):
                res = analyze_logic(df_raw, df_prev)
                st.session_state['analyzed_df'] = apply_ranking_logic(res)

        if not st.session_state['analyzed_df'].empty:
            full_df = st.session_state['analyzed_df']
            places = sorted(full_df['å ´å'].unique())
            
            with st.form("main_form"):
                p_tabs = st.tabs(places)
                edited_list = []
                for p_tab, place in zip(p_tabs, places):
                    with p_tab:
                        p_df = full_df[full_df['å ´å'] == place]
                        r_list = sorted(p_df['R'].unique())
                        r_tabs = st.tabs([f"{r}R" for r in r_list])
                        for r_tab, r_num in zip(r_tabs, r_list):
                            with r_tab:
                                with st.expander(f"ğŸŒ {place}{r_num}R ã‚ªãƒƒã‚ºå–å¾—"):
                                    c1, c2 = st.columns([3,1])
                                    u_in = c1.text_input("URL", key=f"u_{place}_{r_num}")
                                    f_mo = c2.checkbox("å›ºå®š", key=f"f_{place}_{r_num}")
                                    if st.form_submit_button(f"ğŸ“¥ {place}{r_num}R æ›´æ–°"):
                                        if u_in:
                                            new_odds, msg = fetch_odds_from_web(u_in, f_mo)
                                            if new_odds is not None:
                                                for _, o_row in new_odds.iterrows():
                                                    mask = (st.session_state['analyzed_df']['å ´å'] == place) & (st.session_state['analyzed_df']['R'] == r_num) & (st.session_state['analyzed_df']['æ­£ç•ª'] == o_row['æ­£ç•ª'])
                                                    st.session_state['analyzed_df'].loc[mask, 'å˜ï½µï½¯ï½½ï¾'] = o_row['å˜ï½µï½¯ï½½ï¾']
                                                # å†è¨ˆç®—
                                                st.session_state['analyzed_df'] = apply_ranking_logic(st.session_state['analyzed_df'])
                                                st.success("ã‚ªãƒƒã‚ºã‚’æ›´æ–°ã—ã¾ã—ãŸï¼")
                                                st.rerun()
                                            else: st.error(f"å–å¾—å¤±æ•—: {msg}")

                                r_df = p_df[p_df['R'] == r_num]
                                ed = st.data_editor(r_df, disabled=[c for c in r_df.columns if c != 'ç€é †'], hide_index=True, use_container_width=True, key=f"ed_{place}_{r_num}")
                                edited_list.append(ed)
                if st.form_submit_button("ğŸ”„ å…¨ãƒ¬ãƒ¼ã‚¹ç¢ºå®šã—ã¦å†è¨ˆç®—"):
                    combined = pd.concat(edited_list, ignore_index=True)
                    st.session_state['analyzed_df'] = apply_ranking_logic(combined)
                    st.rerun()

            st.divider()
            st.subheader("ğŸ“ˆ æ¨å¥¨é¦¬ãƒªã‚¹ãƒˆ")
            future = st.session_state['analyzed_df'][st.session_state['analyzed_df']['ç€é †'].isna()]
            if not future.empty:
                f_places = sorted(future['å ´å'].unique())
                f_tabs = st.tabs(f_places)
                for tab, place in zip(f_tabs, f_places):
                    with tab:
                        p_future = future[future['å ´å'] == place]
                        f_r_list = sorted(p_future['R'].unique())
                        fr_tabs = st.tabs([f"{r}R" for r in f_r_list])
                        for fr_tab, r_num in zip(fr_tabs, f_r_list):
                            with fr_tab:
                                t_df = p_future[p_future['R'] == r_num].sort_values('ç·åˆã‚¹ã‚³ã‚¢', ascending=False)
                                if not t_df.empty:
                                    top = t_df.iloc[0]
                                    if top['ç·åˆã‚¹ã‚³ã‚¢'] >= 15: st.success(f"ğŸ”¥ **ç›¤çŸ³**: {top['æ­£ç•ª']} {top['é¦¬å']} ({top['ç·åˆã‚¹ã‚³ã‚¢']:.1f})")
                                    elif top['ç·åˆã‚¹ã‚³ã‚¢'] >= 12: st.info(f"ğŸ’¡ **æ¨å¥¨**: {top['æ­£ç•ª']} {top['é¦¬å']} ({top['ç·åˆã‚¹ã‚³ã‚¢']:.1f})")
                                st.dataframe(t_df[['æ­£ç•ª', 'é¦¬å', 'å˜ï½µï½¯ï½½ï¾', 'ã‚¿ã‚¤ãƒ—', 'ç·åˆã‚¹ã‚³ã‚¢', 'æ¨å¥¨è²·ã„ç›®']], hide_index=True, use_container_width=True)
